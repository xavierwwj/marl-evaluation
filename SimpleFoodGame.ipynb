{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "afefb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Discrete, MultiDiscrete, Tuple, Box\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8496c",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eb321d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodGame(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        # Game Settings\n",
    "        self.agents = env_config['agents'] # [0,1,...]\n",
    "        self.agent_names = ['agent'+str(i) for i in range(len(self.agents))]\n",
    "        self.food_loc = env_config['food_loc'] # {'food1': (row,col), ... } in 0 index\n",
    "        self.agent_loc = env_config['agent_loc'] # {0: (row,col), 1: (row,col), ...} in 0 index\n",
    "        self.board_size = env_config['board_size'] # (rows,cols)\n",
    "        self.time_limit = 20\n",
    "        self.cur_timestep = 0\n",
    "        self.mode = env_config['mode']\n",
    "        self.trainer = env_config['trainer']\n",
    "\n",
    "        # Environment Settings\n",
    "        self.action_space = Discrete(4) # L,R,U,D\n",
    "        if self.trainer == 'maddpg':\n",
    "            self.observation_space = Box(low=np.array([0, 0]*len(self.agents)+[0,0]*len(self.food_loc.items())), high=np.array([self.board_size[0]-1, self.board_size[1]-1]*len(self.agents)+[self.board_size[0], self.board_size[1]]*len(self.food_loc.items())), dtype=np.float32)\n",
    "        else:\n",
    "            self.observation_space = MultiDiscrete([self.board_size[0], self.board_size[1]]*len(self.agents)+[1+self.board_size[0], 1+self.board_size[1]]*len(self.food_loc.items())) # agent1_loc, agent2_loc, food1_loc, ...\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        s = ()\n",
    "        self.cur_timestep = 0\n",
    "        for key in self.agent_loc:\n",
    "            s = s + self.agent_loc[key]\n",
    "\n",
    "        for key in self.food_loc:\n",
    "            s = s + (self.food_loc[key][0]+1,self.food_loc[key][1]+1)\n",
    "\n",
    "        self.state = np.array(s)\n",
    "#         if self.trainer == \"qmix\":\n",
    "#             return {agent: {\"obs\": self.state.copy(), ENV_STATE: self.state.copy()} for agent in self.agent_names}\n",
    "        return {agent: self.state.copy() for agent in self.agent_names}\n",
    "\n",
    "    def render(self):\n",
    "        board = np.full(self.board_size,'',dtype=object)\n",
    "        s = self.state.reshape(-1,2)\n",
    "        agents_loc = s[:len(self.agents),:]\n",
    "        foods_loc = s[len(self.agents):,:]\n",
    "        for i in range(len(agents_loc)):\n",
    "            board[agents_loc[i][0],agents_loc[i][1]] += str(i)\n",
    "        for j in range(len(foods_loc)):\n",
    "            if tuple(foods_loc[j]) != (0,0):\n",
    "                board[foods_loc[j][0]-1,foods_loc[j][1]-1] += 'F'\n",
    "        print(board)\n",
    "        print()\n",
    "        return board\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        self.cur_timestep += 1\n",
    "        '''\n",
    "        actions = {\"agent1\": ..., \"agent2\": ..., ...}\n",
    "        '''\n",
    "        if self.trainer == 'maddpg':\n",
    "            action_dict = {\n",
    "                    k: np.random.choice([0, 1, 2, 3], p=v) for k, v in action_dict.items()\n",
    "                }\n",
    "        s = self.state.reshape(-1,2)\n",
    "        rewards = {}\n",
    "        for k in self.agent_names:\n",
    "            rewards[k] = -1 # penalty per step\n",
    "        food_loc_lt = s[len(self.agent_names):,:]\n",
    "        for i in range(len(self.agent_names)):\n",
    "            action = action_dict[self.agent_names[i]]\n",
    "            if action == 0:\n",
    "                # L\n",
    "                new_pos = [s[i][0], max(0,s[i][1]-1)]\n",
    "            elif action == 1:\n",
    "                # R\n",
    "                new_pos = [s[i][0], min(self.board_size[1]-1,s[i][1]+1)]\n",
    "            elif action == 2:\n",
    "                # U\n",
    "                new_pos = [max(0,s[i][0]-1), s[i][1]]\n",
    "            elif action == 3:\n",
    "                # D\n",
    "                new_pos = [min(self.board_size[0]-1,s[i][0]+1), s[i][1]]\n",
    "            else:\n",
    "                raise ValueError('ActionError')\n",
    "\n",
    "            s[i] = np.array(new_pos)\n",
    "            for j in range(len(food_loc_lt)):\n",
    "                if new_pos[0] == food_loc_lt[j][0]-1 and new_pos[1] == food_loc_lt[j][1]-1:\n",
    "                    s[j+len(self.agent_names)] = np.array([0,0])\n",
    "                    if self.mode == \"coop\":\n",
    "                        for k in self.agent_names:\n",
    "                            rewards[k] += 10 # cooperative global reward\n",
    "                    else:\n",
    "                        rewards[self.agent_names[i]] += 10 # competitive individual reward\n",
    "        new_obs_single = s.flatten()\n",
    "        done = True\n",
    "        for j in range(len(food_loc_lt)):\n",
    "            if tuple(food_loc_lt[j]) != (0,0):\n",
    "                done = False\n",
    "                break\n",
    "        if done or self.time_limit == self.cur_timestep:\n",
    "            dones = {agent: True for agent in self.agent_names}\n",
    "            dones['__all__'] = True\n",
    "        else:\n",
    "            dones = {agent: False for agent in self.agent_names}\n",
    "            dones['__all__'] = False\n",
    "            \n",
    "        infos = {agent: dict() for agent in self.agent_names}\n",
    "\n",
    "#         if self.trainer == 'qmix':\n",
    "#             new_obs = {agent: {\"obs\": new_obs_single.copy(), ENV_STATE: new_obs_single.copy()} for agent in self.agent_names}\n",
    "#         else:\n",
    "        new_obs = {agent: new_obs_single.copy() for agent in self.agent_names}\n",
    "\n",
    "        return new_obs, rewards, dones, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "61a19992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_environment():\n",
    "    env_config = dict()\n",
    "    env_config['agents'] = [0,1]\n",
    "    env_config['food_loc'] = {'food1': (3,3), 'food2': (5,5)}\n",
    "    env_config['agent_loc'] = {0: (4,4), 1: (6,7)}\n",
    "    env_config['board_size'] = (7,10)\n",
    "    env_config['mode'] = \"comp\"\n",
    "    env_config['trainer'] = 'ppo'\n",
    "\n",
    "    fg = FoodGame(env_config)\n",
    "    fg.reset()\n",
    "    fg.render()\n",
    "    MOVE_MAPPING = {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
    "    cumu_rewards = [0,0]\n",
    "    for i in range(100):\n",
    "        a0_action = np.random.randint(4)\n",
    "        a1_action = np.random.randint(4)\n",
    "        print('Actions: agent0 = {}, agent1 = {}'.format(MOVE_MAPPING[a0_action],MOVE_MAPPING[a1_action]))\n",
    "        a, rewards, dones, infos = fg.step(action_dict={'agent0': a0_action, 'agent1': a1_action})\n",
    "        fg.render()\n",
    "        cumu_rewards[0] += rewards['agent0']\n",
    "        cumu_rewards[1] += rewards['agent1']\n",
    "        if (np.array(list(dones.values())) == True).all():\n",
    "            print(\"Episode Ended\")\n",
    "            print(\"Rewards:\",cumu_rewards)\n",
    "            break\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "register_env(\"foodenv\", FoodGame)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c45b3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Actions: agent0 = D, agent1 = D\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Actions: agent0 = R, agent1 = L\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Actions: agent0 = U, agent1 = R\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Actions: agent0 = L, agent1 = D\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Actions: agent0 = R, agent1 = D\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Actions: agent0 = U, agent1 = R\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '1' '']]\n",
      "\n",
      "Actions: agent0 = R, agent1 = R\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']]\n",
      "\n",
      "Actions: agent0 = R, agent1 = L\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '1' '']]\n",
      "\n",
      "Actions: agent0 = L, agent1 = R\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']]\n",
      "\n",
      "Actions: agent0 = D, agent1 = D\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']]\n",
      "\n",
      "Actions: agent0 = D, agent1 = U\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '0' '' '' '1']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Actions: agent0 = R, agent1 = L\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '1' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Actions: agent0 = D, agent1 = D\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '1' '']]\n",
      "\n",
      "Actions: agent0 = D, agent1 = R\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '1']]\n",
      "\n",
      "Actions: agent0 = U, agent1 = R\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']]\n",
      "\n",
      "Actions: agent0 = U, agent1 = D\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']]\n",
      "\n",
      "Actions: agent0 = U, agent1 = D\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']]\n",
      "\n",
      "Actions: agent0 = D, agent1 = U\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Actions: agent0 = U, agent1 = U\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Actions: agent0 = L, agent1 = R\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '1']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode Ended\n",
      "Rewards: [-10, -20]\n"
     ]
    }
   ],
   "source": [
    "simulate_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738d6f1",
   "metadata": {},
   "source": [
    "# Model (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85c5f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])\n",
    "    \n",
    "from ray.rllib.models import ModelCatalog\n",
    "ModelCatalog.register_custom_model(\n",
    "        \"my_model\", TorchCustomModel\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2e6b7",
   "metadata": {},
   "source": [
    "\n",
    "# Environment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ef00ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coop Environment\n",
    "coop_env_config = dict()\n",
    "coop_env_config['agents'] = [0,1]\n",
    "coop_env_config['food_loc'] = {'food1': (3,3), 'food2': (5,5)}\n",
    "coop_env_config['agent_loc'] = {0: (4,4), 1: (6,7)}\n",
    "coop_env_config['board_size'] = (7,10)\n",
    "coop_env_config['mode'] = 'coop'\n",
    "coop_env_config['trainer'] = 'ppo'\n",
    "\n",
    "# Competitive Environment\n",
    "comp_env_config = dict()\n",
    "comp_env_config['agents'] = [0,1]\n",
    "comp_env_config['food_loc'] = {'food1': (3,3), 'food2': (5,5)}\n",
    "comp_env_config['agent_loc'] = {0: (4,4), 1: (6,7)}\n",
    "comp_env_config['board_size'] = (7,10)\n",
    "comp_env_config['mode'] = 'comp'\n",
    "comp_env_config['trainer'] = 'ppo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4d23ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray import tune\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9371d",
   "metadata": {},
   "source": [
    "# PPO Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "326dddea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:54:52 (running for 00:00:00.12)<br>Memory usage on this node: 4.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=4233)\u001b[0m 2022-03-06 02:54:56,052\tINFO trainer.py:2055 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=4233)\u001b[0m 2022-03-06 02:54:56,053\tINFO ppo.py:250 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=4233)\u001b[0m 2022-03-06 02:54:56,053\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m 2022-03-06 02:55:00,653\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:02 (running for 00:00:09.82)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=4233)\u001b[0m 2022-03-06 02:55:02,090\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m 2022-03-06 02:55:02,128\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:03 (running for 00:00:10.83)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:08 (running for 00:00:15.83)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=4233)\u001b[0m 2022-03-06 02:55:08,995\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:13 (running for 00:00:20.84)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 18.339449541284402\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -17.871559633027523\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 218\n",
      "  episodes_total: 218\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.375950813293457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010521992109715939\n",
      "          model: {}\n",
      "          policy_loss: -0.008709507994353771\n",
      "          total_loss: 50.425697326660156\n",
      "          vf_explained_var: 0.0059417420998215675\n",
      "          vf_loss: 50.43230438232422\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3733181953430176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01322883553802967\n",
      "          model: {}\n",
      "          policy_loss: -0.009807806462049484\n",
      "          total_loss: 50.3569221496582\n",
      "          vf_explained_var: 0.004928832873702049\n",
      "          vf_loss: 50.364078521728516\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.95263157894737\n",
      "    ram_util_percent: 33.963157894736845\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.935779816513762\n",
      "    policy1: -8.935779816513762\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08688048105542581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05904384566318747\n",
      "    mean_inference_ms: 1.2736506415617161\n",
      "    mean_raw_obs_processing_ms: 0.2801661430612502\n",
      "  time_since_restore: 12.816805124282837\n",
      "  time_this_iter_s: 12.816805124282837\n",
      "  time_total_s: 12.816805124282837\n",
      "  timers:\n",
      "    learn_throughput: 677.196\n",
      "    learn_time_ms: 5906.71\n",
      "    load_throughput: 8144279.612\n",
      "    load_time_ms: 0.491\n",
      "    sample_throughput: 579.372\n",
      "    sample_time_ms: 6904.029\n",
      "    update_time_ms: 3.577\n",
      "  timestamp: 1646564114\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=4233)\u001b[0m 2022-03-06 02:55:14,972\tWARNING deprecation.py:46 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:19 (running for 00:00:26.75)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.8168</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-17.8716</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           18.3394</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:24 (running for 00:00:31.80)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.8168</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-17.8716</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           18.3394</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 16.389344262295083\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -6.713114754098361\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 244\n",
      "  episodes_total: 462\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3278099298477173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018406812101602554\n",
      "          model: {}\n",
      "          policy_loss: -0.027785705402493477\n",
      "          total_loss: 59.033878326416016\n",
      "          vf_explained_var: 0.022440912202000618\n",
      "          vf_loss: 59.0579833984375\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.335693120956421\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01610071398317814\n",
      "          model: {}\n",
      "          policy_loss: -0.018983857706189156\n",
      "          total_loss: 59.23155975341797\n",
      "          vf_explained_var: 0.020012304186820984\n",
      "          vf_loss: 59.24732971191406\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.47222222222222\n",
      "    ram_util_percent: 34.1\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -3.3565573770491803\n",
      "    policy1: -3.3565573770491803\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08666871443582673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05867981550142534\n",
      "    mean_inference_ms: 1.2694637323376416\n",
      "    mean_raw_obs_processing_ms: 0.2812632708650219\n",
      "  time_since_restore: 25.255186319351196\n",
      "  time_this_iter_s: 12.43838119506836\n",
      "  time_total_s: 25.255186319351196\n",
      "  timers:\n",
      "    learn_throughput: 695.789\n",
      "    learn_time_ms: 5748.867\n",
      "    load_throughput: 8015870.043\n",
      "    load_time_ms: 0.499\n",
      "    sample_throughput: 404.69\n",
      "    sample_time_ms: 9884.109\n",
      "    update_time_ms: 3.362\n",
      "  timestamp: 1646564127\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:29 (running for 00:00:37.26)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         25.2552</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-6.71311</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           16.3893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:34 (running for 00:00:42.31)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         25.2552</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-6.71311</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           16.3893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:39 (running for 00:00:47.31)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         25.2552</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-6.71311</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           16.3893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-55-39\n",
      "  done: false\n",
      "  episode_len_mean: 13.293333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 6.1466666666666665\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 300\n",
      "  episodes_total: 762\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2455263137817383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02329617738723755\n",
      "          model: {}\n",
      "          policy_loss: -0.04341830313205719\n",
      "          total_loss: 60.27446365356445\n",
      "          vf_explained_var: 0.10624027997255325\n",
      "          vf_loss: 60.3132209777832\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.259814739227295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019892537966370583\n",
      "          model: {}\n",
      "          policy_loss: -0.028077049180865288\n",
      "          total_loss: 60.393211364746094\n",
      "          vf_explained_var: 0.1053115651011467\n",
      "          vf_loss: 60.41731262207031\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.194117647058825\n",
      "    ram_util_percent: 34.1\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 3.0733333333333333\n",
      "    policy1: 3.0733333333333333\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08595862911896013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05816614694391108\n",
      "    mean_inference_ms: 1.268497437956373\n",
      "    mean_raw_obs_processing_ms: 0.28366464821957177\n",
      "  time_since_restore: 37.6475191116333\n",
      "  time_this_iter_s: 12.392332792282104\n",
      "  time_total_s: 37.6475191116333\n",
      "  timers:\n",
      "    learn_throughput: 705.067\n",
      "    learn_time_ms: 5673.222\n",
      "    load_throughput: 7917515.809\n",
      "    load_time_ms: 0.505\n",
      "    sample_throughput: 371.618\n",
      "    sample_time_ms: 10763.738\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1646564139\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:45 (running for 00:00:52.76)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         37.6475</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 6.14667</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           13.2933</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:50 (running for 00:00:57.77)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         37.6475</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 6.14667</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           13.2933</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-55-53\n",
      "  done: false\n",
      "  episode_len_mean: 9.424882629107982\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 18.708920187793428\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 426\n",
      "  episodes_total: 1188\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1457754373550415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02284691482782364\n",
      "          model: {}\n",
      "          policy_loss: -0.034737031906843185\n",
      "          total_loss: 53.443328857421875\n",
      "          vf_explained_var: 0.22624389827251434\n",
      "          vf_loss: 53.47121047973633\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1314644813537598\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02406664565205574\n",
      "          model: {}\n",
      "          policy_loss: -0.03294580802321434\n",
      "          total_loss: 53.42662811279297\n",
      "          vf_explained_var: 0.22601889073848724\n",
      "          vf_loss: 53.45476150512695\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.189999999999998\n",
      "    ram_util_percent: 34.10000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 9.354460093896714\n",
      "    policy1: 9.354460093896714\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08641629731325082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058357421029143756\n",
      "    mean_inference_ms: 1.2789874306008802\n",
      "    mean_raw_obs_processing_ms: 0.2922239947874809\n",
      "  time_since_restore: 50.967605113983154\n",
      "  time_this_iter_s: 13.320086002349854\n",
      "  time_total_s: 50.967605113983154\n",
      "  timers:\n",
      "    learn_throughput: 691.229\n",
      "    learn_time_ms: 5786.796\n",
      "    load_throughput: 7600958.659\n",
      "    load_time_ms: 0.526\n",
      "    sample_throughput: 355.017\n",
      "    sample_time_ms: 11267.05\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1646564153\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:55:55 (running for 00:01:03.17)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         50.9676</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 18.7089</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           9.42488</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:00 (running for 00:01:08.18)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         50.9676</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 18.7089</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           9.42488</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:05 (running for 00:01:13.19)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         50.9676</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 18.7089</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           9.42488</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-56-07\n",
      "  done: false\n",
      "  episode_len_mean: 6.712605042016807\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 25.768067226890757\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 595\n",
      "  episodes_total: 1783\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0313035249710083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01652257889509201\n",
      "          model: {}\n",
      "          policy_loss: -0.03275007754564285\n",
      "          total_loss: 33.45328903198242\n",
      "          vf_explained_var: 0.43037667870521545\n",
      "          vf_loss: 33.47860336303711\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9823264479637146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021135222166776657\n",
      "          model: {}\n",
      "          policy_loss: -0.03492990881204605\n",
      "          total_loss: 33.316070556640625\n",
      "          vf_explained_var: 0.43146204948425293\n",
      "          vf_loss: 33.34465789794922\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.499999999999996\n",
      "    ram_util_percent: 34.10526315789474\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 12.884033613445379\n",
      "    policy1: 12.884033613445379\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08889987619988507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060226421261792125\n",
      "    mean_inference_ms: 1.301080702209549\n",
      "    mean_raw_obs_processing_ms: 0.30903508440148064\n",
      "  time_since_restore: 64.61228799819946\n",
      "  time_this_iter_s: 13.644682884216309\n",
      "  time_total_s: 64.61228799819946\n",
      "  timers:\n",
      "    learn_throughput: 690.299\n",
      "    learn_time_ms: 5794.589\n",
      "    load_throughput: 7557304.505\n",
      "    load_time_ms: 0.529\n",
      "    sample_throughput: 338.363\n",
      "    sample_time_ms: 11821.608\n",
      "    update_time_ms: 3.116\n",
      "  timestamp: 1646564167\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:11 (running for 00:01:18.84)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         64.6123</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 25.7681</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           6.71261</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:16 (running for 00:01:23.91)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         64.6123</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 25.7681</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           6.71261</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-56-20\n",
      "  done: false\n",
      "  episode_len_mean: 4.838164251207729\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 30.22705314009662\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 828\n",
      "  episodes_total: 2611\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9060773849487305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014765262603759766\n",
      "          model: {}\n",
      "          policy_loss: -0.0358523391187191\n",
      "          total_loss: 9.402002334594727\n",
      "          vf_explained_var: 0.7081252336502075\n",
      "          vf_loss: 9.4312105178833\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8015428185462952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019450156018137932\n",
      "          model: {}\n",
      "          policy_loss: -0.04528382048010826\n",
      "          total_loss: 9.4247407913208\n",
      "          vf_explained_var: 0.7056267857551575\n",
      "          vf_loss: 9.461271286010742\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.568421052631578\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 15.11352657004831\n",
      "    policy1: 15.11352657004831\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08866727453962696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06012907425943334\n",
      "    mean_inference_ms: 1.2986463415866587\n",
      "    mean_raw_obs_processing_ms: 0.3225201989436099\n",
      "  time_since_restore: 77.67589449882507\n",
      "  time_this_iter_s: 13.06360650062561\n",
      "  time_total_s: 77.67589449882507\n",
      "  timers:\n",
      "    learn_throughput: 692.673\n",
      "    learn_time_ms: 5774.732\n",
      "    load_throughput: 7564119.026\n",
      "    load_time_ms: 0.529\n",
      "    sample_throughput: 331.541\n",
      "    sample_time_ms: 12064.877\n",
      "    update_time_ms: 3.035\n",
      "  timestamp: 1646564180\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:21 (running for 00:01:28.97)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         77.6759</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> 30.2271</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           4.83816</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:26 (running for 00:01:34.03)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         77.6759</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> 30.2271</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           4.83816</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:31 (running for 00:01:39.04)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         77.6759</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> 30.2271</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           4.83816</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 4.088957055214724\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 31.801635991820042\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 978\n",
      "  episodes_total: 3589\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8028068542480469\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010326562449336052\n",
      "          model: {}\n",
      "          policy_loss: -0.02910958230495453\n",
      "          total_loss: 3.455625295639038\n",
      "          vf_explained_var: 0.8659370541572571\n",
      "          vf_loss: 3.4800877571105957\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6451361775398254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012827013619244099\n",
      "          model: {}\n",
      "          policy_loss: -0.04356793686747551\n",
      "          total_loss: 3.473072052001953\n",
      "          vf_explained_var: 0.8648013472557068\n",
      "          vf_loss: 3.5108683109283447\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.36315789473684\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 15.900817995910021\n",
      "    policy1: 15.900817995910021\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08851169479782638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060075479108756547\n",
      "    mean_inference_ms: 1.2991169444884851\n",
      "    mean_raw_obs_processing_ms: 0.33526623922988114\n",
      "  time_since_restore: 90.93907952308655\n",
      "  time_this_iter_s: 13.263185024261475\n",
      "  time_total_s: 90.93907952308655\n",
      "  timers:\n",
      "    learn_throughput: 693.578\n",
      "    learn_time_ms: 5767.195\n",
      "    load_throughput: 7591991.208\n",
      "    load_time_ms: 0.527\n",
      "    sample_throughput: 326.828\n",
      "    sample_time_ms: 12238.868\n",
      "    update_time_ms: 3.016\n",
      "  timestamp: 1646564193\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:36 (running for 00:01:44.37)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         90.9391</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> 31.8016</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           4.08896</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:41 (running for 00:01:49.37)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         90.9391</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> 31.8016</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           4.08896</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:46 (running for 00:01:54.38)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         90.9391</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> 31.8016</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           4.08896</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 3.5897666068222622\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 32.820466786355475\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 1114\n",
      "  episodes_total: 4703\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7289505004882812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008693879470229149\n",
      "          model: {}\n",
      "          policy_loss: -0.02640712633728981\n",
      "          total_loss: 1.359155297279358\n",
      "          vf_explained_var: 0.9307165145874023\n",
      "          vf_loss: 1.3816500902175903\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4965778589248657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008263102732598782\n",
      "          model: {}\n",
      "          policy_loss: -0.042550165206193924\n",
      "          total_loss: 1.3623714447021484\n",
      "          vf_explained_var: 0.9298014044761658\n",
      "          vf_loss: 1.4012032747268677\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.052631578947366\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.410233393177737\n",
      "    policy1: 16.410233393177737\n",
      "  policy_reward_min:\n",
      "    policy0: 7.0\n",
      "    policy1: 7.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0887782993676055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060419757523904286\n",
      "    mean_inference_ms: 1.3022085885801022\n",
      "    mean_raw_obs_processing_ms: 0.3494897494744198\n",
      "  time_since_restore: 104.39964509010315\n",
      "  time_this_iter_s: 13.460565567016602\n",
      "  time_total_s: 104.39964509010315\n",
      "  timers:\n",
      "    learn_throughput: 695.313\n",
      "    learn_time_ms: 5752.806\n",
      "    load_throughput: 7594937.076\n",
      "    load_time_ms: 0.527\n",
      "    sample_throughput: 322.349\n",
      "    sample_time_ms: 12408.921\n",
      "    update_time_ms: 2.965\n",
      "  timestamp: 1646564207\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:52 (running for 00:01:59.83)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">           104.4</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> 32.8205</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">           3.58977</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:56:57 (running for 00:02:04.92)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">           104.4</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> 32.8205</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">           3.58977</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-57-00\n",
      "  done: false\n",
      "  episode_len_mean: 3.372681281618887\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.23777403035413\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 1186\n",
      "  episodes_total: 5889\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6801328659057617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004350955132395029\n",
      "          model: {}\n",
      "          policy_loss: -0.020742267370224\n",
      "          total_loss: 1.36017906665802\n",
      "          vf_explained_var: 0.9568761587142944\n",
      "          vf_loss: 1.3789633512496948\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4271683096885681\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004771272651851177\n",
      "          model: {}\n",
      "          policy_loss: -0.02842487208545208\n",
      "          total_loss: 1.356055498123169\n",
      "          vf_explained_var: 0.9569194316864014\n",
      "          vf_loss: 1.3823331594467163\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.455\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.618887015177066\n",
      "    policy1: 16.618887015177066\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08870100286953225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060442344920045354\n",
      "    mean_inference_ms: 1.302016973131243\n",
      "    mean_raw_obs_processing_ms: 0.3609515564934598\n",
      "  time_since_restore: 117.97828602790833\n",
      "  time_this_iter_s: 13.578640937805176\n",
      "  time_total_s: 117.97828602790833\n",
      "  timers:\n",
      "    learn_throughput: 693.815\n",
      "    learn_time_ms: 5765.228\n",
      "    load_throughput: 7523790.124\n",
      "    load_time_ms: 0.532\n",
      "    sample_throughput: 319.436\n",
      "    sample_time_ms: 12522.073\n",
      "    update_time_ms: 2.99\n",
      "  timestamp: 1646564220\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:02 (running for 00:02:10.48)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         117.978</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> 33.2378</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           3.37268</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:07 (running for 00:02:15.56)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         117.978</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> 33.2378</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           3.37268</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:12 (running for 00:02:20.57)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         117.978</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> 33.2378</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           3.37268</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-57-14\n",
      "  done: false\n",
      "  episode_len_mean: 3.229217110573043\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.541565778853915\n",
      "  episode_reward_min: 16.0\n",
      "  episodes_this_iter: 1239\n",
      "  episodes_total: 7128\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6585890650749207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008488683961331844\n",
      "          model: {}\n",
      "          policy_loss: -0.019339345395565033\n",
      "          total_loss: 0.4175325036048889\n",
      "          vf_explained_var: 0.9751597046852112\n",
      "          vf_loss: 0.4349619150161743\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3340522348880768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007232042495161295\n",
      "          model: {}\n",
      "          policy_loss: -0.032854996621608734\n",
      "          total_loss: 0.4059208631515503\n",
      "          vf_explained_var: 0.9750267267227173\n",
      "          vf_loss: 0.43714869022369385\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.03684210526316\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.770782889426957\n",
      "    policy1: 16.770782889426957\n",
      "  policy_reward_min:\n",
      "    policy0: 8.0\n",
      "    policy1: 8.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08891949680208497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0606587742487224\n",
      "    mean_inference_ms: 1.3039107203605769\n",
      "    mean_raw_obs_processing_ms: 0.3720916469962397\n",
      "  time_since_restore: 131.6086311340332\n",
      "  time_this_iter_s: 13.630345106124878\n",
      "  time_total_s: 131.6086311340332\n",
      "  timers:\n",
      "    learn_throughput: 694.158\n",
      "    learn_time_ms: 5762.381\n",
      "    load_throughput: 7569921.04\n",
      "    load_time_ms: 0.528\n",
      "    sample_throughput: 316.132\n",
      "    sample_time_ms: 12652.935\n",
      "    update_time_ms: 2.972\n",
      "  timestamp: 1646564234\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:18 (running for 00:02:26.26)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         131.609</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 33.5416</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">           3.22922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:23 (running for 00:02:31.27)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         131.609</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 33.5416</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">           3.22922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-57-28\n",
      "  done: false\n",
      "  episode_len_mean: 3.134012539184953\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.73197492163009\n",
      "  episode_reward_min: 26.0\n",
      "  episodes_this_iter: 1276\n",
      "  episodes_total: 8404\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6185867190361023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00634278729557991\n",
      "          model: {}\n",
      "          policy_loss: -0.01196957565844059\n",
      "          total_loss: 0.14816413819789886\n",
      "          vf_explained_var: 0.9905228018760681\n",
      "          vf_loss: 0.15870656073093414\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2995809018611908\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034437088761478662\n",
      "          model: {}\n",
      "          policy_loss: -0.025414222851395607\n",
      "          total_loss: 0.13458088040351868\n",
      "          vf_explained_var: 0.9904971122741699\n",
      "          vf_loss: 0.15922026336193085\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.525\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.865987460815045\n",
      "    policy1: 16.865987460815045\n",
      "  policy_reward_min:\n",
      "    policy0: 13.0\n",
      "    policy1: 13.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08890963189155034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060744498096317885\n",
      "    mean_inference_ms: 1.3027954246777789\n",
      "    mean_raw_obs_processing_ms: 0.3813247171760334\n",
      "  time_since_restore: 145.4781210422516\n",
      "  time_this_iter_s: 13.869489908218384\n",
      "  time_total_s: 145.4781210422516\n",
      "  timers:\n",
      "    learn_throughput: 691.851\n",
      "    learn_time_ms: 5781.594\n",
      "    load_throughput: 7529492.864\n",
      "    load_time_ms: 0.531\n",
      "    sample_throughput: 300.304\n",
      "    sample_time_ms: 13319.818\n",
      "    update_time_ms: 2.923\n",
      "  timestamp: 1646564248\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:29 (running for 00:02:37.20)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         145.478</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  33.732</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">           3.13401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:34 (running for 00:02:42.21)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         145.478</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  33.732</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">           3.13401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:39 (running for 00:02:47.21)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         145.478</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  33.732</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">           3.13401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-57-42\n",
      "  done: false\n",
      "  episode_len_mean: 3.0903474903474906\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.81930501930502\n",
      "  episode_reward_min: 22.0\n",
      "  episodes_this_iter: 1295\n",
      "  episodes_total: 9699\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5999578237533569\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006947714369744062\n",
      "          model: {}\n",
      "          policy_loss: -0.011476180516183376\n",
      "          total_loss: 0.18044686317443848\n",
      "          vf_explained_var: 0.9890069961547852\n",
      "          vf_loss: 0.19035981595516205\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2669049799442291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00379255972802639\n",
      "          model: {}\n",
      "          policy_loss: -0.02051040716469288\n",
      "          total_loss: 0.17011697590351105\n",
      "          vf_explained_var: 0.9890144467353821\n",
      "          vf_loss: 0.19020070135593414\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.315\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.90965250965251\n",
      "    policy1: 16.90965250965251\n",
      "  policy_reward_min:\n",
      "    policy0: 11.0\n",
      "    policy1: 11.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08901986789510652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060889803417791745\n",
      "    mean_inference_ms: 1.3045512801177679\n",
      "    mean_raw_obs_processing_ms: 0.3896527870384689\n",
      "  time_since_restore: 159.15467166900635\n",
      "  time_this_iter_s: 13.67655062675476\n",
      "  time_total_s: 159.15467166900635\n",
      "  timers:\n",
      "    learn_throughput: 690.106\n",
      "    learn_time_ms: 5796.209\n",
      "    load_throughput: 7549822.698\n",
      "    load_time_ms: 0.53\n",
      "    sample_throughput: 297.54\n",
      "    sample_time_ms: 13443.592\n",
      "    update_time_ms: 2.923\n",
      "  timestamp: 1646564262\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:45 (running for 00:02:52.86)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         159.155</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> 33.8193</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">           3.09035</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:50 (running for 00:02:57.97)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         159.155</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> 33.8193</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">           3.09035</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:57:55 (running for 00:03:02.97)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         159.155</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> 33.8193</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">           3.09035</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-57-55\n",
      "  done: false\n",
      "  episode_len_mean: 3.0379939209726445\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.92401215805471\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 1316\n",
      "  episodes_total: 11015\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5749653577804565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010239525698125362\n",
      "          model: {}\n",
      "          policy_loss: -0.006517163943499327\n",
      "          total_loss: 0.05973077565431595\n",
      "          vf_explained_var: 0.9960660338401794\n",
      "          vf_loss: 0.06394404917955399\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05624999850988388\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.23226994276046753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004413093440234661\n",
      "          model: {}\n",
      "          policy_loss: -0.012537894770503044\n",
      "          total_loss: 0.05163505673408508\n",
      "          vf_explained_var: 0.9960685968399048\n",
      "          vf_loss: 0.06392472237348557\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.278947368421058\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.962006079027354\n",
      "    policy1: 16.962006079027354\n",
      "  policy_reward_min:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0888450758124459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06076548709811798\n",
      "    mean_inference_ms: 1.3044424825983667\n",
      "    mean_raw_obs_processing_ms: 0.3957909892718175\n",
      "  time_since_restore: 172.55482578277588\n",
      "  time_this_iter_s: 13.400154113769531\n",
      "  time_total_s: 172.55482578277588\n",
      "  timers:\n",
      "    learn_throughput: 688.996\n",
      "    learn_time_ms: 5805.553\n",
      "    load_throughput: 7586351.345\n",
      "    load_time_ms: 0.527\n",
      "    sample_throughput: 295.199\n",
      "    sample_time_ms: 13550.203\n",
      "    update_time_ms: 2.919\n",
      "  timestamp: 1646564275\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:00 (running for 00:03:08.43)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         172.555</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  33.924</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.03799</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:05 (running for 00:03:13.44)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         172.555</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  33.924</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.03799</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-58-09\n",
      "  done: false\n",
      "  episode_len_mean: 3.028009084027252\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.94398183194549\n",
      "  episode_reward_min: 24.0\n",
      "  episodes_this_iter: 1321\n",
      "  episodes_total: 12336\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5809305310249329\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004271918907761574\n",
      "          model: {}\n",
      "          policy_loss: -0.012118248268961906\n",
      "          total_loss: 0.044982463121414185\n",
      "          vf_explained_var: 0.9967002272605896\n",
      "          vf_loss: 0.05613952875137329\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.22197797894477844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004307493567466736\n",
      "          model: {}\n",
      "          policy_loss: -0.009669405408203602\n",
      "          total_loss: 0.04659130424261093\n",
      "          vf_explained_var: 0.9966925978660583\n",
      "          vf_loss: 0.05613956227898598\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.173684210526318\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.971990915972746\n",
      "    policy1: 16.971990915972746\n",
      "  policy_reward_min:\n",
      "    policy0: 12.0\n",
      "    policy1: 12.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08898859260248325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060901235519580585\n",
      "    mean_inference_ms: 1.3044133863011267\n",
      "    mean_raw_obs_processing_ms: 0.4025924733910326\n",
      "  time_since_restore: 186.23943400382996\n",
      "  time_this_iter_s: 13.684608221054077\n",
      "  time_total_s: 186.23943400382996\n",
      "  timers:\n",
      "    learn_throughput: 693.137\n",
      "    learn_time_ms: 5770.864\n",
      "    load_throughput: 7733574.26\n",
      "    load_time_ms: 0.517\n",
      "    sample_throughput: 293.443\n",
      "    sample_time_ms: 13631.246\n",
      "    update_time_ms: 2.896\n",
      "  timestamp: 1646564289\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:11 (running for 00:03:19.18)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         186.239</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  33.944</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">           3.02801</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:16 (running for 00:03:24.18)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         186.239</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  33.944</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">           3.02801</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:21 (running for 00:03:29.19)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         186.239</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  33.944</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">           3.02801</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-58-22\n",
      "  done: false\n",
      "  episode_len_mean: 3.018099547511312\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.963800904977376\n",
      "  episode_reward_min: 24.0\n",
      "  episodes_this_iter: 1326\n",
      "  episodes_total: 13662\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.555726945400238\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008117114193737507\n",
      "          model: {}\n",
      "          policy_loss: -0.008461705408990383\n",
      "          total_loss: 0.038939666002988815\n",
      "          vf_explained_var: 0.9972318410873413\n",
      "          vf_loss: 0.0464881993830204\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01406249962747097\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.17302396893501282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006209991872310638\n",
      "          model: {}\n",
      "          policy_loss: -0.013275590725243092\n",
      "          total_loss: 0.034336723387241364\n",
      "          vf_explained_var: 0.9971747994422913\n",
      "          vf_loss: 0.047524984925985336\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.11\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.981900452488688\n",
      "    policy1: 16.981900452488688\n",
      "  policy_reward_min:\n",
      "    policy0: 12.0\n",
      "    policy1: 12.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08899475625728705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060940664594676994\n",
      "    mean_inference_ms: 1.303736908161923\n",
      "    mean_raw_obs_processing_ms: 0.4077922248659137\n",
      "  time_since_restore: 199.67551684379578\n",
      "  time_this_iter_s: 13.43608283996582\n",
      "  time_total_s: 199.67551684379578\n",
      "  timers:\n",
      "    learn_throughput: 695.538\n",
      "    learn_time_ms: 5750.941\n",
      "    load_throughput: 7812440.512\n",
      "    load_time_ms: 0.512\n",
      "    sample_throughput: 294.272\n",
      "    sample_time_ms: 13592.844\n",
      "    update_time_ms: 2.895\n",
      "  timestamp: 1646564302\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:26 (running for 00:03:34.57)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         199.676</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> 33.9638</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            3.0181</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:31 (running for 00:03:39.70)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         199.676</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> 33.9638</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            3.0181</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.0112951807228914\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.977409638554214\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 1328\n",
      "  episodes_total: 14990\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5589627027511597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008648366667330265\n",
      "          model: {}\n",
      "          policy_loss: -0.008758383803069592\n",
      "          total_loss: 0.00825829803943634\n",
      "          vf_explained_var: 0.9990207552909851\n",
      "          vf_loss: 0.01604374125599861\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01406249962747097\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.11952726542949677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0071756597608327866\n",
      "          model: {}\n",
      "          policy_loss: -0.01053328812122345\n",
      "          total_loss: 0.005809888709336519\n",
      "          vf_explained_var: 0.9990050792694092\n",
      "          vf_loss: 0.016242267563939095\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.78421052631579\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.988704819277107\n",
      "    policy1: 16.988704819277107\n",
      "  policy_reward_min:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08883982292851555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060815851836761005\n",
      "    mean_inference_ms: 1.3033837013413305\n",
      "    mean_raw_obs_processing_ms: 0.4116088248322144\n",
      "  time_since_restore: 213.11306500434875\n",
      "  time_this_iter_s: 13.437548160552979\n",
      "  time_total_s: 213.11306500434875\n",
      "  timers:\n",
      "    learn_throughput: 695.472\n",
      "    learn_time_ms: 5751.492\n",
      "    load_throughput: 7868131.126\n",
      "    load_time_ms: 0.508\n",
      "    sample_throughput: 293.927\n",
      "    sample_time_ms: 13608.842\n",
      "    update_time_ms: 2.905\n",
      "  timestamp: 1646564316\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:37 (running for 00:03:45.07)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.113</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> 33.9774</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">            3.0113</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:42 (running for 00:03:50.19)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.113</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> 33.9774</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">            3.0113</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:47 (running for 00:03:55.19)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.113</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> 33.9774</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">            3.0113</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-58-49\n",
      "  done: false\n",
      "  episode_len_mean: 3.0060105184072126\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.987978963185576\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 1331\n",
      "  episodes_total: 16321\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5367640852928162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013071063905954361\n",
      "          model: {}\n",
      "          policy_loss: -0.004756246693432331\n",
      "          total_loss: 0.007613020949065685\n",
      "          vf_explained_var: 0.9993414282798767\n",
      "          vf_loss: 0.01089877262711525\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01406249962747097\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.1463312804698944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002173163229599595\n",
      "          model: {}\n",
      "          policy_loss: -0.00584911135956645\n",
      "          total_loss: 0.005025647114962339\n",
      "          vf_explained_var: 0.9993427991867065\n",
      "          vf_loss: 0.010844198055565357\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.131578947368425\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.993989481592788\n",
      "    policy1: 16.993989481592788\n",
      "  policy_reward_min:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08868818428514248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06069843352653514\n",
      "    mean_inference_ms: 1.302295042412051\n",
      "    mean_raw_obs_processing_ms: 0.41507302683081077\n",
      "  time_since_restore: 226.40991950035095\n",
      "  time_this_iter_s: 13.296854496002197\n",
      "  time_total_s: 226.40991950035095\n",
      "  timers:\n",
      "    learn_throughput: 697.09\n",
      "    learn_time_ms: 5738.139\n",
      "    load_throughput: 7865180.254\n",
      "    load_time_ms: 0.509\n",
      "    sample_throughput: 293.558\n",
      "    sample_time_ms: 13625.914\n",
      "    update_time_ms: 2.902\n",
      "  timestamp: 1646564329\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:52 (running for 00:04:00.55)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          226.41</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  33.988</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.00601</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:58:57 (running for 00:04:05.55)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          226.41</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  33.988</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.00601</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:59:02 (running for 00:04:10.57)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          226.41</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  33.988</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.00601</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-59-03\n",
      "  done: false\n",
      "  episode_len_mean: 3.003003003003003\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.993993993993996\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 1332\n",
      "  episodes_total: 17653\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5394428968429565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008600348606705666\n",
      "          model: {}\n",
      "          policy_loss: -0.004174966365098953\n",
      "          total_loss: 0.0011222268221899867\n",
      "          vf_explained_var: 0.9997391700744629\n",
      "          vf_loss: 0.004329653922468424\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031249813735485\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2500635087490082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.037810247391462326\n",
      "          model: {}\n",
      "          policy_loss: -0.005537017714232206\n",
      "          total_loss: -0.0009484412148594856\n",
      "          vf_explained_var: 0.9997400045394897\n",
      "          vf_loss: 0.004322722554206848\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.373684210526317\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.996996996996998\n",
      "    policy1: 16.996996996996998\n",
      "  policy_reward_min:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08850630713052875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06059012435078315\n",
      "    mean_inference_ms: 1.3012690513730474\n",
      "    mean_raw_obs_processing_ms: 0.417936513977606\n",
      "  time_since_restore: 239.7654709815979\n",
      "  time_this_iter_s: 13.355551481246948\n",
      "  time_total_s: 239.7654709815979\n",
      "  timers:\n",
      "    learn_throughput: 696.875\n",
      "    learn_time_ms: 5739.911\n",
      "    load_throughput: 7866655.413\n",
      "    load_time_ms: 0.508\n",
      "    sample_throughput: 294.125\n",
      "    sample_time_ms: 13599.675\n",
      "    update_time_ms: 2.92\n",
      "  timestamp: 1646564343\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:59:08 (running for 00:04:15.85)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         239.765</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  33.994</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">             3.003</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:59:13 (running for 00:04:21.00)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         239.765</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  33.994</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">             3.003</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-59-16\n",
      "  done: false\n",
      "  episode_len_mean: 3.0082768999247556\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.98344620015049\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 1329\n",
      "  episodes_total: 18982\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5575036406517029\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009043997153639793\n",
      "          model: {}\n",
      "          policy_loss: -0.0032459888607263565\n",
      "          total_loss: 0.010768668726086617\n",
      "          vf_explained_var: 0.9992141723632812\n",
      "          vf_loss: 0.012997210025787354\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010546875186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2653702199459076\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009585727006196976\n",
      "          model: {}\n",
      "          policy_loss: -0.007734983693808317\n",
      "          total_loss: 0.005425809882581234\n",
      "          vf_explained_var: 0.9992119073867798\n",
      "          vf_loss: 0.013059694319963455\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.16\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.991723100075244\n",
      "    policy1: 16.991723100075244\n",
      "  policy_reward_min:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08850028892217242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06058468282041585\n",
      "    mean_inference_ms: 1.3019193641587408\n",
      "    mean_raw_obs_processing_ms: 0.42118893926252593\n",
      "  time_since_restore: 253.3393111228943\n",
      "  time_this_iter_s: 13.573840141296387\n",
      "  time_total_s: 253.3393111228943\n",
      "  timers:\n",
      "    learn_throughput: 698.909\n",
      "    learn_time_ms: 5723.205\n",
      "    load_throughput: 7983827.924\n",
      "    load_time_ms: 0.501\n",
      "    sample_throughput: 293.721\n",
      "    sample_time_ms: 13618.374\n",
      "    update_time_ms: 2.887\n",
      "  timestamp: 1646564356\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:59:18 (running for 00:04:26.49)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         253.339</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> 33.9834</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.00828</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:59:23 (running for 00:04:31.62)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         253.339</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> 33.9834</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.00828</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:59:28 (running for 00:04:36.63)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>RUNNING </td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         253.339</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> 33.9834</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">           3.00828</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_d9b3e_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-59-30\n",
      "  done: true\n",
      "  episode_len_mean: 3.0015003750937734\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.996999249812454\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 1333\n",
      "  episodes_total: 20315\n",
      "  experiment_id: 4c60d222bcb243f3a24b5f056b18be0a\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11249999701976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5216752290725708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00932738184928894\n",
      "          model: {}\n",
      "          policy_loss: -0.0026260078884661198\n",
      "          total_loss: 0.000766995653975755\n",
      "          vf_explained_var: 0.9998573660850525\n",
      "          vf_loss: 0.0023436725605279207\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010546875186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.32358673214912415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003622479736804962\n",
      "          model: {}\n",
      "          policy_loss: -0.004091262351721525\n",
      "          total_loss: -0.0017100455006584525\n",
      "          vf_explained_var: 0.9998573064804077\n",
      "          vf_loss: 0.0023430108558386564\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.64210526315789\n",
      "    ram_util_percent: 34.20000000000001\n",
      "  pid: 4233\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.998499624906227\n",
      "    policy1: 16.998499624906227\n",
      "  policy_reward_min:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08838910242686907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.060532319870736116\n",
      "    mean_inference_ms: 1.3016261922837948\n",
      "    mean_raw_obs_processing_ms: 0.4236603648651749\n",
      "  time_since_restore: 266.6717360019684\n",
      "  time_this_iter_s: 13.332424879074097\n",
      "  time_total_s: 266.6717360019684\n",
      "  timers:\n",
      "    learn_throughput: 700.885\n",
      "    learn_time_ms: 5707.071\n",
      "    load_throughput: 7961097.086\n",
      "    load_time_ms: 0.502\n",
      "    sample_throughput: 294.404\n",
      "    sample_time_ms: 13586.754\n",
      "    update_time_ms: 2.892\n",
      "  timestamp: 1646564370\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: d9b3e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:59:30 (running for 00:04:38.02)<br>Memory usage on this node: 5.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_d9b3e_00000</td><td>TERMINATED</td><td>172.17.0.2:4233</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         266.672</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  33.997</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">            3.0015</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m 2022-03-06 02:59:30,711\tERROR worker.py:432 -- SystemExit was raised from the worker.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 639, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1156, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1235, in exit_actor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"python/ray/_raylet.pyx\", line 770, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"python/ray/_raylet.pyx\", line 591, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 121, in format_exception\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     capture_locals=capture_locals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 363, in extract\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 285, in line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 137, in updatecache\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     lines = fp.readlines()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/codecs.py\", line 319, in decode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     def decode(self, input, final=False):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 429, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4232)\u001b[0m SystemExit: 1\n",
      "2022-03-06 02:59:30,820\tINFO tune.py:636 -- Total run time: 278.55 seconds (277.88 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "e = FoodGame(coop_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"timesteps_per_iteration\"] = 1000\n",
    "config[\"env_config\"] = coop_env_config\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"training_iteration\": 20}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6975622",
   "metadata": {},
   "source": [
    "# Results Analysis (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "98cc6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "817e3573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=4360)\u001b[0m 2022-03-06 03:00:39,901\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-06 03:00:41,327\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-06 03:00:41,427\tINFO trainable.py:473 -- Restored on 172.17.0.2 from checkpoint: /home/ray/cs4246/06_03_2022_02:54:52/PPOTrainer_2022-03-06_02-54-52/PPOTrainer_foodenv_d9b3e_00000_0_2022-03-06_02-54-52/checkpoint_000020/checkpoint-20\n",
      "2022-03-06 03:00:41,427\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 80000, '_time_total': 266.6717360019684, '_episodes_total': 20315}\n"
     ]
    }
   ],
   "source": [
    "trainer = ppo.PPOTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(best_checkpoint[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c8030c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state at Time 0:\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 1\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 2\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '1' '' '' '' '']]\n",
      "\n",
      "Time: 3\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '1' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode Ended\n",
      "Episode Rewards {'agent0': 17, 'agent1': 17}\n"
     ]
    }
   ],
   "source": [
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe12e4",
   "metadata": {},
   "source": [
    "# PPO Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3eaef693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:06 (running for 00:00:00.12)<br>Memory usage on this node: 3.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3770)\u001b[0m 2022-03-06 02:34:09,852\tINFO trainer.py:2055 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3770)\u001b[0m 2022-03-06 02:34:09,853\tINFO ppo.py:250 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=3770)\u001b[0m 2022-03-06 02:34:09,853\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m 2022-03-06 02:34:14,457\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:15 (running for 00:00:09.88)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3770)\u001b[0m 2022-03-06 02:34:15,873\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m 2022-03-06 02:34:15,911\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:16 (running for 00:00:10.89)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:21 (running for 00:00:15.90)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3770)\u001b[0m 2022-03-06 02:34:22,741\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:26 (running for 00:00:20.90)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-34-28\n",
      "  done: false\n",
      "  episode_len_mean: 18.205479452054796\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -26.319634703196346\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 219\n",
      "  episodes_total: 219\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.37324059009552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013065727427601814\n",
      "          model: {}\n",
      "          policy_loss: -0.023408152163028717\n",
      "          total_loss: 45.62505340576172\n",
      "          vf_explained_var: 0.008458759635686874\n",
      "          vf_loss: 45.645851135253906\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3730924129486084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013288713991641998\n",
      "          model: {}\n",
      "          policy_loss: -0.02583553083240986\n",
      "          total_loss: 51.264827728271484\n",
      "          vf_explained_var: 0.021896280348300934\n",
      "          vf_loss: 51.28800582885742\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.947368421052634\n",
      "    ram_util_percent: 29.357894736842102\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 11.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -11.310502283105023\n",
      "    policy1: -15.009132420091325\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08657550072854711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05690624224665878\n",
      "    mean_inference_ms: 1.269810559063725\n",
      "    mean_raw_obs_processing_ms: 0.27717467815510716\n",
      "  time_since_restore: 12.758416414260864\n",
      "  time_this_iter_s: 12.758416414260864\n",
      "  time_total_s: 12.758416414260864\n",
      "  timers:\n",
      "    learn_throughput: 679.699\n",
      "    learn_time_ms: 5884.961\n",
      "    load_throughput: 7876627.23\n",
      "    load_time_ms: 0.508\n",
      "    sample_throughput: 582.477\n",
      "    sample_time_ms: 6867.218\n",
      "    update_time_ms: 2.887\n",
      "  timestamp: 1646562868\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=3770)\u001b[0m 2022-03-06 02:34:28,693\tWARNING deprecation.py:46 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:32 (running for 00:00:26.75)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.7584</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-26.3196</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           18.2055</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:37 (running for 00:00:31.80)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.7584</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-26.3196</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           18.2055</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-34-41\n",
      "  done: false\n",
      "  episode_len_mean: 16.48971193415638\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -19.234567901234566\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 243\n",
      "  episodes_total: 462\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3291960954666138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019748281687498093\n",
      "          model: {}\n",
      "          policy_loss: -0.034526389092206955\n",
      "          total_loss: 42.509395599365234\n",
      "          vf_explained_var: 0.01757637783885002\n",
      "          vf_loss: 42.53997039794922\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3197855949401855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02552935667335987\n",
      "          model: {}\n",
      "          policy_loss: -0.040506135672330856\n",
      "          total_loss: 44.138099670410156\n",
      "          vf_explained_var: 0.07225672900676727\n",
      "          vf_loss: 44.17349624633789\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.388888888888893\n",
      "    ram_util_percent: 29.5\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.518518518518518\n",
      "    policy1: -11.716049382716049\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08755361358905758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05767652652007672\n",
      "    mean_inference_ms: 1.2724336989595508\n",
      "    mean_raw_obs_processing_ms: 0.2810964285172666\n",
      "  time_since_restore: 25.15099811553955\n",
      "  time_this_iter_s: 12.392581701278687\n",
      "  time_total_s: 25.15099811553955\n",
      "  timers:\n",
      "    learn_throughput: 703.8\n",
      "    learn_time_ms: 5683.435\n",
      "    load_throughput: 7788865.367\n",
      "    load_time_ms: 0.514\n",
      "    sample_throughput: 404.733\n",
      "    sample_time_ms: 9883.051\n",
      "    update_time_ms: 3.029\n",
      "  timestamp: 1646562881\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:43 (running for 00:00:37.21)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          25.151</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-19.2346</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           16.4897</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:48 (running for 00:00:42.26)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          25.151</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-19.2346</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           16.4897</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:53 (running for 00:00:47.26)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          25.151</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-19.2346</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           16.4897</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 13.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -9.913333333333334\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 300\n",
      "  episodes_total: 762\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2406936883926392\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02633821591734886\n",
      "          model: {}\n",
      "          policy_loss: -0.041184499859809875\n",
      "          total_loss: 53.8184700012207\n",
      "          vf_explained_var: 0.052820127457380295\n",
      "          vf_loss: 53.85438537597656\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.234985589981079\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021322065964341164\n",
      "          model: {}\n",
      "          policy_loss: -0.04102913662791252\n",
      "          total_loss: 46.43207931518555\n",
      "          vf_explained_var: 0.07862315326929092\n",
      "          vf_loss: 46.46670913696289\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.035294117647055\n",
      "    ram_util_percent: 29.5\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -1.9733333333333334\n",
      "    policy1: -7.94\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08705955437347991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05735968383249963\n",
      "    mean_inference_ms: 1.2750640708936687\n",
      "    mean_raw_obs_processing_ms: 0.28360361814916096\n",
      "  time_since_restore: 37.5633339881897\n",
      "  time_this_iter_s: 12.412335872650146\n",
      "  time_total_s: 37.5633339881897\n",
      "  timers:\n",
      "    learn_throughput: 712.223\n",
      "    learn_time_ms: 5616.216\n",
      "    load_throughput: 7887736.718\n",
      "    load_time_ms: 0.507\n",
      "    sample_throughput: 372.124\n",
      "    sample_time_ms: 10749.116\n",
      "    update_time_ms: 2.989\n",
      "  timestamp: 1646562893\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:34:58 (running for 00:00:52.74)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         37.5633</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-9.91333</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             13.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:03 (running for 00:00:57.74)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         37.5633</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-9.91333</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             13.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 9.60576923076923\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -0.1971153846153846\n",
      "  episode_reward_min: -30.0\n",
      "  episodes_this_iter: 416\n",
      "  episodes_total: 1178\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1180161237716675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026903493329882622\n",
      "          model: {}\n",
      "          policy_loss: -0.05394747480750084\n",
      "          total_loss: 47.69227600097656\n",
      "          vf_explained_var: 0.0785447433590889\n",
      "          vf_loss: 47.7381591796875\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.117930293083191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024491414427757263\n",
      "          model: {}\n",
      "          policy_loss: -0.05005425587296486\n",
      "          total_loss: 46.37116241455078\n",
      "          vf_explained_var: 0.076431043446064\n",
      "          vf_loss: 46.410194396972656\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.38888888888889\n",
      "    ram_util_percent: 29.544444444444448\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 2.293269230769231\n",
      "    policy1: -2.4903846153846154\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0867055852773316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05702300113734959\n",
      "    mean_inference_ms: 1.2708841789574128\n",
      "    mean_raw_obs_processing_ms: 0.28967230061159815\n",
      "  time_since_restore: 49.959112882614136\n",
      "  time_this_iter_s: 12.395778894424438\n",
      "  time_total_s: 49.959112882614136\n",
      "  timers:\n",
      "    learn_throughput: 716.501\n",
      "    learn_time_ms: 5582.686\n",
      "    load_throughput: 7529323.909\n",
      "    load_time_ms: 0.531\n",
      "    sample_throughput: 357.825\n",
      "    sample_time_ms: 11178.651\n",
      "    update_time_ms: 2.977\n",
      "  timestamp: 1646562906\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:09 (running for 00:01:03.21)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         49.9591</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-0.197115</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">           9.60577</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:14 (running for 00:01:08.21)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         49.9591</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-0.197115</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">           9.60577</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-35-18\n",
      "  done: false\n",
      "  episode_len_mean: 6.443729903536977\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 6.983922829581994\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 622\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9872897267341614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02665587328374386\n",
      "          model: {}\n",
      "          policy_loss: -0.05086745321750641\n",
      "          total_loss: 26.47137451171875\n",
      "          vf_explained_var: 0.22563855350017548\n",
      "          vf_loss: 26.510244369506836\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9740062355995178\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017573043704032898\n",
      "          model: {}\n",
      "          policy_loss: -0.05110420286655426\n",
      "          total_loss: 33.21519088745117\n",
      "          vf_explained_var: 0.19133302569389343\n",
      "          vf_loss: 33.254432678222656\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.549999999999997\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 5.581993569131833\n",
      "    policy1: 1.4019292604501608\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08661301992731175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05697776100574854\n",
      "    mean_inference_ms: 1.2713032217956977\n",
      "    mean_raw_obs_processing_ms: 0.30007575024462285\n",
      "  time_since_restore: 62.549700021743774\n",
      "  time_this_iter_s: 12.590587139129639\n",
      "  time_total_s: 62.549700021743774\n",
      "  timers:\n",
      "    learn_throughput: 719.335\n",
      "    learn_time_ms: 5560.692\n",
      "    load_throughput: 7581209.218\n",
      "    load_time_ms: 0.528\n",
      "    sample_throughput: 348.548\n",
      "    sample_time_ms: 11476.172\n",
      "    update_time_ms: 3.002\n",
      "  timestamp: 1646562918\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:19 (running for 00:01:13.87)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         62.5497</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 6.98392</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           6.44373</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:24 (running for 00:01:18.88)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         62.5497</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 6.98392</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           6.44373</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:29 (running for 00:01:23.89)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         62.5497</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 6.98392</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           6.44373</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-35-31\n",
      "  done: false\n",
      "  episode_len_mean: 5.383580080753701\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.179004037685061\n",
      "  episode_reward_min: -30.0\n",
      "  episodes_this_iter: 743\n",
      "  episodes_total: 2543\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8998331427574158\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017797553911805153\n",
      "          model: {}\n",
      "          policy_loss: -0.058445826172828674\n",
      "          total_loss: 18.84800148010254\n",
      "          vf_explained_var: 0.35562750697135925\n",
      "          vf_loss: 18.894433975219727\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8525352478027344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012510914355516434\n",
      "          model: {}\n",
      "          policy_loss: -0.045734111219644547\n",
      "          total_loss: 26.88658905029297\n",
      "          vf_explained_var: 0.252567321062088\n",
      "          vf_loss: 26.92387580871582\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.083333333333332\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 7.1467025572005385\n",
      "    policy1: 2.0323014804845223\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08649232213603751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05692790177537873\n",
      "    mean_inference_ms: 1.269656595332062\n",
      "    mean_raw_obs_processing_ms: 0.3103380635164066\n",
      "  time_since_restore: 75.17585563659668\n",
      "  time_this_iter_s: 12.626155614852905\n",
      "  time_total_s: 75.17585563659668\n",
      "  timers:\n",
      "    learn_throughput: 721.236\n",
      "    learn_time_ms: 5546.037\n",
      "    load_throughput: 7678945.457\n",
      "    load_time_ms: 0.521\n",
      "    sample_throughput: 342.48\n",
      "    sample_time_ms: 11679.518\n",
      "    update_time_ms: 2.999\n",
      "  timestamp: 1646562931\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:35 (running for 00:01:29.52)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         75.1759</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   9.179</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">           5.38358</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:40 (running for 00:01:34.59)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         75.1759</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   9.179</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">           5.38358</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 4.950433705080545\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 10.074349442379182\n",
      "  episode_reward_min: -30.0\n",
      "  episodes_this_iter: 807\n",
      "  episodes_total: 3350\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.825964629650116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018055064603686333\n",
      "          model: {}\n",
      "          policy_loss: -0.06609361618757248\n",
      "          total_loss: 15.506969451904297\n",
      "          vf_explained_var: 0.44298866391181946\n",
      "          vf_loss: 15.560876846313477\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7550716996192932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008658917620778084\n",
      "          model: {}\n",
      "          policy_loss: -0.020431531593203545\n",
      "          total_loss: 20.051267623901367\n",
      "          vf_explained_var: 0.39284542202949524\n",
      "          vf_loss: 20.06585693359375\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.11578947368421\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 8.234200743494425\n",
      "    policy1: 1.8401486988847584\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08642666254983256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.056847040706275005\n",
      "    mean_inference_ms: 1.2701609097124484\n",
      "    mean_raw_obs_processing_ms: 0.3193470457741919\n",
      "  time_since_restore: 87.92845845222473\n",
      "  time_this_iter_s: 12.752602815628052\n",
      "  time_total_s: 87.92845845222473\n",
      "  timers:\n",
      "    learn_throughput: 721.994\n",
      "    learn_time_ms: 5540.215\n",
      "    load_throughput: 7600343.774\n",
      "    load_time_ms: 0.526\n",
      "    sample_throughput: 337.887\n",
      "    sample_time_ms: 11838.269\n",
      "    update_time_ms: 2.975\n",
      "  timestamp: 1646562944\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:46 (running for 00:01:40.34)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         87.9285</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> 10.0743</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">           4.95043</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:51 (running for 00:01:45.41)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         87.9285</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> 10.0743</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">           4.95043</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:35:56 (running for 00:01:50.41)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         87.9285</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> 10.0743</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">           4.95043</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 5.128205128205129\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.743589743589743\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 780\n",
      "  episodes_total: 4130\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7236355543136597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016552314162254333\n",
      "          model: {}\n",
      "          policy_loss: -0.05815799534320831\n",
      "          total_loss: 12.897686004638672\n",
      "          vf_explained_var: 0.4687221646308899\n",
      "          vf_loss: 12.944671630859375\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6940853595733643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006523142568767071\n",
      "          model: {}\n",
      "          policy_loss: -0.020656786859035492\n",
      "          total_loss: 17.733226776123047\n",
      "          vf_explained_var: 0.4144138991832733\n",
      "          vf_loss: 17.749481201171875\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.361111111111107\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 9.423076923076923\n",
      "    policy1: 0.32051282051282054\n",
      "  policy_reward_min:\n",
      "    policy0: -9.0\n",
      "    policy1: -12.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08627173416406143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.056674043028314615\n",
      "    mean_inference_ms: 1.2656989490526556\n",
      "    mean_raw_obs_processing_ms: 0.3253091093503044\n",
      "  time_since_restore: 100.48105669021606\n",
      "  time_this_iter_s: 12.552598237991333\n",
      "  time_total_s: 100.48105669021606\n",
      "  timers:\n",
      "    learn_throughput: 722.779\n",
      "    learn_time_ms: 5534.197\n",
      "    load_throughput: 7553476.729\n",
      "    load_time_ms: 0.53\n",
      "    sample_throughput: 335.075\n",
      "    sample_time_ms: 11937.614\n",
      "    update_time_ms: 2.98\n",
      "  timestamp: 1646562956\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:02 (running for 00:01:56.02)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         100.481</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> 9.74359</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">           5.12821</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:07 (running for 00:02:01.03)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         100.481</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> 9.74359</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">           5.12821</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-36-09\n",
      "  done: false\n",
      "  episode_len_mean: 5.444897959183673\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.110204081632654\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 735\n",
      "  episodes_total: 4865\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6315877437591553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01062021404504776\n",
      "          model: {}\n",
      "          policy_loss: -0.043713513761758804\n",
      "          total_loss: 9.505982398986816\n",
      "          vf_explained_var: 0.5462391972541809\n",
      "          vf_loss: 9.54252815246582\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6629542708396912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00418840954080224\n",
      "          model: {}\n",
      "          policy_loss: -0.011364351026713848\n",
      "          total_loss: 12.527992248535156\n",
      "          vf_explained_var: 0.4416380524635315\n",
      "          vf_loss: 12.536530494689941\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.827777777777776\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 11.112925170068028\n",
      "    policy1: -2.0027210884353743\n",
      "  policy_reward_min:\n",
      "    policy0: -7.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08619937932225408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05656774665033786\n",
      "    mean_inference_ms: 1.2662531928828566\n",
      "    mean_raw_obs_processing_ms: 0.32921062039387256\n",
      "  time_since_restore: 113.2525041103363\n",
      "  time_this_iter_s: 12.77144742012024\n",
      "  time_total_s: 113.2525041103363\n",
      "  timers:\n",
      "    learn_throughput: 721.958\n",
      "    learn_time_ms: 5540.487\n",
      "    load_throughput: 7524165.039\n",
      "    load_time_ms: 0.532\n",
      "    sample_throughput: 332.584\n",
      "    sample_time_ms: 12027.022\n",
      "    update_time_ms: 2.982\n",
      "  timestamp: 1646562969\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:12 (running for 00:02:06.87)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         113.253</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  9.1102</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            5.4449</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:17 (running for 00:02:11.87)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         113.253</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  9.1102</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            5.4449</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-36-22\n",
      "  done: false\n",
      "  episode_len_mean: 5.662889518413598\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.674220963172804\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 706\n",
      "  episodes_total: 5571\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5683839321136475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006266110111027956\n",
      "          model: {}\n",
      "          policy_loss: -0.038955360651016235\n",
      "          total_loss: 8.66318130493164\n",
      "          vf_explained_var: 0.6192721724510193\n",
      "          vf_loss: 8.697906494140625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6308108568191528\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006040985230356455\n",
      "          model: {}\n",
      "          policy_loss: -0.00884491577744484\n",
      "          total_loss: 10.35006332397461\n",
      "          vf_explained_var: 0.46956104040145874\n",
      "          vf_loss: 10.356870651245117\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.5\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 11.73087818696884\n",
      "    policy1: -3.056657223796034\n",
      "  policy_reward_min:\n",
      "    policy0: 1.0\n",
      "    policy1: -12.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08611477499279253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05644683983799148\n",
      "    mean_inference_ms: 1.2636309441785591\n",
      "    mean_raw_obs_processing_ms: 0.33180677474449555\n",
      "  time_since_restore: 125.79565048217773\n",
      "  time_this_iter_s: 12.54314637184143\n",
      "  time_total_s: 125.79565048217773\n",
      "  timers:\n",
      "    learn_throughput: 722.377\n",
      "    learn_time_ms: 5537.274\n",
      "    load_throughput: 7587380.608\n",
      "    load_time_ms: 0.527\n",
      "    sample_throughput: 330.755\n",
      "    sample_time_ms: 12093.559\n",
      "    update_time_ms: 2.981\n",
      "  timestamp: 1646562982\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:23 (running for 00:02:17.48)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         125.796</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 8.67422</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">           5.66289</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:28 (running for 00:02:22.48)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         125.796</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 8.67422</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">           5.66289</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:33 (running for 00:02:27.49)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         125.796</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 8.67422</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">           5.66289</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-36-35\n",
      "  done: false\n",
      "  episode_len_mean: 5.821220930232558\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.357558139534884\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 688\n",
      "  episodes_total: 6259\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5158219337463379\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004861412104219198\n",
      "          model: {}\n",
      "          policy_loss: -0.030947940424084663\n",
      "          total_loss: 7.382653713226318\n",
      "          vf_explained_var: 0.6732740998268127\n",
      "          vf_loss: 7.41032075881958\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5890405774116516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007004424463957548\n",
      "          model: {}\n",
      "          policy_loss: -0.010576347820460796\n",
      "          total_loss: 7.97367000579834\n",
      "          vf_explained_var: 0.48753583431243896\n",
      "          vf_loss: 7.981882572174072\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.655555555555555\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 12.390988372093023\n",
      "    policy1: -4.03343023255814\n",
      "  policy_reward_min:\n",
      "    policy0: -7.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08607658997781899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05635912641986186\n",
      "    mean_inference_ms: 1.2641083811909517\n",
      "    mean_raw_obs_processing_ms: 0.33352805312087686\n",
      "  time_since_restore: 138.42881679534912\n",
      "  time_this_iter_s: 12.633166313171387\n",
      "  time_total_s: 138.42881679534912\n",
      "  timers:\n",
      "    learn_throughput: 727.424\n",
      "    learn_time_ms: 5498.856\n",
      "    load_throughput: 7612512.364\n",
      "    load_time_ms: 0.525\n",
      "    sample_throughput: 315.519\n",
      "    sample_time_ms: 12677.52\n",
      "    update_time_ms: 3.041\n",
      "  timestamp: 1646562995\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:39 (running for 00:02:33.11)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         138.429</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> 8.35756</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           5.82122</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:44 (running for 00:02:38.20)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         138.429</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> 8.35756</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           5.82122</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-36-47\n",
      "  done: false\n",
      "  episode_len_mean: 5.809593023255814\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.380813953488373\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 688\n",
      "  episodes_total: 6947\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4715823531150818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006364115048199892\n",
      "          model: {}\n",
      "          policy_loss: -0.04019234701991081\n",
      "          total_loss: 4.656186103820801\n",
      "          vf_explained_var: 0.7781650424003601\n",
      "          vf_loss: 4.694231033325195\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5650336146354675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004593200050294399\n",
      "          model: {}\n",
      "          policy_loss: 0.005038667470216751\n",
      "          total_loss: 5.222914695739746\n",
      "          vf_explained_var: 0.6053583025932312\n",
      "          vf_loss: 5.2163262367248535\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.716666666666665\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 12.824127906976743\n",
      "    policy1: -4.443313953488372\n",
      "  policy_reward_min:\n",
      "    policy0: -7.0\n",
      "    policy1: -8.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08607074118229736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05635236025388032\n",
      "    mean_inference_ms: 1.2656991246993265\n",
      "    mean_raw_obs_processing_ms: 0.33515867214719836\n",
      "  time_since_restore: 151.15691542625427\n",
      "  time_this_iter_s: 12.728098630905151\n",
      "  time_total_s: 151.15691542625427\n",
      "  timers:\n",
      "    learn_throughput: 726.877\n",
      "    learn_time_ms: 5502.993\n",
      "    load_throughput: 7652442.985\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 315.83\n",
      "    sample_time_ms: 12665.034\n",
      "    update_time_ms: 3.048\n",
      "  timestamp: 1646563007\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:49 (running for 00:02:43.90)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         151.157</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> 8.38081</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           5.80959</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:54 (running for 00:02:48.98)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         151.157</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> 8.38081</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           5.80959</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:36:59 (running for 00:02:53.99)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         151.157</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> 8.38081</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           5.80959</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 5.918639053254438\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.162721893491124\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 676\n",
      "  episodes_total: 7623\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.426715224981308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008138692937791348\n",
      "          model: {}\n",
      "          policy_loss: -0.02577308379113674\n",
      "          total_loss: 3.28844952583313\n",
      "          vf_explained_var: 0.8221600651741028\n",
      "          vf_loss: 3.3114757537841797\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5372871160507202\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005254887975752354\n",
      "          model: {}\n",
      "          policy_loss: -0.005147289950400591\n",
      "          total_loss: 3.3228156566619873\n",
      "          vf_explained_var: 0.6709563136100769\n",
      "          vf_loss: 3.327075958251953\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.344444444444445\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.282544378698224\n",
      "    policy1: -5.1198224852071\n",
      "  policy_reward_min:\n",
      "    policy0: 1.0\n",
      "    policy1: -8.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08602022301708057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05627334105280972\n",
      "    mean_inference_ms: 1.26617859538726\n",
      "    mean_raw_obs_processing_ms: 0.3362244632261945\n",
      "  time_since_restore: 163.78928661346436\n",
      "  time_this_iter_s: 12.632371187210083\n",
      "  time_total_s: 163.78928661346436\n",
      "  timers:\n",
      "    learn_throughput: 726.665\n",
      "    learn_time_ms: 5504.598\n",
      "    load_throughput: 7660129.669\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 315.225\n",
      "    sample_time_ms: 12689.343\n",
      "    update_time_ms: 3.053\n",
      "  timestamp: 1646563020\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:05 (running for 00:02:59.68)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         163.789</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> 8.16272</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           5.91864</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:10 (running for 00:03:04.68)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         163.789</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> 8.16272</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           5.91864</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 5.950892857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.098214285714286\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 672\n",
      "  episodes_total: 8295\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3962229788303375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004285112023353577\n",
      "          model: {}\n",
      "          policy_loss: -0.017291121184825897\n",
      "          total_loss: 2.334390640258789\n",
      "          vf_explained_var: 0.8761123418807983\n",
      "          vf_loss: 2.3502357006073\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5446244478225708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005758595187216997\n",
      "          model: {}\n",
      "          policy_loss: -0.007352281361818314\n",
      "          total_loss: 2.3387668132781982\n",
      "          vf_explained_var: 0.7442353367805481\n",
      "          vf_loss: 2.3451476097106934\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.76111111111111\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.52827380952381\n",
      "    policy1: -5.430059523809524\n",
      "  policy_reward_min:\n",
      "    policy0: -7.0\n",
      "    policy1: -6.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08602121983619058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05622260990007609\n",
      "    mean_inference_ms: 1.267720596255712\n",
      "    mean_raw_obs_processing_ms: 0.3371792170314333\n",
      "  time_since_restore: 176.4929494857788\n",
      "  time_this_iter_s: 12.703662872314453\n",
      "  time_total_s: 176.4929494857788\n",
      "  timers:\n",
      "    learn_throughput: 726.431\n",
      "    learn_time_ms: 5506.372\n",
      "    load_throughput: 7836524.826\n",
      "    load_time_ms: 0.51\n",
      "    sample_throughput: 314.492\n",
      "    sample_time_ms: 12718.94\n",
      "    update_time_ms: 3.098\n",
      "  timestamp: 1646563033\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:16 (running for 00:03:10.45)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         176.493</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> 8.09821</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           5.95089</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:21 (running for 00:03:15.45)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         176.493</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> 8.09821</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">           5.95089</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-37-25\n",
      "  done: false\n",
      "  episode_len_mean: 5.986526946107785\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.02694610778443\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 668\n",
      "  episodes_total: 8963\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.373714417219162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009819568134844303\n",
      "          model: {}\n",
      "          policy_loss: -0.01877244934439659\n",
      "          total_loss: 1.1385146379470825\n",
      "          vf_explained_var: 0.9281362891197205\n",
      "          vf_loss: 1.155630111694336\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5128410458564758\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008728351444005966\n",
      "          model: {}\n",
      "          policy_loss: -0.004787984304130077\n",
      "          total_loss: 1.1910216808319092\n",
      "          vf_explained_var: 0.7988761067390442\n",
      "          vf_loss: 1.1943367719650269\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.142105263157895\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.758982035928144\n",
      "    policy1: -5.732035928143713\n",
      "  policy_reward_min:\n",
      "    policy0: 3.0\n",
      "    policy1: -8.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08598630168958855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05613548179755272\n",
      "    mean_inference_ms: 1.267396494364\n",
      "    mean_raw_obs_processing_ms: 0.3378724807902271\n",
      "  time_since_restore: 189.1089313030243\n",
      "  time_this_iter_s: 12.615981817245483\n",
      "  time_total_s: 189.1089313030243\n",
      "  timers:\n",
      "    learn_throughput: 725.77\n",
      "    learn_time_ms: 5511.39\n",
      "    load_throughput: 7835426.863\n",
      "    load_time_ms: 0.511\n",
      "    sample_throughput: 314.52\n",
      "    sample_time_ms: 12717.784\n",
      "    update_time_ms: 3.078\n",
      "  timestamp: 1646563045\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:27 (running for 00:03:21.13)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         189.109</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> 8.02695</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           5.98653</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:32 (running for 00:03:26.13)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         189.109</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> 8.02695</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           5.98653</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:37 (running for 00:03:31.14)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         189.109</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> 8.02695</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           5.98653</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-37-38\n",
      "  done: false\n",
      "  episode_len_mean: 6.004504504504505\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 7.990990990990991\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 666\n",
      "  episodes_total: 9629\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3213249146938324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006361422594636679\n",
      "          model: {}\n",
      "          policy_loss: -0.012260351330041885\n",
      "          total_loss: 1.2037289142608643\n",
      "          vf_explained_var: 0.9278494715690613\n",
      "          vf_loss: 1.2149157524108887\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4525774121284485\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008651058189570904\n",
      "          model: {}\n",
      "          policy_loss: -0.007542809005826712\n",
      "          total_loss: 1.0713318586349487\n",
      "          vf_explained_var: 0.8370316624641418\n",
      "          vf_loss: 1.077414870262146\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.044444444444444\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.785285285285285\n",
      "    policy1: -5.7942942942942945\n",
      "  policy_reward_min:\n",
      "    policy0: 3.0\n",
      "    policy1: -8.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08599224061668374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05610547466793348\n",
      "    mean_inference_ms: 1.2682160054018183\n",
      "    mean_raw_obs_processing_ms: 0.3385302800725855\n",
      "  time_since_restore: 201.79154205322266\n",
      "  time_this_iter_s: 12.682610750198364\n",
      "  time_total_s: 201.79154205322266\n",
      "  timers:\n",
      "    learn_throughput: 725.255\n",
      "    learn_time_ms: 5515.298\n",
      "    load_throughput: 7805534.568\n",
      "    load_time_ms: 0.512\n",
      "    sample_throughput: 314.379\n",
      "    sample_time_ms: 12723.491\n",
      "    update_time_ms: 3.05\n",
      "  timestamp: 1646563058\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:42 (running for 00:03:36.79)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         201.792</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> 7.99099</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">            6.0045</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:47 (running for 00:03:41.90)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         201.792</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> 7.99099</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">            6.0045</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-37-51\n",
      "  done: false\n",
      "  episode_len_mean: 5.994011976047904\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.011976047904191\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 668\n",
      "  episodes_total: 10297\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27376291155815125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00534240435808897\n",
      "          model: {}\n",
      "          policy_loss: -0.010850643739104271\n",
      "          total_loss: 0.4971477687358856\n",
      "          vf_explained_var: 0.96705162525177\n",
      "          vf_loss: 0.5070968866348267\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4094758629798889\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034032289404422045\n",
      "          model: {}\n",
      "          policy_loss: -0.0034279755782335997\n",
      "          total_loss: 0.5025619268417358\n",
      "          vf_explained_var: 0.9115393161773682\n",
      "          vf_loss: 0.5054156184196472\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.294444444444444\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.901197604790418\n",
      "    policy1: -5.889221556886228\n",
      "  policy_reward_min:\n",
      "    policy0: 3.0\n",
      "    policy1: -8.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08599934259798016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.056070383239378976\n",
      "    mean_inference_ms: 1.2686915212591872\n",
      "    mean_raw_obs_processing_ms: 0.3391430186211376\n",
      "  time_since_restore: 214.4745168685913\n",
      "  time_this_iter_s: 12.682974815368652\n",
      "  time_total_s: 214.4745168685913\n",
      "  timers:\n",
      "    learn_throughput: 724.975\n",
      "    learn_time_ms: 5517.428\n",
      "    load_throughput: 7844585.963\n",
      "    load_time_ms: 0.51\n",
      "    sample_throughput: 314.521\n",
      "    sample_time_ms: 12717.745\n",
      "    update_time_ms: 3.077\n",
      "  timestamp: 1646563071\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:53 (running for 00:03:47.53)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         214.475</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> 8.01198</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           5.99401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:37:58 (running for 00:03:52.64)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         214.475</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> 8.01198</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           5.99401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:38:03 (running for 00:03:57.64)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         214.475</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> 8.01198</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           5.99401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-38-04\n",
      "  done: false\n",
      "  episode_len_mean: 6.016541353383459\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 7.966917293233083\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 665\n",
      "  episodes_total: 10962\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16875000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.26339468359947205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003279311116784811\n",
      "          model: {}\n",
      "          policy_loss: -0.009250523522496223\n",
      "          total_loss: 1.0983420610427856\n",
      "          vf_explained_var: 0.9371823668479919\n",
      "          vf_loss: 1.1070390939712524\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08437500149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.37885940074920654\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009814059361815453\n",
      "          model: {}\n",
      "          policy_loss: -0.006505568511784077\n",
      "          total_loss: 0.789451003074646\n",
      "          vf_explained_var: 0.8884669542312622\n",
      "          vf_loss: 0.7951285243034363\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.905555555555555\n",
      "    ram_util_percent: 29.600000000000005\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 4.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.84812030075188\n",
      "    policy1: -5.881203007518797\n",
      "  policy_reward_min:\n",
      "    policy0: 2.0\n",
      "    policy1: -8.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08600088386452465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.056034912574761606\n",
      "    mean_inference_ms: 1.2684896011120481\n",
      "    mean_raw_obs_processing_ms: 0.3396094626011868\n",
      "  time_since_restore: 227.11656737327576\n",
      "  time_this_iter_s: 12.642050504684448\n",
      "  time_total_s: 227.11656737327576\n",
      "  timers:\n",
      "    learn_throughput: 724.407\n",
      "    learn_time_ms: 5521.757\n",
      "    load_throughput: 7943757.576\n",
      "    load_time_ms: 0.504\n",
      "    sample_throughput: 314.365\n",
      "    sample_time_ms: 12724.078\n",
      "    update_time_ms: 3.12\n",
      "  timestamp: 1646563084\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:38:09 (running for 00:04:03.34)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         227.117</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> 7.96692</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           6.01654</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:38:14 (running for 00:04:08.34)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         227.117</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> 7.96692</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           6.01654</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-38-16\n",
      "  done: false\n",
      "  episode_len_mean: 6.007518796992481\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 7.984962406015038\n",
      "  episode_reward_min: 6.0\n",
      "  episodes_this_iter: 665\n",
      "  episodes_total: 11627\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08437500149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.22511355578899384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00565635971724987\n",
      "          model: {}\n",
      "          policy_loss: -0.008726169355213642\n",
      "          total_loss: 0.6411762833595276\n",
      "          vf_explained_var: 0.9607021808624268\n",
      "          vf_loss: 0.649425208568573\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08437500149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3637952208518982\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006688469555228949\n",
      "          model: {}\n",
      "          policy_loss: -0.003985689952969551\n",
      "          total_loss: 0.4463050067424774\n",
      "          vf_explained_var: 0.9347082376480103\n",
      "          vf_loss: 0.449726402759552\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.244444444444444\n",
      "    ram_util_percent: 29.61111111111111\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 3.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.917293233082706\n",
      "    policy1: -5.932330827067669\n",
      "  policy_reward_min:\n",
      "    policy0: 3.0\n",
      "    policy1: -6.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08596881571773593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05599466167426913\n",
      "    mean_inference_ms: 1.2685321205247477\n",
      "    mean_raw_obs_processing_ms: 0.34003378151263075\n",
      "  time_since_restore: 239.79260540008545\n",
      "  time_this_iter_s: 12.676038026809692\n",
      "  time_total_s: 239.79260540008545\n",
      "  timers:\n",
      "    learn_throughput: 724.84\n",
      "    learn_time_ms: 5518.456\n",
      "    load_throughput: 8039300.398\n",
      "    load_time_ms: 0.498\n",
      "    sample_throughput: 314.423\n",
      "    sample_time_ms: 12721.734\n",
      "    update_time_ms: 3.116\n",
      "  timestamp: 1646563096\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:38:20 (running for 00:04:14.09)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         239.793</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> 7.98496</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">           6.00752</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:38:25 (running for 00:04:19.09)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>RUNNING </td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         239.793</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> 7.98496</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">           6.00752</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPOTrainer_foodenv_f2dcd_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-03-06_02-38-29\n",
      "  done: true\n",
      "  episode_len_mean: 5.994011976047904\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 8.011976047904191\n",
      "  episode_reward_min: 6.0\n",
      "  episodes_this_iter: 668\n",
      "  episodes_total: 12295\n",
      "  experiment_id: 5e56b41f05ce4fff83faa7e71123084e\n",
      "  hostname: 310c4782fa76\n",
      "  info:\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08437500149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.21561482548713684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004171359818428755\n",
      "          model: {}\n",
      "          policy_loss: -0.00849121529608965\n",
      "          total_loss: 0.2004651129245758\n",
      "          vf_explained_var: 0.9855367541313171\n",
      "          vf_loss: 0.2086043655872345\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08437500149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.34342655539512634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007013346534222364\n",
      "          model: {}\n",
      "          policy_loss: -0.004162473604083061\n",
      "          total_loss: 0.2211611121892929\n",
      "          vf_explained_var: 0.9595999717712402\n",
      "          vf_loss: 0.22473183274269104\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.777777777777775\n",
      "    ram_util_percent: 29.700000000000003\n",
      "  pid: 3770\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.946107784431138\n",
      "    policy1: -5.934131736526946\n",
      "  policy_reward_min:\n",
      "    policy0: 3.0\n",
      "    policy1: -6.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08596259786704752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05596985592070052\n",
      "    mean_inference_ms: 1.268217744556788\n",
      "    mean_raw_obs_processing_ms: 0.3405307790946695\n",
      "  time_since_restore: 252.38456273078918\n",
      "  time_this_iter_s: 12.591957330703735\n",
      "  time_total_s: 252.38456273078918\n",
      "  timers:\n",
      "    learn_throughput: 725.098\n",
      "    learn_time_ms: 5516.497\n",
      "    load_throughput: 8043540.128\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 314.346\n",
      "    sample_time_ms: 12724.822\n",
      "    update_time_ms: 3.124\n",
      "  timestamp: 1646563109\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: f2dcd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-06 02:38:29 (running for 00:04:23.74)<br>Memory usage on this node: 4.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/9.11 GiB heap, 0.0/4.55 GiB objects<br>Result logdir: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPOTrainer_foodenv_f2dcd_00000</td><td>TERMINATED</td><td>172.17.0.2:3770</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         252.385</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> 8.01198</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">           5.99401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m 2022-03-06 02:38:29,769\tERROR worker.py:432 -- SystemExit was raised from the worker.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 639, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1156, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1235, in exit_actor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"python/ray/_raylet.pyx\", line 770, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"python/ray/_raylet.pyx\", line 591, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 121, in format_exception\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     capture_locals=capture_locals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 363, in extract\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 285, in line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 137, in updatecache\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     lines = fp.readlines()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/codecs.py\", line 319, in decode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     def decode(self, input, final=False):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 429, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3769)\u001b[0m SystemExit: 1\n",
      "2022-03-06 02:38:29,875\tINFO tune.py:636 -- Total run time: 263.89 seconds (263.63 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "e = FoodGame(comp_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"timesteps_per_iteration\"] = 1000\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = comp_env_config\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"training_iteration\": 20}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e399f68",
   "metadata": {},
   "source": [
    "# Results Analysis (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fa6fae4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3855)\u001b[0m 2022-03-06 02:38:46,097\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-06 02:38:47,481\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-06 02:38:47,583\tINFO trainable.py:473 -- Restored on 172.17.0.2 from checkpoint: /home/ray/cs4246/06_03_2022_02:34:05/PPOTrainer_2022-03-06_02-34-05/PPOTrainer_foodenv_f2dcd_00000_0_2022-03-06_02-34-06/checkpoint_000020/checkpoint-20\n",
      "2022-03-06 02:38:47,584\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 80000, '_time_total': 252.38456273078918, '_episodes_total': 12295}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state at Time 0:\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 1\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 2\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '1' '' '' '' '']]\n",
      "\n",
      "Time: 3\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '1' '' '' '' '' '']]\n",
      "\n",
      "Time: 4\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '1' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 5\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '1' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 6\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '1' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode Ended\n",
      "Episode Rewards {'agent0': 14, 'agent1': -6}\n"
     ]
    }
   ],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f724e43",
   "metadata": {},
   "source": [
    "# DQN Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d964d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coop Environment\n",
    "coop_env_config['trainer'] = 'dqn'\n",
    "\n",
    "# Competitive Environment\n",
    "comp_env_config['trainer'] = 'dqn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b9ead0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:11:22 (running for 00:00:00.13)<br>Memory usage on this node: 4.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQNTrainer pid=19093)\u001b[0m 2022-04-01 04:11:26,065\tINFO trainer.py:2055 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=19093)\u001b[0m 2022-04-01 04:11:26,067\tINFO simple_q.py:154 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=19093)\u001b[0m 2022-04-01 04:11:26,067\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=19093)\u001b[0m 2022-04-01 04:11:26,067\tWARNING deprecation.py:46 -- DeprecationWarning: `config['buffer_size']` has been deprecated. Use `config['replay_buffer_config']['capacity']` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m 2022-04-01 04:11:31,511\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:11:33 (running for 00:00:11.63)<br>Memory usage on this node: 5.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQNTrainer pid=19093)\u001b[0m 2022-04-01 04:11:33,772\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m 2022-04-01 04:11:33,845\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:11:34 (running for 00:00:12.64)<br>Memory usage on this node: 5.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 2028\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-11-36\n",
      "  done: false\n",
      "  episode_len_mean: 17.789473684210527\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -12.771929824561404\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 57\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 1014\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 0.32941052317619324\n",
      "          mean_q: -0.3010759949684143\n",
      "          mean_td_error: -0.3353292942047119\n",
      "          min_q: -0.9126817584037781\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.7900476455688477\n",
      "        - -9.476641654968262\n",
      "        - 0.18028044700622559\n",
      "        - 1.2315980195999146\n",
      "        - 0.38541746139526367\n",
      "        - 1.1500840187072754\n",
      "        - -9.660634994506836\n",
      "        - 0.5378077030181885\n",
      "        - 1.0408021211624146\n",
      "        - 0.9927923679351807\n",
      "        - 0.786128044128418\n",
      "        - 0.9665673971176147\n",
      "        - 0.4723970890045166\n",
      "        - 0.6946125030517578\n",
      "        - 0.08596590161323547\n",
      "        - 1.1840133666992188\n",
      "        - 1.104079246520996\n",
      "        - 0.25887781381607056\n",
      "        - 0.18028044700622559\n",
      "        - 0.5596625804901123\n",
      "        - 0.2367616891860962\n",
      "        - 0.5203996896743774\n",
      "        - 0.5338175296783447\n",
      "        - 0.23175948858261108\n",
      "        - 0.464324951171875\n",
      "        - 0.36813053488731384\n",
      "        - 0.6917102336883545\n",
      "        - 0.6974208950996399\n",
      "        - -10.030817031860352\n",
      "        - 0.6809210777282715\n",
      "        - 0.26203423738479614\n",
      "        - 1.1488627195358276\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 0.6936180591583252\n",
      "          mean_q: 0.14957398176193237\n",
      "          mean_td_error: 0.6002287864685059\n",
      "          min_q: -0.3788982629776001\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.2182807922363281\n",
      "        - 1.4353182315826416\n",
      "        - 0.7136898636817932\n",
      "        - 1.3576122522354126\n",
      "        - 0.9455446004867554\n",
      "        - -8.598501205444336\n",
      "        - 1.4573473930358887\n",
      "        - 0.5085705518722534\n",
      "        - 1.5434942245483398\n",
      "        - -8.971863746643066\n",
      "        - 1.9145948886871338\n",
      "        - 1.014810562133789\n",
      "        - 1.017248511314392\n",
      "        - 1.1961796283721924\n",
      "        - 1.7883769273757935\n",
      "        - 1.602455496788025\n",
      "        - 0.9528239965438843\n",
      "        - 1.3623442649841309\n",
      "        - 1.5361366271972656\n",
      "        - 0.6778309345245361\n",
      "        - 1.3556532859802246\n",
      "        - 0.8391449451446533\n",
      "        - 1.0179247856140137\n",
      "        - 1.6407544612884521\n",
      "        - 1.2887141704559326\n",
      "        - 1.295971393585205\n",
      "        - 1.6419486999511719\n",
      "        - 1.1149064302444458\n",
      "        - 0.6906952261924744\n",
      "        - 1.4714542627334595\n",
      "        - 0.4216254949569702\n",
      "        - 1.7562347650527954\n",
      "    num_agent_steps_sampled: 2028\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1014\n",
      "    num_steps_trained: 32\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.68\n",
      "    ram_util_percent: 32.82\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -6.385964912280702\n",
      "    policy1: -6.385964912280702\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09836304951183904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06566587927306228\n",
      "    mean_inference_ms: 1.5777766410940386\n",
      "    mean_raw_obs_processing_ms: 0.27786804537467763\n",
      "  time_since_restore: 2.893098831176758\n",
      "  time_this_iter_s: 2.893098831176758\n",
      "  time_total_s: 2.893098831176758\n",
      "  timers:\n",
      "    learn_throughput: 102.351\n",
      "    learn_time_ms: 312.649\n",
      "    load_throughput: 70752.624\n",
      "    load_time_ms: 0.452\n",
      "    update_time_ms: 6.339\n",
      "  timestamp: 1648811496\n",
      "  timesteps_since_restore: 32\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 1014\n",
      "  training_iteration: 1\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:11:40 (running for 00:00:17.99)<br>Memory usage on this node: 5.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         5.19381</td><td style=\"text-align: right;\">1670</td><td style=\"text-align: right;\">-14.5652</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           18.1522</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 5208\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-11-42\n",
      "  done: false\n",
      "  episode_len_mean: 18.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: -15.88\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 143\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 2524\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -8.597418785095215\n",
      "          mean_q: -10.731786727905273\n",
      "          mean_td_error: -3.298344612121582\n",
      "          min_q: -12.186394691467285\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.596226692199707\n",
      "        - -7.484169006347656\n",
      "        - 0.13162803649902344\n",
      "        - -10.007979393005371\n",
      "        - -9.990279197692871\n",
      "        - -11.101592063903809\n",
      "        - -10.084178924560547\n",
      "        - 0.29924488067626953\n",
      "        - -9.368812561035156\n",
      "        - -0.05985450744628906\n",
      "        - -7.387759208679199\n",
      "        - 0.8220911026000977\n",
      "        - -7.21031379699707\n",
      "        - 0.5892457962036133\n",
      "        - -0.18993091583251953\n",
      "        - -1.1655292510986328\n",
      "        - -8.126008033752441\n",
      "        - -20.82125473022461\n",
      "        - -0.5886878967285156\n",
      "        - -6.305870056152344\n",
      "        - 0.5177526473999023\n",
      "        - 0.38411426544189453\n",
      "        - -0.33321475982666016\n",
      "        - -0.0759124755859375\n",
      "        - 0.17977619171142578\n",
      "        - 0.39060020446777344\n",
      "        - 0.45183849334716797\n",
      "        - 0.2927970886230469\n",
      "        - -0.05545997619628906\n",
      "        - 0.7180700302124023\n",
      "        - -0.1396198272705078\n",
      "        - -0.42398929595947266\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -7.281827449798584\n",
      "          mean_q: -8.545185089111328\n",
      "          mean_td_error: -5.41909646987915\n",
      "          min_q: -10.146405220031738\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.0017833709716796875\n",
      "        - -0.38973093032836914\n",
      "        - -26.921770095825195\n",
      "        - 0.16838645935058594\n",
      "        - -7.479426383972168\n",
      "        - 0.07230377197265625\n",
      "        - -8.935176849365234\n",
      "        - -0.24190235137939453\n",
      "        - -17.143455505371094\n",
      "        - 0.23551082611083984\n",
      "        - 0.4622201919555664\n",
      "        - 0.14694881439208984\n",
      "        - -17.330320358276367\n",
      "        - -7.673189640045166\n",
      "        - -0.2721223831176758\n",
      "        - -0.17719650268554688\n",
      "        - -8.393420219421387\n",
      "        - -0.06232929229736328\n",
      "        - -0.1744098663330078\n",
      "        - -10.177694320678711\n",
      "        - -8.456449508666992\n",
      "        - -9.755109786987305\n",
      "        - -16.316293716430664\n",
      "        - -8.685224533081055\n",
      "        - -0.03882789611816406\n",
      "        - -0.01798534393310547\n",
      "        - -0.07851791381835938\n",
      "        - -18.984224319458008\n",
      "        - -0.14815282821655273\n",
      "        - 1.2757539749145508\n",
      "        - -8.257705688476562\n",
      "        - 0.3402128219604492\n",
      "    num_agent_steps_sampled: 5208\n",
      "    num_agent_steps_trained: 5568\n",
      "    num_steps_sampled: 2604\n",
      "    num_steps_trained: 2784\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 14\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.6\n",
      "    ram_util_percent: 33.2\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.94\n",
      "    policy1: -7.94\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09822790363062303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06552347173371326\n",
      "    mean_inference_ms: 1.5369737254675646\n",
      "    mean_raw_obs_processing_ms: 0.2764786729873554\n",
      "  time_since_restore: 8.552220821380615\n",
      "  time_this_iter_s: 1.0137860774993896\n",
      "  time_total_s: 8.552220821380615\n",
      "  timers:\n",
      "    learn_throughput: 4888.68\n",
      "    learn_time_ms: 6.546\n",
      "    load_throughput: 75107.85\n",
      "    load_time_ms: 0.426\n",
      "    update_time_ms: 4.913\n",
      "  timestamp: 1648811502\n",
      "  timesteps_since_restore: 192\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 2604\n",
      "  training_iteration: 6\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:11:45 (running for 00:00:23.79)<br>Memory usage on this node: 5.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         10.7422</td><td style=\"text-align: right;\">3235</td><td style=\"text-align: right;\">  -15.64</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 8456\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 18.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.0\n",
      "  episode_reward_mean: -14.68\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 232\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 4228\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -8.534445762634277\n",
      "          mean_q: -14.113641738891602\n",
      "          mean_td_error: -3.7987375259399414\n",
      "          min_q: -17.65914535522461\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -13.362943649291992\n",
      "        - -3.289889335632324\n",
      "        - 0.3116006851196289\n",
      "        - 0.6304130554199219\n",
      "        - -25.761676788330078\n",
      "        - 0.23036861419677734\n",
      "        - -0.06643867492675781\n",
      "        - -3.408297538757324\n",
      "        - -10.604764938354492\n",
      "        - -3.455242156982422\n",
      "        - -14.681010246276855\n",
      "        - 0.8519630432128906\n",
      "        - 1.5349140167236328\n",
      "        - -15.191579818725586\n",
      "        - 1.326873779296875\n",
      "        - 0.6334991455078125\n",
      "        - 0.9790019989013672\n",
      "        - 0.8817005157470703\n",
      "        - 0.3104085922241211\n",
      "        - 1.673370361328125\n",
      "        - 0.21025466918945312\n",
      "        - -0.08461189270019531\n",
      "        - -14.270012855529785\n",
      "        - 0.204132080078125\n",
      "        - 0.18001174926757812\n",
      "        - 0.8783140182495117\n",
      "        - 0.80828857421875\n",
      "        - -0.7162799835205078\n",
      "        - -0.8040952682495117\n",
      "        - 0.2504768371582031\n",
      "        - -12.104205131530762\n",
      "        - -15.654144287109375\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -6.154556751251221\n",
      "          mean_q: -10.622922897338867\n",
      "          mean_td_error: -1.8292286396026611\n",
      "          min_q: -15.301863670349121\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.6664524078369141\n",
      "        - 0.4988279342651367\n",
      "        - -13.253634452819824\n",
      "        - 0.9155254364013672\n",
      "        - 0.16834640502929688\n",
      "        - 1.0306987762451172\n",
      "        - -0.14087677001953125\n",
      "        - -21.657852172851562\n",
      "        - 0.2752218246459961\n",
      "        - 1.0849628448486328\n",
      "        - -3.8917832374572754\n",
      "        - -0.4508676528930664\n",
      "        - 0.9861955642700195\n",
      "        - 2.5765743255615234\n",
      "        - 1.19622802734375\n",
      "        - -0.8311381340026855\n",
      "        - -0.6647429466247559\n",
      "        - -19.937049865722656\n",
      "        - -0.9991121292114258\n",
      "        - 0.11682510375976562\n",
      "        - 0.7408437728881836\n",
      "        - 0.9130115509033203\n",
      "        - 1.178776741027832\n",
      "        - -0.40150928497314453\n",
      "        - -4.6825151443481445\n",
      "        - -7.219588279724121\n",
      "        - 0.742243766784668\n",
      "        - 1.5357255935668945\n",
      "        - 0.4805746078491211\n",
      "        - 0.5720977783203125\n",
      "        - 0.4734764099121094\n",
      "        - -0.5572514533996582\n",
      "    num_agent_steps_sampled: 8456\n",
      "    num_agent_steps_trained: 11264\n",
      "    num_steps_sampled: 4228\n",
      "    num_steps_trained: 5632\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 29\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.299999999999997\n",
      "    ram_util_percent: 33.2\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.34\n",
      "    policy1: -7.34\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09849705859575182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06548017617456639\n",
      "    mean_inference_ms: 1.5107987927903355\n",
      "    mean_raw_obs_processing_ms: 0.27673598483255213\n",
      "  time_since_restore: 14.476135492324829\n",
      "  time_this_iter_s: 1.3706932067871094\n",
      "  time_total_s: 14.476135492324829\n",
      "  timers:\n",
      "    learn_throughput: 4084.744\n",
      "    learn_time_ms: 7.834\n",
      "    load_throughput: 71203.039\n",
      "    load_time_ms: 0.449\n",
      "    update_time_ms: 4.947\n",
      "  timestamp: 1648811508\n",
      "  timesteps_since_restore: 352\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 4228\n",
      "  training_iteration: 11\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:11:51 (running for 00:00:29.04)<br>Memory usage on this node: 5.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">          16.715</td><td style=\"text-align: right;\">4764</td><td style=\"text-align: right;\">  -11.74</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 11464\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-11-54\n",
      "  done: false\n",
      "  episode_len_mean: 16.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -9.1\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 321\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 5672\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -10.030786514282227\n",
      "          mean_q: -16.82680320739746\n",
      "          mean_td_error: -4.2961015701293945\n",
      "          min_q: -21.93215560913086\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -2.808797836303711\n",
      "        - 0.4657630920410156\n",
      "        - -19.979211807250977\n",
      "        - -0.42719078063964844\n",
      "        - 0.8417816162109375\n",
      "        - -8.312045097351074\n",
      "        - -27.975433349609375\n",
      "        - -1.5690927505493164\n",
      "        - 0.6160526275634766\n",
      "        - 2.6054859161376953\n",
      "        - -19.156505584716797\n",
      "        - -14.09277057647705\n",
      "        - 2.4155445098876953\n",
      "        - -18.3916072845459\n",
      "        - -1.2328977584838867\n",
      "        - 0.2362842559814453\n",
      "        - -0.6323890686035156\n",
      "        - -12.431443214416504\n",
      "        - -1.0051345825195312\n",
      "        - 0.6645145416259766\n",
      "        - 1.5561294555664062\n",
      "        - -0.6916618347167969\n",
      "        - -1.829315185546875\n",
      "        - -1.6801605224609375\n",
      "        - 0.09070968627929688\n",
      "        - -0.6805934906005859\n",
      "        - -0.743499755859375\n",
      "        - 0.05828285217285156\n",
      "        - -1.4581413269042969\n",
      "        - 3.009139060974121\n",
      "        - -14.260163307189941\n",
      "        - -0.6768856048583984\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -11.448465347290039\n",
      "          mean_q: -16.16550064086914\n",
      "          mean_td_error: -3.8977208137512207\n",
      "          min_q: -20.341678619384766\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.46738433837890625\n",
      "        - 0.7161350250244141\n",
      "        - 0.7293510437011719\n",
      "        - -17.674949645996094\n",
      "        - -0.24491214752197266\n",
      "        - 0.3612995147705078\n",
      "        - -16.64983367919922\n",
      "        - 0.0975494384765625\n",
      "        - 0.7267313003540039\n",
      "        - 0.6912899017333984\n",
      "        - 0.09404277801513672\n",
      "        - 0.48924827575683594\n",
      "        - 0.4037322998046875\n",
      "        - -0.4095296859741211\n",
      "        - -24.66193389892578\n",
      "        - -0.2600412368774414\n",
      "        - -1.2013158798217773\n",
      "        - -27.63812828063965\n",
      "        - -15.994804382324219\n",
      "        - 0.09066390991210938\n",
      "        - -0.27558326721191406\n",
      "        - 0.17093658447265625\n",
      "        - -17.54495620727539\n",
      "        - -2.3153076171875\n",
      "        - -0.4071331024169922\n",
      "        - -0.5581550598144531\n",
      "        - -3.647113800048828\n",
      "        - -0.575892448425293\n",
      "        - 0.7411746978759766\n",
      "        - 0.10299873352050781\n",
      "        - 0.1263742446899414\n",
      "        - 0.2583808898925781\n",
      "    num_agent_steps_sampled: 11464\n",
      "    num_agent_steps_trained: 16960\n",
      "    num_steps_sampled: 5732\n",
      "    num_steps_trained: 8480\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 42\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.45\n",
      "    ram_util_percent: 34.65\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -4.55\n",
      "    policy1: -4.55\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10254285413832108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06802150442774746\n",
      "    mean_inference_ms: 1.5555298117835008\n",
      "    mean_raw_obs_processing_ms: 0.2837210056523879\n",
      "  time_since_restore: 20.41497015953064\n",
      "  time_this_iter_s: 1.174222469329834\n",
      "  time_total_s: 20.41497015953064\n",
      "  timers:\n",
      "    learn_throughput: 4357.435\n",
      "    learn_time_ms: 7.344\n",
      "    load_throughput: 70462.898\n",
      "    load_time_ms: 0.454\n",
      "    update_time_ms: 4.53\n",
      "  timestamp: 1648811514\n",
      "  timesteps_since_restore: 512\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 5732\n",
      "  training_iteration: 16\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:11:56 (running for 00:00:34.11)<br>Memory usage on this node: 5.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         21.5637</td><td style=\"text-align: right;\">6043</td><td style=\"text-align: right;\">  -10.06</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             16.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 14004\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 16.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -9.28\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 398\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 7002\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 5.031968116760254\n",
      "          mean_q: -11.484315872192383\n",
      "          mean_td_error: -0.9186485409736633\n",
      "          min_q: -21.814868927001953\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -14.9752836227417\n",
      "        - -2.2466869354248047\n",
      "        - 2.3300304412841797\n",
      "        - -8.78941535949707\n",
      "        - 2.412179946899414\n",
      "        - -1.2233190536499023\n",
      "        - -0.7419300079345703\n",
      "        - 1.7512664794921875\n",
      "        - 0.07807254791259766\n",
      "        - 2.923196792602539\n",
      "        - -6.019831657409668\n",
      "        - 2.4331722259521484\n",
      "        - 1.607461929321289\n",
      "        - -2.1782546043395996\n",
      "        - 2.1281747817993164\n",
      "        - 4.825434684753418\n",
      "        - 1.3660869598388672\n",
      "        - -7.821126937866211\n",
      "        - -1.5200109481811523\n",
      "        - 2.247952461242676\n",
      "        - -1.0144233703613281\n",
      "        - 1.3988447189331055\n",
      "        - 0.4415435791015625\n",
      "        - 1.6740360260009766\n",
      "        - -8.949118614196777\n",
      "        - 8.32409954071045\n",
      "        - -8.75405216217041\n",
      "        - 4.703823566436768\n",
      "        - 0.29230499267578125\n",
      "        - -0.6116142272949219\n",
      "        - 1.6827888488769531\n",
      "        - -7.172155380249023\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -3.468280076980591\n",
      "          mean_q: -15.57731819152832\n",
      "          mean_td_error: -4.703962326049805\n",
      "          min_q: -22.351451873779297\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.6506729125976562\n",
      "        - -19.073558807373047\n",
      "        - -20.76076889038086\n",
      "        - -15.617950439453125\n",
      "        - -0.7582225799560547\n",
      "        - 0.005370140075683594\n",
      "        - -9.447437286376953\n",
      "        - 0.5761232376098633\n",
      "        - -12.831185340881348\n",
      "        - 0.5095930099487305\n",
      "        - -0.1002960205078125\n",
      "        - 0.1800098419189453\n",
      "        - 0.9013748168945312\n",
      "        - -3.9100704193115234\n",
      "        - -5.523907661437988\n",
      "        - -10.50121021270752\n",
      "        - -4.078246593475342\n",
      "        - 0.28558921813964844\n",
      "        - 0.9924688339233398\n",
      "        - 0.1617259979248047\n",
      "        - -3.3578319549560547\n",
      "        - -10.782797813415527\n",
      "        - -26.740760803222656\n",
      "        - 1.351912498474121\n",
      "        - -2.5734100341796875\n",
      "        - -7.736167907714844\n",
      "        - -0.6328315734863281\n",
      "        - 0.27141380310058594\n",
      "        - -0.08599090576171875\n",
      "        - -1.167128562927246\n",
      "        - -0.09090995788574219\n",
      "        - -0.6423587799072266\n",
      "    num_agent_steps_sampled: 14004\n",
      "    num_agent_steps_trained: 21824\n",
      "    num_steps_sampled: 7002\n",
      "    num_steps_trained: 10912\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 54\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.35\n",
      "    ram_util_percent: 35.650000000000006\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -4.64\n",
      "    policy1: -4.64\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10330231740139423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06859122997578553\n",
      "    mean_inference_ms: 1.5647913831480098\n",
      "    mean_raw_obs_processing_ms: 0.2856073346286864\n",
      "  time_since_restore: 25.22793674468994\n",
      "  time_this_iter_s: 1.193052053451538\n",
      "  time_total_s: 25.22793674468994\n",
      "  timers:\n",
      "    learn_throughput: 4370.674\n",
      "    learn_time_ms: 7.322\n",
      "    load_throughput: 72374.078\n",
      "    load_time_ms: 0.442\n",
      "    update_time_ms: 4.615\n",
      "  timestamp: 1648811520\n",
      "  timesteps_since_restore: 640\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 7002\n",
      "  training_iteration: 20\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:01 (running for 00:00:39.27)<br>Memory usage on this node: 5.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         26.5001</td><td style=\"text-align: right;\">7330</td><td style=\"text-align: right;\">   -6.96</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             16.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 16614\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-05\n",
      "  done: false\n",
      "  episode_len_mean: 15.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -1.1\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 486\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 8307\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -0.1849309206008911\n",
      "          mean_q: -5.428021430969238\n",
      "          mean_td_error: -2.6723365783691406\n",
      "          min_q: -15.555228233337402\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -22.049835205078125\n",
      "        - 3.0053348541259766\n",
      "        - 0.9420056343078613\n",
      "        - -0.37282657623291016\n",
      "        - -1.3367996215820312\n",
      "        - -1.6943583488464355\n",
      "        - -4.551888942718506\n",
      "        - -1.0888638496398926\n",
      "        - -0.024297714233398438\n",
      "        - 2.3098950386047363\n",
      "        - -11.016603469848633\n",
      "        - 2.6244773864746094\n",
      "        - -0.8494076728820801\n",
      "        - -0.3099546432495117\n",
      "        - -10.325193405151367\n",
      "        - 0.31520748138427734\n",
      "        - 1.8465371131896973\n",
      "        - -1.3464903831481934\n",
      "        - 1.7572650909423828\n",
      "        - 0.3051316738128662\n",
      "        - -24.55522918701172\n",
      "        - -23.626972198486328\n",
      "        - 1.8903305530548096\n",
      "        - -0.8414766788482666\n",
      "        - 4.908670902252197\n",
      "        - -2.8152425289154053\n",
      "        - 0.0939779281616211\n",
      "        - -0.4161027669906616\n",
      "        - 0.7682557106018066\n",
      "        - 0.27039480209350586\n",
      "        - 1.4584331512451172\n",
      "        - -0.7891445159912109\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 0.0064585208892822266\n",
      "          mean_q: -7.201889991760254\n",
      "          mean_td_error: -2.2917845249176025\n",
      "          min_q: -14.262693405151367\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -15.323978424072266\n",
      "        - -2.19808292388916\n",
      "        - 0.18629586696624756\n",
      "        - -14.86697006225586\n",
      "        - 4.048973083496094\n",
      "        - 1.8654603958129883\n",
      "        - 1.634462833404541\n",
      "        - -9.503835678100586\n",
      "        - 0.13847887516021729\n",
      "        - 3.1055049896240234\n",
      "        - -0.8302445411682129\n",
      "        - -10.272035598754883\n",
      "        - 3.6054086685180664\n",
      "        - 1.5232133865356445\n",
      "        - 0.7623934745788574\n",
      "        - -6.092736721038818\n",
      "        - 2.800326108932495\n",
      "        - -12.186925888061523\n",
      "        - -0.130859375\n",
      "        - 2.187729835510254\n",
      "        - -0.31182193756103516\n",
      "        - 1.43536376953125\n",
      "        - 0.6744284629821777\n",
      "        - -8.993541717529297\n",
      "        - -1.8634757995605469\n",
      "        - 0.3192100524902344\n",
      "        - -3.098146438598633\n",
      "        - -0.4094209671020508\n",
      "        - -2.309415817260742\n",
      "        - -0.056075096130371094\n",
      "        - 3.5018672943115234\n",
      "        - -12.678659439086914\n",
      "    num_agent_steps_sampled: 16614\n",
      "    num_agent_steps_trained: 27456\n",
      "    num_steps_sampled: 8307\n",
      "    num_steps_trained: 13728\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 66\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.1\n",
      "    ram_util_percent: 35.7\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -0.55\n",
      "    policy1: -0.55\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10262630676422095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06817941854914317\n",
      "    mean_inference_ms: 1.5570864547314733\n",
      "    mean_raw_obs_processing_ms: 0.2854269148878664\n",
      "  time_since_restore: 30.39318013191223\n",
      "  time_this_iter_s: 1.2219626903533936\n",
      "  time_total_s: 30.39318013191223\n",
      "  timers:\n",
      "    learn_throughput: 4234.773\n",
      "    learn_time_ms: 7.556\n",
      "    load_throughput: 73306.968\n",
      "    load_time_ms: 0.437\n",
      "    update_time_ms: 4.711\n",
      "  timestamp: 1648811525\n",
      "  timesteps_since_restore: 768\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 8307\n",
      "  training_iteration: 24\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:06 (running for 00:00:44.32)<br>Memory usage on this node: 5.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         30.3932</td><td style=\"text-align: right;\">8307</td><td style=\"text-align: right;\">    -1.1</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             15.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 19208\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 14.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: 3.12\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 575\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 9604\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 11.453048706054688\n",
      "          mean_q: 3.5956525802612305\n",
      "          mean_td_error: -0.40196797251701355\n",
      "          min_q: -3.4768013954162598\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -7.625107765197754\n",
      "        - 3.077608108520508\n",
      "        - -0.5496244430541992\n",
      "        - -1.9940552711486816\n",
      "        - -1.5091133117675781\n",
      "        - 1.680534839630127\n",
      "        - -10.226167678833008\n",
      "        - -0.04870271682739258\n",
      "        - 1.921762466430664\n",
      "        - -1.8827171325683594\n",
      "        - -5.330982685089111\n",
      "        - 5.853010177612305\n",
      "        - -1.8247394561767578\n",
      "        - 0.33383989334106445\n",
      "        - 1.7828389406204224\n",
      "        - -1.629584789276123\n",
      "        - -4.514610290527344\n",
      "        - 3.74294376373291\n",
      "        - 6.8986430168151855\n",
      "        - 0.11954569816589355\n",
      "        - -1.4508665800094604\n",
      "        - -1.6575571298599243\n",
      "        - 1.917311191558838\n",
      "        - 0.8178901672363281\n",
      "        - -0.12458628416061401\n",
      "        - -8.793420791625977\n",
      "        - 4.199341773986816\n",
      "        - 0.595728874206543\n",
      "        - 0.17790746688842773\n",
      "        - 2.827627658843994\n",
      "        - -0.1641101837158203\n",
      "        - 0.5164375305175781\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -5.454522609710693\n",
      "          mean_q: -8.449936866760254\n",
      "          mean_td_error: -1.90791916847229\n",
      "          min_q: -12.64816665649414\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -7.103888988494873\n",
      "        - 0.9367499351501465\n",
      "        - -7.082942008972168\n",
      "        - 0.8399624824523926\n",
      "        - -9.41600513458252\n",
      "        - -9.656063079833984\n",
      "        - 0.7944536209106445\n",
      "        - -5.56723165512085\n",
      "        - -8.597223281860352\n",
      "        - 0.06790828704833984\n",
      "        - 0.2897491455078125\n",
      "        - 0.48096656799316406\n",
      "        - 0.11550521850585938\n",
      "        - 0.16703081130981445\n",
      "        - 0.9196929931640625\n",
      "        - 0.22459030151367188\n",
      "        - 0.08506917953491211\n",
      "        - 0.8633365631103516\n",
      "        - 0.39533138275146484\n",
      "        - 0.6651906967163086\n",
      "        - 1.059189796447754\n",
      "        - 0.28645849227905273\n",
      "        - -11.64816665649414\n",
      "        - 1.1700234413146973\n",
      "        - 0.4918069839477539\n",
      "        - 0.2637662887573242\n",
      "        - 0.14693355560302734\n",
      "        - 0.8855857849121094\n",
      "        - -15.401954650878906\n",
      "        - 1.3641576766967773\n",
      "        - 0.45453643798828125\n",
      "        - 0.4520683288574219\n",
      "    num_agent_steps_sampled: 19208\n",
      "    num_agent_steps_trained: 33152\n",
      "    num_steps_sampled: 9604\n",
      "    num_steps_trained: 16576\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 78\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.15\n",
      "    ram_util_percent: 35.8\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 1.56\n",
      "    policy1: 1.56\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10199963892415521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06770264488088482\n",
      "    mean_inference_ms: 1.5487406367919079\n",
      "    mean_raw_obs_processing_ms: 0.28523723114790017\n",
      "  time_since_restore: 35.50199627876282\n",
      "  time_this_iter_s: 1.2433631420135498\n",
      "  time_total_s: 35.50199627876282\n",
      "  timers:\n",
      "    learn_throughput: 4200.92\n",
      "    learn_time_ms: 7.617\n",
      "    load_throughput: 74124.774\n",
      "    load_time_ms: 0.432\n",
      "    update_time_ms: 4.621\n",
      "  timestamp: 1648811530\n",
      "  timesteps_since_restore: 896\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 9604\n",
      "  training_iteration: 28\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:11 (running for 00:00:49.64)<br>Memory usage on this node: 5.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">          35.502</td><td style=\"text-align: right;\">9604</td><td style=\"text-align: right;\">    3.12</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             14.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 21806\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-16\n",
      "  done: false\n",
      "  episode_len_mean: 12.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 8.74\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 679\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 10903\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 9.965612411499023\n",
      "          mean_q: 4.0150299072265625\n",
      "          mean_td_error: -0.12075597792863846\n",
      "          min_q: -3.588446617126465\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -8.883025169372559\n",
      "        - 1.131911277770996\n",
      "        - -0.13797855377197266\n",
      "        - -6.992007732391357\n",
      "        - -0.5037169456481934\n",
      "        - 1.161865234375\n",
      "        - 1.8453998565673828\n",
      "        - -0.28517913818359375\n",
      "        - -0.18423223495483398\n",
      "        - -2.110116481781006\n",
      "        - -1.182283639907837\n",
      "        - 3.1389498710632324\n",
      "        - 1.7953044176101685\n",
      "        - 0.039589881896972656\n",
      "        - 0.5377928614616394\n",
      "        - 0.7942543029785156\n",
      "        - 0.6552705764770508\n",
      "        - -0.6755867004394531\n",
      "        - 2.1662237644195557\n",
      "        - -0.6322705745697021\n",
      "        - 0.16598224639892578\n",
      "        - 1.8434970378875732\n",
      "        - 0.28597211837768555\n",
      "        - -1.4707436561584473\n",
      "        - -1.2504111528396606\n",
      "        - -0.18853521347045898\n",
      "        - 0.7781753540039062\n",
      "        - 2.257725238800049\n",
      "        - -0.4228057861328125\n",
      "        - -0.1191052794456482\n",
      "        - 0.9485337734222412\n",
      "        - 1.6273584365844727\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -7.061098098754883\n",
      "          mean_q: -10.971120834350586\n",
      "          mean_td_error: -1.8448988199234009\n",
      "          min_q: -14.428485870361328\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -3.683070182800293\n",
      "        - -0.9951200485229492\n",
      "        - -12.09366512298584\n",
      "        - -3.281858444213867\n",
      "        - 0.0975179672241211\n",
      "        - -0.13107013702392578\n",
      "        - -0.7666959762573242\n",
      "        - -5.309679985046387\n",
      "        - 1.4670944213867188\n",
      "        - -11.57614517211914\n",
      "        - 0.6010408401489258\n",
      "        - -0.8563022613525391\n",
      "        - -0.09233760833740234\n",
      "        - 0.6711397171020508\n",
      "        - -0.09868717193603516\n",
      "        - 0.19142532348632812\n",
      "        - 1.2616519927978516\n",
      "        - -0.3118267059326172\n",
      "        - 1.0649223327636719\n",
      "        - -20.214244842529297\n",
      "        - -3.807610511779785\n",
      "        - 0.13104629516601562\n",
      "        - -0.7397127151489258\n",
      "        - 1.1445646286010742\n",
      "        - 2.508546829223633\n",
      "        - -0.47123241424560547\n",
      "        - -0.9390630722045898\n",
      "        - -0.8644866943359375\n",
      "        - 0.9124355316162109\n",
      "        - 1.9791278839111328\n",
      "        - -1.3236417770385742\n",
      "        - -3.5108261108398438\n",
      "    num_agent_steps_sampled: 21806\n",
      "    num_agent_steps_trained: 39616\n",
      "    num_steps_sampled: 10903\n",
      "    num_steps_trained: 19808\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 90\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.15\n",
      "    ram_util_percent: 35.849999999999994\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 4.37\n",
      "    policy1: 4.37\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10132370787466663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06728329066960734\n",
      "    mean_inference_ms: 1.5418510896063762\n",
      "    mean_raw_obs_processing_ms: 0.2852417708614986\n",
      "  time_since_restore: 40.89869427680969\n",
      "  time_this_iter_s: 1.4076666831970215\n",
      "  time_total_s: 40.89869427680969\n",
      "  timers:\n",
      "    learn_throughput: 4485.557\n",
      "    learn_time_ms: 7.134\n",
      "    load_throughput: 75196.217\n",
      "    load_time_ms: 0.426\n",
      "    update_time_ms: 4.311\n",
      "  timestamp: 1648811536\n",
      "  timesteps_since_restore: 1024\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 10903\n",
      "  training_iteration: 32\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:17 (running for 00:00:55.25)<br>Memory usage on this node: 5.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         40.8987</td><td style=\"text-align: right;\">10903</td><td style=\"text-align: right;\">    8.74</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             12.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 24360\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 14.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: 4.54\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 764\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 12101\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 8.095492362976074\n",
      "          mean_q: 1.6845781803131104\n",
      "          mean_td_error: -0.03299658000469208\n",
      "          min_q: -5.0462517738342285\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.914867639541626\n",
      "        - -0.17677640914916992\n",
      "        - 0.6906156539916992\n",
      "        - -0.681574821472168\n",
      "        - -0.596646785736084\n",
      "        - -0.2894020080566406\n",
      "        - -0.11177109181880951\n",
      "        - -1.0229387283325195\n",
      "        - 0.03333711624145508\n",
      "        - 4.072486400604248\n",
      "        - -0.16252517700195312\n",
      "        - 1.5203624963760376\n",
      "        - 1.125533103942871\n",
      "        - -0.5313091278076172\n",
      "        - -0.07956719398498535\n",
      "        - -1.028455376625061\n",
      "        - -4.0462517738342285\n",
      "        - 0.22577285766601562\n",
      "        - 2.1027793884277344\n",
      "        - -1.022966980934143\n",
      "        - 2.57084321975708\n",
      "        - 0.8752541542053223\n",
      "        - -1.1681065559387207\n",
      "        - -0.7804836630821228\n",
      "        - 2.438845634460449\n",
      "        - -1.0326611995697021\n",
      "        - -7.229660511016846\n",
      "        - 0.7023875713348389\n",
      "        - 0.24237918853759766\n",
      "        - 1.4387973546981812\n",
      "        - -0.14666271209716797\n",
      "        - 0.09760808944702148\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -0.795734167098999\n",
      "          mean_q: -9.635025024414062\n",
      "          mean_td_error: -5.223695755004883\n",
      "          min_q: -17.13189697265625\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.1770687103271484\n",
      "        - -1.3904094696044922\n",
      "        - 1.091902732849121\n",
      "        - 1.0606002807617188\n",
      "        - -12.893211364746094\n",
      "        - -1.476480484008789\n",
      "        - -0.7858419418334961\n",
      "        - -22.30982208251953\n",
      "        - -1.3532323837280273\n",
      "        - -14.260361671447754\n",
      "        - -13.34139347076416\n",
      "        - -24.562763214111328\n",
      "        - -2.298065662384033\n",
      "        - -0.6206464767456055\n",
      "        - -1.9627766609191895\n",
      "        - 0.0012989044189453125\n",
      "        - -3.372109889984131\n",
      "        - -0.1564922332763672\n",
      "        - 0.3501625061035156\n",
      "        - -2.424135208129883\n",
      "        - 0.07082462310791016\n",
      "        - -1.903315544128418\n",
      "        - -9.715191841125488\n",
      "        - 0.13659095764160156\n",
      "        - -3.111260414123535\n",
      "        - -1.879396915435791\n",
      "        - -9.795734405517578\n",
      "        - -11.352770805358887\n",
      "        - -2.107304573059082\n",
      "        - -12.091378211975098\n",
      "        - -15.499984741210938\n",
      "        - -0.38263893127441406\n",
      "    num_agent_steps_sampled: 24360\n",
      "    num_agent_steps_trained: 45056\n",
      "    num_steps_sampled: 12180\n",
      "    num_steps_trained: 22528\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 101\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.350000000000001\n",
      "    ram_util_percent: 35.9\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 2.27\n",
      "    policy1: 2.27\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10093392409678746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06698497822067054\n",
      "    mean_inference_ms: 1.5375875799083152\n",
      "    mean_raw_obs_processing_ms: 0.2852319024682542\n",
      "  time_since_restore: 45.858434438705444\n",
      "  time_this_iter_s: 1.220933198928833\n",
      "  time_total_s: 45.858434438705444\n",
      "  timers:\n",
      "    learn_throughput: 4706.075\n",
      "    learn_time_ms: 6.8\n",
      "    load_throughput: 76525.302\n",
      "    load_time_ms: 0.418\n",
      "    update_time_ms: 4.664\n",
      "  timestamp: 1648811541\n",
      "  timesteps_since_restore: 1152\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 12180\n",
      "  training_iteration: 36\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:22 (running for 00:01:00.45)<br>Memory usage on this node: 5.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         46.8312</td><td style=\"text-align: right;\">12392</td><td style=\"text-align: right;\">    3.82</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             14.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 26878\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-27\n",
      "  done: false\n",
      "  episode_len_mean: 10.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 16.46\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 879\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 13391\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 11.516716003417969\n",
      "          mean_q: 5.139641761779785\n",
      "          mean_td_error: -1.1111465692520142\n",
      "          min_q: -3.193758726119995\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -1.1378860473632812\n",
      "        - -1.6024723052978516\n",
      "        - -1.4521920680999756\n",
      "        - -3.2711868286132812\n",
      "        - -7.698770523071289\n",
      "        - -0.4961634874343872\n",
      "        - 0.020639419555664062\n",
      "        - 0.7644863128662109\n",
      "        - -0.0701441764831543\n",
      "        - 0.33293676376342773\n",
      "        - -11.22476577758789\n",
      "        - -0.6359596252441406\n",
      "        - -2.5665407180786133\n",
      "        - -2.0805463790893555\n",
      "        - -2.611941337585449\n",
      "        - -1.1844215393066406\n",
      "        - 0.010283470153808594\n",
      "        - -0.5923013687133789\n",
      "        - 0.523597240447998\n",
      "        - 0.24573683738708496\n",
      "        - 0.6180944442749023\n",
      "        - 0.39800775051116943\n",
      "        - 1.0029796361923218\n",
      "        - 2.5946602821350098\n",
      "        - -1.5574336051940918\n",
      "        - 4.972013473510742\n",
      "        - -2.2601699829101562\n",
      "        - -0.5406532287597656\n",
      "        - 3.2835159301757812\n",
      "        - 0.25945568084716797\n",
      "        - -3.420375108718872\n",
      "        - -6.179174900054932\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -1.104374647140503\n",
      "          mean_q: -4.857464790344238\n",
      "          mean_td_error: -1.6812515258789062\n",
      "          min_q: -10.632223129272461\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -2.2622737884521484\n",
      "        - 0.06958389282226562\n",
      "        - -0.44380974769592285\n",
      "        - 0.6328425407409668\n",
      "        - 2.3036742210388184\n",
      "        - 2.4777960777282715\n",
      "        - -1.1215271949768066\n",
      "        - -0.1901073455810547\n",
      "        - -1.279815912246704\n",
      "        - -2.2622737884521484\n",
      "        - 0.9711761474609375\n",
      "        - -3.6774303913116455\n",
      "        - -5.176246643066406\n",
      "        - 2.7968201637268066\n",
      "        - -12.179222106933594\n",
      "        - 3.472538948059082\n",
      "        - -0.817072868347168\n",
      "        - -1.0708985328674316\n",
      "        - 0.8985781669616699\n",
      "        - 0.19475078582763672\n",
      "        - -17.475849151611328\n",
      "        - -4.89448356628418\n",
      "        - -2.407280921936035\n",
      "        - -10.268936157226562\n",
      "        - -0.6386721134185791\n",
      "        - 0.895258903503418\n",
      "        - -0.41300344467163086\n",
      "        - -4.073306560516357\n",
      "        - 0.8002424240112305\n",
      "        - 0.6658000946044922\n",
      "        - 1.4685750007629395\n",
      "        - -0.7954769134521484\n",
      "    num_agent_steps_sampled: 26878\n",
      "    num_agent_steps_trained: 52096\n",
      "    num_steps_sampled: 13439\n",
      "    num_steps_trained: 26048\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 113\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.0\n",
      "    ram_util_percent: 36.0\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 8.23\n",
      "    policy1: 8.23\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10068808133509581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06683321622937909\n",
      "    mean_inference_ms: 1.5317983178155685\n",
      "    mean_raw_obs_processing_ms: 0.2860023587954386\n",
      "  time_since_restore: 51.55396628379822\n",
      "  time_this_iter_s: 0.9854018688201904\n",
      "  time_total_s: 51.55396628379822\n",
      "  timers:\n",
      "    learn_throughput: 4508.065\n",
      "    learn_time_ms: 7.098\n",
      "    load_throughput: 75141.489\n",
      "    load_time_ms: 0.426\n",
      "    update_time_ms: 4.449\n",
      "  timestamp: 1648811547\n",
      "  timesteps_since_restore: 1312\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 13439\n",
      "  training_iteration: 41\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:28 (running for 00:01:06.39)<br>Memory usage on this node: 5.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          51.554</td><td style=\"text-align: right;\">13439</td><td style=\"text-align: right;\">   16.46</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">             10.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 29446\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 11.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 13.14\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 990\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 14703\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 11.280315399169922\n",
      "          mean_q: 2.7298262119293213\n",
      "          mean_td_error: -0.5881629586219788\n",
      "          min_q: -6.844762802124023\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.67216157913208\n",
      "        - -0.279803991317749\n",
      "        - -1.7829484939575195\n",
      "        - -0.9018895626068115\n",
      "        - -0.3642911911010742\n",
      "        - -2.2780637741088867\n",
      "        - -0.6521854400634766\n",
      "        - -0.9120477437973022\n",
      "        - 0.2535606622695923\n",
      "        - -2.3059475421905518\n",
      "        - 2.694058895111084\n",
      "        - 0.6732387542724609\n",
      "        - -0.19723081588745117\n",
      "        - 0.004123687744140625\n",
      "        - -2.7437520027160645\n",
      "        - -0.4313795566558838\n",
      "        - -5.3652496337890625\n",
      "        - -0.2729630470275879\n",
      "        - 0.38364195823669434\n",
      "        - 0.36086052656173706\n",
      "        - 0.9494009017944336\n",
      "        - 3.31756854057312\n",
      "        - 1.1625633239746094\n",
      "        - 0.6858184337615967\n",
      "        - 1.0419425964355469\n",
      "        - -0.23875761032104492\n",
      "        - 0.4001929759979248\n",
      "        - -0.14236736297607422\n",
      "        - -0.9401111602783203\n",
      "        - 0.603813648223877\n",
      "        - 2.1514580249786377\n",
      "        - -15.366630554199219\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 9.348431587219238\n",
      "          mean_q: -1.2662571668624878\n",
      "          mean_td_error: -2.3886420726776123\n",
      "          min_q: -5.938241481781006\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -5.68164587020874\n",
      "        - 0.2694060802459717\n",
      "        - -3.866344451904297\n",
      "        - -1.42649507522583\n",
      "        - -8.361285209655762\n",
      "        - -0.20948004722595215\n",
      "        - -1.0278682708740234\n",
      "        - -13.364678382873535\n",
      "        - -1.5822944641113281\n",
      "        - -1.3704841136932373\n",
      "        - -10.585105895996094\n",
      "        - -0.9494953155517578\n",
      "        - -2.011209011077881\n",
      "        - -4.004800796508789\n",
      "        - -6.015007495880127\n",
      "        - -0.7711386680603027\n",
      "        - -4.21942138671875\n",
      "        - 4.184271812438965\n",
      "        - -5.750682353973389\n",
      "        - -1.0324463844299316\n",
      "        - 2.4746596813201904\n",
      "        - -1.604504108428955\n",
      "        - -1.3231029510498047\n",
      "        - -3.4588241577148438\n",
      "        - -1.375622272491455\n",
      "        - -0.004494190216064453\n",
      "        - -8.561080932617188\n",
      "        - 6.449892044067383\n",
      "        - 0.878204345703125\n",
      "        - 0.8467284440994263\n",
      "        - -4.252780914306641\n",
      "        - 1.2705817222595215\n",
      "    num_agent_steps_sampled: 29446\n",
      "    num_agent_steps_trained: 58880\n",
      "    num_steps_sampled: 14723\n",
      "    num_steps_trained: 29440\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 125\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.65\n",
      "    ram_util_percent: 36.0\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 6.57\n",
      "    policy1: 6.57\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10040414244831884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06664963895327901\n",
      "    mean_inference_ms: 1.5270912460202561\n",
      "    mean_raw_obs_processing_ms: 0.2871112651075665\n",
      "  time_since_restore: 57.02987790107727\n",
      "  time_this_iter_s: 1.3098602294921875\n",
      "  time_total_s: 57.02987790107727\n",
      "  timers:\n",
      "    learn_throughput: 4515.011\n",
      "    learn_time_ms: 7.087\n",
      "    load_throughput: 75556.028\n",
      "    load_time_ms: 0.424\n",
      "    update_time_ms: 4.323\n",
      "  timestamp: 1648811553\n",
      "  timesteps_since_restore: 1440\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 14723\n",
      "  training_iteration: 45\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:34 (running for 00:01:12.07)<br>Memory usage on this node: 5.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         57.0299</td><td style=\"text-align: right;\">14723</td><td style=\"text-align: right;\">   13.14</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">             11.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 31580\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-38\n",
      "  done: false\n",
      "  episode_len_mean: 10.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 15.92\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1093\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 15690\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 14.233137130737305\n",
      "          mean_q: 5.675693511962891\n",
      "          mean_td_error: -0.13954231142997742\n",
      "          min_q: -2.9593420028686523\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.9370412826538086\n",
      "        - -1.369354248046875\n",
      "        - -2.0208911895751953\n",
      "        - -5.445400238037109\n",
      "        - 2.4252710342407227\n",
      "        - 0.33203768730163574\n",
      "        - -6.579209327697754\n",
      "        - -3.9847660064697266\n",
      "        - 1.1085634231567383\n",
      "        - 2.656569004058838\n",
      "        - -3.8461556434631348\n",
      "        - 0.5638246536254883\n",
      "        - 1.5519626140594482\n",
      "        - -2.387690544128418\n",
      "        - -0.005532264709472656\n",
      "        - 3.436338424682617\n",
      "        - 7.511578559875488\n",
      "        - -1.5030136108398438\n",
      "        - 7.346726894378662\n",
      "        - -0.27617359161376953\n",
      "        - -1.429889440536499\n",
      "        - -0.3268303871154785\n",
      "        - -3.028158187866211\n",
      "        - 0.47462034225463867\n",
      "        - 0.7257061004638672\n",
      "        - 0.8520393371582031\n",
      "        - 2.08651065826416\n",
      "        - -1.4271445274353027\n",
      "        - -1.5642242431640625\n",
      "        - -0.005532264709472656\n",
      "        - -0.24009418487548828\n",
      "        - 0.839998722076416\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 5.526458740234375\n",
      "          mean_q: -1.5525672435760498\n",
      "          mean_td_error: -0.8176873922348022\n",
      "          min_q: -6.0404157638549805\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.8822653293609619\n",
      "        - 1.6348321437835693\n",
      "        - 1.1445870399475098\n",
      "        - 1.9952454566955566\n",
      "        - 1.418713092803955\n",
      "        - -3.900956392288208\n",
      "        - -2.5867412090301514\n",
      "        - -3.464675188064575\n",
      "        - 0.14689016342163086\n",
      "        - 3.096569776535034\n",
      "        - -11.51435661315918\n",
      "        - 0.04957127571105957\n",
      "        - 1.3793365955352783\n",
      "        - 0.6867187023162842\n",
      "        - 1.0687785148620605\n",
      "        - 0.16171884536743164\n",
      "        - -9.125296592712402\n",
      "        - -0.49892520904541016\n",
      "        - 1.0858490467071533\n",
      "        - -2.103701591491699\n",
      "        - 1.0906563997268677\n",
      "        - 0.24219369888305664\n",
      "        - 0.8802571296691895\n",
      "        - 2.507915496826172\n",
      "        - 1.137561321258545\n",
      "        - -6.204919338226318\n",
      "        - -4.656717777252197\n",
      "        - -5.245963096618652\n",
      "        - 0.7776083946228027\n",
      "        - 2.0215024948120117\n",
      "        - 0.4044792652130127\n",
      "        - -0.6769932508468628\n",
      "    num_agent_steps_sampled: 31580\n",
      "    num_agent_steps_trained: 65024\n",
      "    num_steps_sampled: 15790\n",
      "    num_steps_trained: 32512\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 134\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.9\n",
      "    ram_util_percent: 37.3\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 7.96\n",
      "    policy1: 7.96\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.100270953191263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0665225692806265\n",
      "    mean_inference_ms: 1.524370861178985\n",
      "    mean_raw_obs_processing_ms: 0.28801204468045716\n",
      "  time_since_restore: 61.95074701309204\n",
      "  time_this_iter_s: 1.0638816356658936\n",
      "  time_total_s: 61.95074701309204\n",
      "  timers:\n",
      "    learn_throughput: 4368.455\n",
      "    learn_time_ms: 7.325\n",
      "    load_throughput: 75979.467\n",
      "    load_time_ms: 0.421\n",
      "    update_time_ms: 5.082\n",
      "  timestamp: 1648811558\n",
      "  timesteps_since_restore: 1568\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 15790\n",
      "  training_iteration: 49\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:39 (running for 00:01:17.21)<br>Memory usage on this node: 6.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         61.9507</td><td style=\"text-align: right;\">15790</td><td style=\"text-align: right;\">   15.92</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             10.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 33332\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-43\n",
      "  done: false\n",
      "  episode_len_mean: 8.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 21.06\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1202\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 16666\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 14.372072219848633\n",
      "          mean_q: 7.382848262786865\n",
      "          mean_td_error: -1.0041651725769043\n",
      "          min_q: -4.405240058898926\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -2.8905792236328125\n",
      "        - 1.0245399475097656\n",
      "        - -8.555474281311035\n",
      "        - 0.4991464614868164\n",
      "        - -4.516040802001953\n",
      "        - -2.6707096099853516\n",
      "        - -1.6971111297607422\n",
      "        - 0.7747335433959961\n",
      "        - 0.7109451293945312\n",
      "        - 1.3670203685760498\n",
      "        - 1.4866418838500977\n",
      "        - -1.2192420959472656\n",
      "        - 0.6306781768798828\n",
      "        - 1.5075035095214844\n",
      "        - 1.1189937591552734\n",
      "        - 1.5075035095214844\n",
      "        - -0.3115987777709961\n",
      "        - -1.8194036483764648\n",
      "        - 0.2308816909790039\n",
      "        - -2.908825397491455\n",
      "        - 0.41158342361450195\n",
      "        - 6.06217098236084\n",
      "        - 0.32111072540283203\n",
      "        - -0.09832382202148438\n",
      "        - -2.8905792236328125\n",
      "        - 0.055617332458496094\n",
      "        - -0.7672688961029053\n",
      "        - 1.2654399871826172\n",
      "        - -6.16157865524292\n",
      "        - -8.390425682067871\n",
      "        - 1.4866437911987305\n",
      "        - -7.697277069091797\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 10.013548851013184\n",
      "          mean_q: 0.3258320689201355\n",
      "          mean_td_error: -1.3488945960998535\n",
      "          min_q: -5.285489082336426\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -3.2445597648620605\n",
      "        - -3.4090635776519775\n",
      "        - 0.48046016693115234\n",
      "        - -1.1711554527282715\n",
      "        - 2.243058204650879\n",
      "        - -1.8839905261993408\n",
      "        - -0.9833104014396667\n",
      "        - 1.1164007186889648\n",
      "        - 0.21097326278686523\n",
      "        - -1.376558780670166\n",
      "        - -9.905862808227539\n",
      "        - -0.9653408527374268\n",
      "        - -11.923354148864746\n",
      "        - 1.7537736892700195\n",
      "        - -0.8490090370178223\n",
      "        - 3.435697317123413\n",
      "        - 4.749736309051514\n",
      "        - 0.5014071464538574\n",
      "        - -0.442962646484375\n",
      "        - -5.579370498657227\n",
      "        - -0.5263404846191406\n",
      "        - -6.918893814086914\n",
      "        - -2.020286798477173\n",
      "        - -4.448228359222412\n",
      "        - 0.8516864776611328\n",
      "        - -10.37072467803955\n",
      "        - -5.298084259033203\n",
      "        - 6.813422203063965\n",
      "        - 3.410719871520996\n",
      "        - 1.1854324340820312\n",
      "        - 1.0135488510131836\n",
      "        - 0.3861508369445801\n",
      "    num_agent_steps_sampled: 33332\n",
      "    num_agent_steps_trained: 70720\n",
      "    num_steps_sampled: 16666\n",
      "    num_steps_trained: 35360\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 143\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.9\n",
      "    ram_util_percent: 44.7\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 10.53\n",
      "    policy1: 10.53\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10173098448935189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06753932520070621\n",
      "    mean_inference_ms: 1.5520120843494825\n",
      "    mean_raw_obs_processing_ms: 0.29428644798873976\n",
      "  time_since_restore: 67.29691553115845\n",
      "  time_this_iter_s: 0.9503152370452881\n",
      "  time_total_s: 67.29691553115845\n",
      "  timers:\n",
      "    learn_throughput: 4030.429\n",
      "    learn_time_ms: 7.94\n",
      "    load_throughput: 71312.751\n",
      "    load_time_ms: 0.449\n",
      "    update_time_ms: 5.258\n",
      "  timestamp: 1648811563\n",
      "  timesteps_since_restore: 1696\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 16666\n",
      "  training_iteration: 53\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:44 (running for 00:01:22.81)<br>Memory usage on this node: 7.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         67.2969</td><td style=\"text-align: right;\">16666</td><td style=\"text-align: right;\">   21.06</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">              8.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 35468\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-50\n",
      "  done: false\n",
      "  episode_len_mean: 9.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 21.0\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1314\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 17734\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 13.083006858825684\n",
      "          mean_q: 5.959346771240234\n",
      "          mean_td_error: -0.4045462906360626\n",
      "          min_q: -3.13037371635437\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.13582134246826172\n",
      "        - -1.2202162742614746\n",
      "        - -3.606572389602661\n",
      "        - 3.6320719718933105\n",
      "        - -1.4628114700317383\n",
      "        - -1.2327601909637451\n",
      "        - -1.946859359741211\n",
      "        - -0.3401479721069336\n",
      "        - -1.411271095275879\n",
      "        - -1.3453278541564941\n",
      "        - -0.9715986251831055\n",
      "        - -1.5844900608062744\n",
      "        - -3.8222408294677734\n",
      "        - -2.1906862258911133\n",
      "        - -0.8754472732543945\n",
      "        - 2.0102202892303467\n",
      "        - -1.4543638229370117\n",
      "        - -0.7155437469482422\n",
      "        - 0.6857843399047852\n",
      "        - 0.5834236145019531\n",
      "        - -1.677642822265625\n",
      "        - -0.5228610038757324\n",
      "        - -1.4264662265777588\n",
      "        - 4.216644763946533\n",
      "        - -0.9107460975646973\n",
      "        - 0.8315465450286865\n",
      "        - 0.04757307469844818\n",
      "        - -0.5122823715209961\n",
      "        - -1.1090736389160156\n",
      "        - 5.054481029510498\n",
      "        - 1.018789291381836\n",
      "        - -0.8224287033081055\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 6.843070983886719\n",
      "          mean_q: 0.3347713053226471\n",
      "          mean_td_error: -0.9045981168746948\n",
      "          min_q: -5.726581573486328\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.2388620376586914\n",
      "        - -2.679663896560669\n",
      "        - 0.8234412670135498\n",
      "        - -0.12885046005249023\n",
      "        - -0.6189179420471191\n",
      "        - -0.8418617248535156\n",
      "        - -10.76026725769043\n",
      "        - 1.2678139209747314\n",
      "        - 0.5136480331420898\n",
      "        - -0.06582045555114746\n",
      "        - -3.5656352043151855\n",
      "        - -0.26587915420532227\n",
      "        - -0.021110057830810547\n",
      "        - -0.01557159423828125\n",
      "        - -0.7658758163452148\n",
      "        - 1.8060131072998047\n",
      "        - -0.7941536903381348\n",
      "        - 5.195387840270996\n",
      "        - -2.4822213649749756\n",
      "        - 0.6335070133209229\n",
      "        - 3.2117013931274414\n",
      "        - -13.420732498168945\n",
      "        - 0.8802237510681152\n",
      "        - 0.40737462043762207\n",
      "        - 0.07809925079345703\n",
      "        - 2.691758155822754\n",
      "        - 1.028653621673584\n",
      "        - 0.631312370300293\n",
      "        - -0.5024893283843994\n",
      "        - 0.7995936870574951\n",
      "        - -12.696825981140137\n",
      "        - -0.5286521911621094\n",
      "    num_agent_steps_sampled: 35468\n",
      "    num_agent_steps_trained: 77760\n",
      "    num_steps_sampled: 17734\n",
      "    num_steps_trained: 38880\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 153\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.0\n",
      "    ram_util_percent: 46.9\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 10.5\n",
      "    policy1: 10.5\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10238347949649476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06804051218004459\n",
      "    mean_inference_ms: 1.5614999813913721\n",
      "    mean_raw_obs_processing_ms: 0.29709308203961415\n",
      "  time_since_restore: 73.24502038955688\n",
      "  time_this_iter_s: 1.230454683303833\n",
      "  time_total_s: 73.24502038955688\n",
      "  timers:\n",
      "    learn_throughput: 3857.751\n",
      "    learn_time_ms: 8.295\n",
      "    load_throughput: 66303.279\n",
      "    load_time_ms: 0.483\n",
      "    update_time_ms: 5.153\n",
      "  timestamp: 1648811570\n",
      "  timesteps_since_restore: 1856\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 17734\n",
      "  training_iteration: 58\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:50 (running for 00:01:28.06)<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">          73.245</td><td style=\"text-align: right;\">17734</td><td style=\"text-align: right;\">      21</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">               9.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:12:55 (running for 00:01:33.18)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         77.1945</td><td style=\"text-align: right;\">18352</td><td style=\"text-align: right;\">   22.28</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">              8.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 37116\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-12-55\n",
      "  done: false\n",
      "  episode_len_mean: 8.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 23.28\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1413\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 18466\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 14.388236045837402\n",
      "          mean_q: 7.176314353942871\n",
      "          mean_td_error: 0.49485981464385986\n",
      "          min_q: -1.8163902759552002\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.8039312362670898\n",
      "        - 1.875650405883789\n",
      "        - 0.10837554931640625\n",
      "        - -0.21106243133544922\n",
      "        - 0.07729148864746094\n",
      "        - 0.20067119598388672\n",
      "        - -0.5483837127685547\n",
      "        - 0.5539035797119141\n",
      "        - -0.037848472595214844\n",
      "        - -1.3414030075073242\n",
      "        - 1.054764747619629\n",
      "        - 6.878635883331299\n",
      "        - 0.5104860067367554\n",
      "        - 0.21394872665405273\n",
      "        - 0.8038263320922852\n",
      "        - -1.4005064964294434\n",
      "        - -1.3823490142822266\n",
      "        - -0.32323360443115234\n",
      "        - 0.06357622146606445\n",
      "        - -0.037848472595214844\n",
      "        - 1.2726860046386719\n",
      "        - -0.32891273498535156\n",
      "        - -0.5355734825134277\n",
      "        - -0.36513757705688477\n",
      "        - 0.44350600242614746\n",
      "        - 0.9129128456115723\n",
      "        - 0.9239833354949951\n",
      "        - 0.40580177307128906\n",
      "        - 0.07325363159179688\n",
      "        - 3.5868539810180664\n",
      "        - 0.1975780725479126\n",
      "        - 1.3861356973648071\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 12.18112850189209\n",
      "          mean_q: 2.25783634185791\n",
      "          mean_td_error: -0.8244170546531677\n",
      "          min_q: -5.478352069854736\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 4.206325531005859\n",
      "        - -0.7755645513534546\n",
      "        - -0.41754150390625\n",
      "        - -1.0217432975769043\n",
      "        - -9.184494972229004\n",
      "        - 0.7833585739135742\n",
      "        - -7.369344234466553\n",
      "        - -0.5777339935302734\n",
      "        - -9.987996101379395\n",
      "        - 0.7850885391235352\n",
      "        - 0.6374473571777344\n",
      "        - 2.1370925903320312\n",
      "        - 1.4783427715301514\n",
      "        - -0.1889629364013672\n",
      "        - 8.993916511535645\n",
      "        - -7.7292375564575195\n",
      "        - 6.572575092315674\n",
      "        - -2.2588634490966797\n",
      "        - -0.47407007217407227\n",
      "        - -1.9333620071411133\n",
      "        - -0.36723852157592773\n",
      "        - -1.7245796918869019\n",
      "        - -4.421043395996094\n",
      "        - 0.09465956687927246\n",
      "        - -9.273765563964844\n",
      "        - 0.1911630630493164\n",
      "        - -1.1994094848632812\n",
      "        - -0.42412662506103516\n",
      "        - 0.5726007223129272\n",
      "        - 3.8296055793762207\n",
      "        - 1.446866512298584\n",
      "        - 1.2186899185180664\n",
      "    num_agent_steps_sampled: 37116\n",
      "    num_agent_steps_trained: 84032\n",
      "    num_steps_sampled: 18558\n",
      "    num_steps_trained: 42016\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 160\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.15\n",
      "    ram_util_percent: 48.349999999999994\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 11.64\n",
      "    policy1: 11.64\n",
      "  policy_reward_min:\n",
      "    policy0: 0.0\n",
      "    policy1: 0.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10399932076830652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06906751183208366\n",
      "    mean_inference_ms: 1.5834544547619362\n",
      "    mean_raw_obs_processing_ms: 0.30151251968848364\n",
      "  time_since_restore: 78.50867509841919\n",
      "  time_this_iter_s: 1.3142211437225342\n",
      "  time_total_s: 78.50867509841919\n",
      "  timers:\n",
      "    learn_throughput: 3994.67\n",
      "    learn_time_ms: 8.011\n",
      "    load_throughput: 66237.836\n",
      "    load_time_ms: 0.483\n",
      "    update_time_ms: 4.868\n",
      "  timestamp: 1648811575\n",
      "  timesteps_since_restore: 1984\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 18558\n",
      "  training_iteration: 62\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:01 (running for 00:01:38.88)<br>Memory usage on this node: 7.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         82.6907</td><td style=\"text-align: right;\">19178</td><td style=\"text-align: right;\">   25.32</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">              7.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 38764\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-01\n",
      "  done: false\n",
      "  episode_len_mean: 6.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 26.52\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1532\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 19311\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 16.284128189086914\n",
      "          mean_q: 7.975105285644531\n",
      "          mean_td_error: -0.1541382074356079\n",
      "          min_q: 1.140122413635254\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.07731819152832031\n",
      "        - -0.5054183006286621\n",
      "        - -0.02882838249206543\n",
      "        - 0.10416030883789062\n",
      "        - 0.5951786041259766\n",
      "        - 0.9249563217163086\n",
      "        - 1.0743083953857422\n",
      "        - -0.01121377944946289\n",
      "        - 0.47177886962890625\n",
      "        - -1.5644638538360596\n",
      "        - 1.402113914489746\n",
      "        - 0.625361442565918\n",
      "        - -0.3030662536621094\n",
      "        - 0.14871501922607422\n",
      "        - -0.4754800796508789\n",
      "        - -0.322329044342041\n",
      "        - -1.296792984008789\n",
      "        - 0.009386062622070312\n",
      "        - -0.9568977355957031\n",
      "        - 0.03615617752075195\n",
      "        - -2.0560994148254395\n",
      "        - -0.46720361709594727\n",
      "        - -0.4084053039550781\n",
      "        - 0.4126706123352051\n",
      "        - 1.0833330154418945\n",
      "        - -0.805019736289978\n",
      "        - 0.4848661422729492\n",
      "        - -0.5958328247070312\n",
      "        - -0.4983487129211426\n",
      "        - 0.19962692260742188\n",
      "        - 0.10798454284667969\n",
      "        - -2.394937515258789\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 11.108977317810059\n",
      "          mean_q: 3.1313588619232178\n",
      "          mean_td_error: -1.7177213430404663\n",
      "          min_q: -3.6826181411743164\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.04365992546081543\n",
      "        - -1.2386293411254883\n",
      "        - -0.5148043632507324\n",
      "        - -6.603738784790039\n",
      "        - 1.6191396713256836\n",
      "        - -7.506748199462891\n",
      "        - -0.6163130402565002\n",
      "        - -10.696632385253906\n",
      "        - -1.2386293411254883\n",
      "        - 1.691007375717163\n",
      "        - -8.197066307067871\n",
      "        - -2.0138258934020996\n",
      "        - 0.20394229888916016\n",
      "        - 0.37969350814819336\n",
      "        - -4.372673034667969\n",
      "        - -3.9263410568237305\n",
      "        - -5.712527275085449\n",
      "        - -2.247084617614746\n",
      "        - 2.862989902496338\n",
      "        - 1.283177137374878\n",
      "        - 0.5212481021881104\n",
      "        - -9.4964017868042\n",
      "        - -0.8794737458229065\n",
      "        - 4.953874588012695\n",
      "        - 0.429457426071167\n",
      "        - -2.9447221755981445\n",
      "        - -3.958866596221924\n",
      "        - -0.5495905876159668\n",
      "        - 2.891362190246582\n",
      "        - 0.4901564121246338\n",
      "        - 0.4584522247314453\n",
      "        - 0.006142616271972656\n",
      "    num_agent_steps_sampled: 38764\n",
      "    num_agent_steps_trained: 91072\n",
      "    num_steps_sampled: 19382\n",
      "    num_steps_trained: 45536\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 168\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.45\n",
      "    ram_util_percent: 50.4\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 13.26\n",
      "    policy1: 13.26\n",
      "  policy_reward_min:\n",
      "    policy0: 4.0\n",
      "    policy1: 4.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1052154508427445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06979603923298155\n",
      "    mean_inference_ms: 1.5999611104206202\n",
      "    mean_raw_obs_processing_ms: 0.3059767146779621\n",
      "  time_since_restore: 84.01123785972595\n",
      "  time_this_iter_s: 1.3204903602600098\n",
      "  time_total_s: 84.01123785972595\n",
      "  timers:\n",
      "    learn_throughput: 4036.429\n",
      "    learn_time_ms: 7.928\n",
      "    load_throughput: 69313.018\n",
      "    load_time_ms: 0.462\n",
      "    update_time_ms: 5.003\n",
      "  timestamp: 1648811581\n",
      "  timesteps_since_restore: 2112\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 19382\n",
      "  training_iteration: 66\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:06 (running for 00:01:44.70)<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         88.3004</td><td style=\"text-align: right;\">19995</td><td style=\"text-align: right;\">   30.78</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">              4.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 40408\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-07\n",
      "  done: false\n",
      "  episode_len_mean: 5.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 29.52\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1696\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 20149\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 16.447742462158203\n",
      "          mean_q: 7.835210800170898\n",
      "          mean_td_error: 0.36630475521087646\n",
      "          min_q: 1.4538906812667847\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.39189577102661133\n",
      "        - -0.2056121826171875\n",
      "        - 0.25525569915771484\n",
      "        - 0.15714263916015625\n",
      "        - 0.7758417129516602\n",
      "        - -1.252023696899414\n",
      "        - 0.0024394989013671875\n",
      "        - -0.6430091857910156\n",
      "        - -0.2124767303466797\n",
      "        - -0.4377899169921875\n",
      "        - 0.11780929565429688\n",
      "        - -0.25320911407470703\n",
      "        - 1.3585538864135742\n",
      "        - 4.930269718170166\n",
      "        - 0.4046158790588379\n",
      "        - 1.335352897644043\n",
      "        - 1.1954690217971802\n",
      "        - 0.5856361389160156\n",
      "        - 0.04886579513549805\n",
      "        - 1.9033315181732178\n",
      "        - 0.3567934036254883\n",
      "        - -0.7823400497436523\n",
      "        - 0.04593658447265625\n",
      "        - 2.352637529373169\n",
      "        - 0.5853142738342285\n",
      "        - 0.3971834182739258\n",
      "        - -0.038739681243896484\n",
      "        - -1.1515116691589355\n",
      "        - 0.10651016235351562\n",
      "        - 0.26036834716796875\n",
      "        - -0.6673851013183594\n",
      "        - -0.20137405395507812\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 8.921527862548828\n",
      "          mean_q: 3.871673107147217\n",
      "          mean_td_error: 0.03666354715824127\n",
      "          min_q: -3.6450295448303223\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -1.8459515571594238\n",
      "        - -0.6980404853820801\n",
      "        - 3.637944221496582\n",
      "        - 2.39996600151062\n",
      "        - -0.16794109344482422\n",
      "        - -1.6429202556610107\n",
      "        - -0.09728527069091797\n",
      "        - -0.04179859161376953\n",
      "        - 2.3215575218200684\n",
      "        - 0.649592399597168\n",
      "        - 1.5139780044555664\n",
      "        - 2.3638405799865723\n",
      "        - -6.457941055297852\n",
      "        - -5.288530349731445\n",
      "        - -1.0872416496276855\n",
      "        - 0.8814067840576172\n",
      "        - -0.44091570377349854\n",
      "        - 0.40541017055511475\n",
      "        - -2.1555848121643066\n",
      "        - 0.5599377155303955\n",
      "        - 0.10097503662109375\n",
      "        - -0.09728527069091797\n",
      "        - 0.5799260139465332\n",
      "        - 1.356006145477295\n",
      "        - 3.3423593044281006\n",
      "        - 1.599403977394104\n",
      "        - 6.798655033111572\n",
      "        - -10.19432544708252\n",
      "        - 4.817028045654297\n",
      "        - 1.1704552173614502\n",
      "        - -7.6663818359375\n",
      "        - 4.556935787200928\n",
      "    num_agent_steps_sampled: 40408\n",
      "    num_agent_steps_trained: 98880\n",
      "    num_steps_sampled: 20204\n",
      "    num_steps_trained: 49440\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 176\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.85\n",
      "    ram_util_percent: 51.650000000000006\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 14.76\n",
      "    policy1: 14.76\n",
      "  policy_reward_min:\n",
      "    policy0: 6.0\n",
      "    policy1: 6.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10591998769382563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07022296796199161\n",
      "    mean_inference_ms: 1.6086142237996517\n",
      "    mean_raw_obs_processing_ms: 0.3110765610566885\n",
      "  time_since_restore: 89.6533751487732\n",
      "  time_this_iter_s: 1.3529770374298096\n",
      "  time_total_s: 89.6533751487732\n",
      "  timers:\n",
      "    learn_throughput: 3987.135\n",
      "    learn_time_ms: 8.026\n",
      "    load_throughput: 72585.435\n",
      "    load_time_ms: 0.441\n",
      "    update_time_ms: 5.019\n",
      "  timestamp: 1648811587\n",
      "  timesteps_since_restore: 2240\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 20204\n",
      "  training_iteration: 70\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:12 (running for 00:01:50.33)<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">          93.738</td><td style=\"text-align: right;\">20825</td><td style=\"text-align: right;\">   27.58</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">              6.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 42058\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-12\n",
      "  done: false\n",
      "  episode_len_mean: 5.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 28.04\n",
      "  episode_reward_min: 6.0\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1828\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 21003\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 15.605137825012207\n",
      "          mean_q: 7.503571033477783\n",
      "          mean_td_error: -0.1668790578842163\n",
      "          min_q: -1.6695919036865234\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.042363643646240234\n",
      "        - 2.4024460315704346\n",
      "        - -2.0495529174804688\n",
      "        - -2.300631523132324\n",
      "        - 0.2927737236022949\n",
      "        - 0.20853519439697266\n",
      "        - -0.7211768627166748\n",
      "        - 0.5228853225708008\n",
      "        - 0.0068781375885009766\n",
      "        - 0.16663789749145508\n",
      "        - -0.015906810760498047\n",
      "        - -0.6643996238708496\n",
      "        - -0.5206918716430664\n",
      "        - -1.0853424072265625\n",
      "        - -2.1455984115600586\n",
      "        - 2.0560474395751953\n",
      "        - -2.4838714599609375\n",
      "        - 0.3787667751312256\n",
      "        - -0.43286705017089844\n",
      "        - -1.1924371719360352\n",
      "        - 0.5090117454528809\n",
      "        - 0.07056903839111328\n",
      "        - -0.30730271339416504\n",
      "        - 2.7622339725494385\n",
      "        - -1.3790504932403564\n",
      "        - -1.8788766860961914\n",
      "        - -2.3927364349365234\n",
      "        - -1.5983715057373047\n",
      "        - 7.7669453620910645\n",
      "        - -1.163304328918457\n",
      "        - 0.04756426811218262\n",
      "        - -0.15694189071655273\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 12.612784385681152\n",
      "          mean_q: 2.909548282623291\n",
      "          mean_td_error: 0.2633117139339447\n",
      "          min_q: -1.7437701225280762\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.22028225660324097\n",
      "        - 7.307440757751465\n",
      "        - 3.729853868484497\n",
      "        - -0.21559596061706543\n",
      "        - -0.13625997304916382\n",
      "        - 1.4609034061431885\n",
      "        - 0.48714756965637207\n",
      "        - 1.5459508895874023\n",
      "        - 4.123631000518799\n",
      "        - 2.1511917114257812\n",
      "        - -5.9415998458862305\n",
      "        - 0.8255490064620972\n",
      "        - -1.8398470878601074\n",
      "        - 2.2247633934020996\n",
      "        - 0.831043004989624\n",
      "        - 3.188655376434326\n",
      "        - 0.2903628349304199\n",
      "        - 1.4490703344345093\n",
      "        - -1.3393607139587402\n",
      "        - 0.6370006799697876\n",
      "        - -3.195767402648926\n",
      "        - 0.832127571105957\n",
      "        - 0.540334165096283\n",
      "        - 0.1633879542350769\n",
      "        - -0.2224695086479187\n",
      "        - 1.2663002014160156\n",
      "        - -1.6681013107299805\n",
      "        - -0.10442161560058594\n",
      "        - 3.0406882762908936\n",
      "        - -6.110316276550293\n",
      "        - 1.776876449584961\n",
      "        - -8.89284610748291\n",
      "    num_agent_steps_sampled: 42058\n",
      "    num_agent_steps_trained: 106240\n",
      "    num_steps_sampled: 21029\n",
      "    num_steps_trained: 53120\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 184\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.8\n",
      "    ram_util_percent: 51.75\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 14.02\n",
      "    policy1: 14.02\n",
      "  policy_reward_min:\n",
      "    policy0: 3.0\n",
      "    policy1: 3.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10651914583227377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0705829915071555\n",
      "    mean_inference_ms: 1.61402018905317\n",
      "    mean_raw_obs_processing_ms: 0.3143301964331058\n",
      "  time_since_restore: 94.94754695892334\n",
      "  time_this_iter_s: 1.2094988822937012\n",
      "  time_total_s: 94.94754695892334\n",
      "  timers:\n",
      "    learn_throughput: 4159.236\n",
      "    learn_time_ms: 7.694\n",
      "    load_throughput: 67896.463\n",
      "    load_time_ms: 0.471\n",
      "    update_time_ms: 4.732\n",
      "  timestamp: 1648811592\n",
      "  timesteps_since_restore: 2368\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 21029\n",
      "  training_iteration: 74\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:18 (running for 00:01:55.87)<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">          99.065</td><td style=\"text-align: right;\">21648</td><td style=\"text-align: right;\">    30.6</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">               4.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 43720\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-18\n",
      "  done: false\n",
      "  episode_len_mean: 3.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 32.52\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2013\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 21833\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 13.746150970458984\n",
      "          mean_q: 6.81682825088501\n",
      "          mean_td_error: -0.2084493339061737\n",
      "          min_q: -2.269033670425415\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.43277931213378906\n",
      "        - -0.36921072006225586\n",
      "        - 0.553380012512207\n",
      "        - 1.095445156097412\n",
      "        - -1.3824377059936523\n",
      "        - -0.2669558525085449\n",
      "        - 7.668928146362305\n",
      "        - -0.049345970153808594\n",
      "        - -0.02456808090209961\n",
      "        - -0.21499180793762207\n",
      "        - -0.7917861938476562\n",
      "        - 0.12484860420227051\n",
      "        - -1.6607723236083984\n",
      "        - -1.567475438117981\n",
      "        - -1.0196094512939453\n",
      "        - -1.4788265228271484\n",
      "        - -0.8897190093994141\n",
      "        - 0.05545949935913086\n",
      "        - 0.1514376401901245\n",
      "        - -0.7482504844665527\n",
      "        - -4.429797172546387\n",
      "        - -0.4886007308959961\n",
      "        - 0.3734760284423828\n",
      "        - -0.3593564033508301\n",
      "        - -0.32161617279052734\n",
      "        - -0.6469182968139648\n",
      "        - 0.4996764659881592\n",
      "        - 0.7423582077026367\n",
      "        - -0.8015408515930176\n",
      "        - -0.48210620880126953\n",
      "        - -0.02591419219970703\n",
      "        - -0.34836912155151367\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 7.18994140625\n",
      "          mean_q: 3.1304030418395996\n",
      "          mean_td_error: -1.079691767692566\n",
      "          min_q: -5.428798675537109\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -4.5433549880981445\n",
      "        - -8.807720184326172\n",
      "        - -5.43277645111084\n",
      "        - 4.5945305824279785\n",
      "        - 4.0701141357421875\n",
      "        - 3.5339250564575195\n",
      "        - 0.8363032341003418\n",
      "        - -4.706472396850586\n",
      "        - -2.290132522583008\n",
      "        - 2.7509219646453857\n",
      "        - -3.012117862701416\n",
      "        - -1.864206314086914\n",
      "        - 0.05314350128173828\n",
      "        - -2.3735103607177734\n",
      "        - 0.006383419036865234\n",
      "        - -0.13196563720703125\n",
      "        - 0.8748655319213867\n",
      "        - -0.10262775421142578\n",
      "        - 1.7364697456359863\n",
      "        - -4.839619159698486\n",
      "        - -1.5939130783081055\n",
      "        - -3.297687530517578\n",
      "        - -2.2235970497131348\n",
      "        - -0.14467763900756836\n",
      "        - -1.0768065452575684\n",
      "        - -1.893470048904419\n",
      "        - 1.2850892543792725\n",
      "        - -1.0578923225402832\n",
      "        - -4.8244452476501465\n",
      "        - -3.4476332664489746\n",
      "        - 3.488011598587036\n",
      "        - -0.11526870727539062\n",
      "    num_agent_steps_sampled: 43720\n",
      "    num_agent_steps_trained: 114176\n",
      "    num_steps_sampled: 21860\n",
      "    num_steps_trained: 57088\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 192\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.450000000000003\n",
      "    ram_util_percent: 51.8\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.26\n",
      "    policy1: 16.26\n",
      "  policy_reward_min:\n",
      "    policy0: 9.0\n",
      "    policy1: 9.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10697469861563082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07088742158808362\n",
      "    mean_inference_ms: 1.618973944847217\n",
      "    mean_raw_obs_processing_ms: 0.31913854195639063\n",
      "  time_since_restore: 100.57467341423035\n",
      "  time_this_iter_s: 1.5096712112426758\n",
      "  time_total_s: 100.57467341423035\n",
      "  timers:\n",
      "    learn_throughput: 4023.639\n",
      "    learn_time_ms: 7.953\n",
      "    load_throughput: 73746.004\n",
      "    load_time_ms: 0.434\n",
      "    update_time_ms: 4.63\n",
      "  timestamp: 1648811598\n",
      "  timesteps_since_restore: 2496\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 21860\n",
      "  training_iteration: 78\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:23 (running for 00:02:01.77)<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         104.755</td><td style=\"text-align: right;\">22475</td><td style=\"text-align: right;\">    31.7</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">              4.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 45368\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 3.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 32.14\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2212\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 22684\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.05537223815918\n",
      "          mean_q: 8.997303009033203\n",
      "          mean_td_error: 0.1611904501914978\n",
      "          min_q: -2.415780782699585\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.08132076263427734\n",
      "        - 0.4536857604980469\n",
      "        - 0.48641324043273926\n",
      "        - -0.6228733062744141\n",
      "        - -0.3916759490966797\n",
      "        - -0.34851741790771484\n",
      "        - 0.2766728401184082\n",
      "        - 1.1870386600494385\n",
      "        - -1.5412979125976562\n",
      "        - 0.2693452835083008\n",
      "        - 0.07235336303710938\n",
      "        - -0.4670381546020508\n",
      "        - -0.4526076316833496\n",
      "        - 0.22468852996826172\n",
      "        - 0.1823892593383789\n",
      "        - -1.4936599731445312\n",
      "        - 0.23797130584716797\n",
      "        - 0.15347766876220703\n",
      "        - 2.807076930999756\n",
      "        - -1.7399177551269531\n",
      "        - 1.1619939804077148\n",
      "        - -0.7222042083740234\n",
      "        - 2.3219361305236816\n",
      "        - -0.10902023315429688\n",
      "        - 2.7651913166046143\n",
      "        - -0.12975692749023438\n",
      "        - -0.6592617034912109\n",
      "        - 0.9298152923583984\n",
      "        - -0.0705265998840332\n",
      "        - -0.2593727111816406\n",
      "        - 0.40097713470458984\n",
      "        - 0.15347766876220703\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.75103187561035\n",
      "          mean_q: 7.3167009353637695\n",
      "          mean_td_error: -1.0694944858551025\n",
      "          min_q: -1.952476978302002\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.8557977676391602\n",
      "        - 0.41619420051574707\n",
      "        - -2.1379590034484863\n",
      "        - -1.3706340789794922\n",
      "        - -1.7987031936645508\n",
      "        - 0.36784446239471436\n",
      "        - 2.6204452514648438\n",
      "        - -1.9615812301635742\n",
      "        - 0.3043365478515625\n",
      "        - -0.8460788726806641\n",
      "        - -0.7594671249389648\n",
      "        - -4.387308597564697\n",
      "        - 1.889746904373169\n",
      "        - -5.715149879455566\n",
      "        - -9.467567443847656\n",
      "        - -1.119431972503662\n",
      "        - -0.8170757293701172\n",
      "        - -1.7042837142944336\n",
      "        - -0.8455209732055664\n",
      "        - 3.2003378868103027\n",
      "        - -0.8867321014404297\n",
      "        - -1.3706340789794922\n",
      "        - -1.346311092376709\n",
      "        - -0.30434513092041016\n",
      "        - -1.8887073993682861\n",
      "        - -1.3706340789794922\n",
      "        - -2.7612123489379883\n",
      "        - 0.2508573532104492\n",
      "        - -1.262467861175537\n",
      "        - -3.1907429695129395\n",
      "        - 1.1630010604858398\n",
      "        - 3.731760025024414\n",
      "    num_agent_steps_sampled: 45368\n",
      "    num_agent_steps_trained: 122304\n",
      "    num_steps_sampled: 22684\n",
      "    num_steps_trained: 61152\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 200\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.45\n",
      "    ram_util_percent: 51.9\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.07\n",
      "    policy1: 16.07\n",
      "  policy_reward_min:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1072851922404735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07107700281515263\n",
      "    mean_inference_ms: 1.6219957663160052\n",
      "    mean_raw_obs_processing_ms: 0.3237100552881944\n",
      "  time_since_restore: 106.15260887145996\n",
      "  time_this_iter_s: 1.3976819515228271\n",
      "  time_total_s: 106.15260887145996\n",
      "  timers:\n",
      "    learn_throughput: 4091.28\n",
      "    learn_time_ms: 7.822\n",
      "    load_throughput: 67711.496\n",
      "    load_time_ms: 0.473\n",
      "    update_time_ms: 5.041\n",
      "  timestamp: 1648811604\n",
      "  timesteps_since_restore: 2624\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 22684\n",
      "  training_iteration: 82\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:29 (running for 00:02:06.86)<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         110.603</td><td style=\"text-align: right;\">23299</td><td style=\"text-align: right;\">    33.1</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">              3.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 47012\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-30\n",
      "  done: false\n",
      "  episode_len_mean: 3.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.1\n",
      "  episode_reward_min: 24.0\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 2442\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 23506\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 18.013113021850586\n",
      "          mean_q: 7.6880950927734375\n",
      "          mean_td_error: -0.020867682993412018\n",
      "          min_q: 0.47741472721099854\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.809389591217041\n",
      "        - -0.7824668884277344\n",
      "        - -2.5116405487060547\n",
      "        - -0.23265838623046875\n",
      "        - 4.311681747436523\n",
      "        - 0.32550549507141113\n",
      "        - 4.4841742515563965\n",
      "        - 0.30559349060058594\n",
      "        - 0.3843822479248047\n",
      "        - 0.20838594436645508\n",
      "        - -1.6796345710754395\n",
      "        - 4.11464786529541\n",
      "        - -0.18735122680664062\n",
      "        - -0.6126556396484375\n",
      "        - -0.6722822189331055\n",
      "        - -0.18735122680664062\n",
      "        - -1.2421083450317383\n",
      "        - -0.453566312789917\n",
      "        - -0.628199577331543\n",
      "        - 4.1078386306762695\n",
      "        - -1.6496009826660156\n",
      "        - 0.15622568130493164\n",
      "        - -0.18735122680664062\n",
      "        - 0.09863948822021484\n",
      "        - -0.9959716796875\n",
      "        - -0.21497535705566406\n",
      "        - -0.7391400337219238\n",
      "        - -2.1645116806030273\n",
      "        - -1.1404201984405518\n",
      "        - -2.0853238105773926\n",
      "        - -0.1127920150756836\n",
      "        - 0.12455081939697266\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 16.621475219726562\n",
      "          mean_q: 6.802111625671387\n",
      "          mean_td_error: 0.1868899166584015\n",
      "          min_q: -4.514614105224609\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 6.070484638214111\n",
      "        - -0.4333651065826416\n",
      "        - 0.036273956298828125\n",
      "        - -1.6352310180664062\n",
      "        - 3.043728828430176\n",
      "        - -5.812510967254639\n",
      "        - 0.16201114654541016\n",
      "        - 0.1700420379638672\n",
      "        - 2.2342405319213867\n",
      "        - -2.5438709259033203\n",
      "        - -0.03900146484375\n",
      "        - 2.332139730453491\n",
      "        - 2.254426956176758\n",
      "        - -0.9619617462158203\n",
      "        - -0.3158559799194336\n",
      "        - -3.119013786315918\n",
      "        - 1.8141660690307617\n",
      "        - 1.3062396049499512\n",
      "        - -1.626223087310791\n",
      "        - -1.6352310180664062\n",
      "        - 1.4677619934082031\n",
      "        - -1.6352310180664062\n",
      "        - 2.6169302463531494\n",
      "        - -0.5592613220214844\n",
      "        - 3.555988073348999\n",
      "        - -0.4761016368865967\n",
      "        - 3.178042411804199\n",
      "        - -0.818049430847168\n",
      "        - -6.047779560089111\n",
      "        - -1.6352310180664062\n",
      "        - 2.1061484813690186\n",
      "        - 2.9257707595825195\n",
      "    num_agent_steps_sampled: 47012\n",
      "    num_agent_steps_trained: 130944\n",
      "    num_steps_sampled: 23506\n",
      "    num_steps_trained: 65472\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 208\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.25\n",
      "    ram_util_percent: 52.0\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.55\n",
      "    policy1: 16.55\n",
      "  policy_reward_min:\n",
      "    policy0: 12.0\n",
      "    policy1: 12.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10760174314177653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07129446146289337\n",
      "    mean_inference_ms: 1.626041665909014\n",
      "    mean_raw_obs_processing_ms: 0.32890184169998593\n",
      "  time_since_restore: 112.11587691307068\n",
      "  time_this_iter_s: 1.5130720138549805\n",
      "  time_total_s: 112.11587691307068\n",
      "  timers:\n",
      "    learn_throughput: 3756.917\n",
      "    learn_time_ms: 8.518\n",
      "    load_throughput: 62797.795\n",
      "    load_time_ms: 0.51\n",
      "    update_time_ms: 5.388\n",
      "  timestamp: 1648811610\n",
      "  timesteps_since_restore: 2752\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 23506\n",
      "  training_iteration: 86\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:34 (running for 00:02:12.50)<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         115.125</td><td style=\"text-align: right;\">23922</td><td style=\"text-align: right;\">    33.6</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">               3.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 48664\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.2\n",
      "  episode_reward_min: 26.0\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 2691\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 24332\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 18.087295532226562\n",
      "          mean_q: 9.441871643066406\n",
      "          mean_td_error: 0.00913470983505249\n",
      "          min_q: 0.8215368986129761\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.4573628902435303\n",
      "        - 0.5746574401855469\n",
      "        - 0.5746574401855469\n",
      "        - -0.27515125274658203\n",
      "        - -0.07477188110351562\n",
      "        - 2.080082416534424\n",
      "        - 0.0012340545654296875\n",
      "        - -1.974686622619629\n",
      "        - 0.545966625213623\n",
      "        - -0.8052225112915039\n",
      "        - 0.0012340545654296875\n",
      "        - 0.1984882354736328\n",
      "        - 1.819253921508789\n",
      "        - 0.11284637451171875\n",
      "        - -2.2936134338378906\n",
      "        - 0.029491424560546875\n",
      "        - -0.8432035446166992\n",
      "        - 0.04238605499267578\n",
      "        - 0.04238605499267578\n",
      "        - -0.6411275863647461\n",
      "        - -1.6510400772094727\n",
      "        - 0.0012340545654296875\n",
      "        - -1.3151718378067017\n",
      "        - 0.9169015884399414\n",
      "        - 0.38797950744628906\n",
      "        - -0.5388519763946533\n",
      "        - -0.3394460678100586\n",
      "        - 1.1496086120605469\n",
      "        - 0.9831395149230957\n",
      "        - -0.9300093650817871\n",
      "        - 1.0544624328613281\n",
      "        - 0.0012340545654296875\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 16.631977081298828\n",
      "          mean_q: 6.304978370666504\n",
      "          mean_td_error: 0.21301162242889404\n",
      "          min_q: -2.747962474822998\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 8.179071426391602\n",
      "        - 4.491464614868164\n",
      "        - 2.349849224090576\n",
      "        - -1.7048664093017578\n",
      "        - -0.09224319458007812\n",
      "        - -3.612544059753418\n",
      "        - 3.2159652709960938\n",
      "        - -3.8787364959716797\n",
      "        - -1.1289281845092773\n",
      "        - 0.2936187982559204\n",
      "        - 3.167597770690918\n",
      "        - -1.2883796691894531\n",
      "        - -3.6612465381622314\n",
      "        - 5.253652572631836\n",
      "        - -2.152000904083252\n",
      "        - -1.9050073623657227\n",
      "        - 6.210606098175049\n",
      "        - -1.2376213073730469\n",
      "        - -3.7243916988372803\n",
      "        - 1.6462478637695312\n",
      "        - -1.8150653839111328\n",
      "        - 0.49448585510253906\n",
      "        - -0.20781707763671875\n",
      "        - 5.4369707107543945\n",
      "        - -2.1024012565612793\n",
      "        - -2.1124868392944336\n",
      "        - -1.3729991912841797\n",
      "        - -0.6840982437133789\n",
      "        - 0.6707534790039062\n",
      "        - 0.30538153648376465\n",
      "        - -1.2866599559783936\n",
      "        - -0.9317994117736816\n",
      "    num_agent_steps_sampled: 48664\n",
      "    num_agent_steps_trained: 139776\n",
      "    num_steps_sampled: 24332\n",
      "    num_steps_trained: 69888\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 216\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.03333333333333\n",
      "    ram_util_percent: 52.1\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.6\n",
      "    policy1: 16.6\n",
      "  policy_reward_min:\n",
      "    policy0: 13.0\n",
      "    policy1: 13.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10777606411340834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07146306298638944\n",
      "    mean_inference_ms: 1.6287939641225488\n",
      "    mean_raw_obs_processing_ms: 0.33426351445692576\n",
      "  time_since_restore: 118.06886625289917\n",
      "  time_this_iter_s: 1.4789960384368896\n",
      "  time_total_s: 118.06886625289917\n",
      "  timers:\n",
      "    learn_throughput: 4046.774\n",
      "    learn_time_ms: 7.908\n",
      "    load_throughput: 65440.14\n",
      "    load_time_ms: 0.489\n",
      "    update_time_ms: 5.101\n",
      "  timestamp: 1648811616\n",
      "  timesteps_since_restore: 2880\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 24332\n",
      "  training_iteration: 90\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:39 (running for 00:02:17.65)<br>Memory usage on this node: 8.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         121.083</td><td style=\"text-align: right;\">24743</td><td style=\"text-align: right;\">      33</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">               3.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 50300\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-42\n",
      "  done: false\n",
      "  episode_len_mean: 3.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.98\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 2944\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 25054\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.47691535949707\n",
      "          mean_q: 8.407682418823242\n",
      "          mean_td_error: 0.06286823749542236\n",
      "          min_q: -2.134502410888672\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.056301116943359375\n",
      "        - 2.25924015045166\n",
      "        - -0.11838626861572266\n",
      "        - -0.2757568359375\n",
      "        - -0.284912109375\n",
      "        - -0.11732673645019531\n",
      "        - 0.29251527786254883\n",
      "        - 0.10895252227783203\n",
      "        - -0.04439735412597656\n",
      "        - -1.4186639785766602\n",
      "        - 3.814697265625e-06\n",
      "        - 1.8732671737670898\n",
      "        - 3.814697265625e-06\n",
      "        - -0.06687402725219727\n",
      "        - -0.16828536987304688\n",
      "        - -3.8950061798095703\n",
      "        - 0.641803503036499\n",
      "        - -0.284912109375\n",
      "        - 0.060498714447021484\n",
      "        - 0.40332603454589844\n",
      "        - 1.3651390075683594\n",
      "        - 0.015478134155273438\n",
      "        - 1.1539387702941895\n",
      "        - 0.10184669494628906\n",
      "        - 1.0206718444824219\n",
      "        - 0.11256599426269531\n",
      "        - -1.6266404390335083\n",
      "        - 0.016803741455078125\n",
      "        - -0.7086629867553711\n",
      "        - 0.0951080322265625\n",
      "        - -0.21152067184448242\n",
      "        - 1.6556644439697266\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 16.082042694091797\n",
      "          mean_q: 7.679932594299316\n",
      "          mean_td_error: 0.19918453693389893\n",
      "          min_q: -1.5310686826705933\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -1.615971565246582\n",
      "        - 2.7264633178710938\n",
      "        - -0.23694515228271484\n",
      "        - -2.4404540061950684\n",
      "        - 0.5574417114257812\n",
      "        - 3.9038753509521484\n",
      "        - -0.8303422927856445\n",
      "        - 2.2338242530822754\n",
      "        - -2.6133174896240234\n",
      "        - 0.7235579490661621\n",
      "        - -0.6168332099914551\n",
      "        - 0.3711061477661133\n",
      "        - 3.657095432281494\n",
      "        - 0.30679893493652344\n",
      "        - -1.9958629608154297\n",
      "        - -1.1911964416503906\n",
      "        - 1.9980782270431519\n",
      "        - 1.577380657196045\n",
      "        - -5.177985191345215\n",
      "        - 2.0571746826171875\n",
      "        - -1.7502551078796387\n",
      "        - 4.3324103355407715\n",
      "        - -1.085758090019226\n",
      "        - 4.59625244140625\n",
      "        - -0.49694061279296875\n",
      "        - -3.2179675102233887\n",
      "        - -0.6190476417541504\n",
      "        - 6.204056739807129\n",
      "        - -1.860173225402832\n",
      "        - -1.173445701599121\n",
      "        - -2.7674026489257812\n",
      "        - 0.8182878494262695\n",
      "    num_agent_steps_sampled: 50300\n",
      "    num_agent_steps_trained: 148480\n",
      "    num_steps_sampled: 25150\n",
      "    num_steps_trained: 74240\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 223\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.0\n",
      "    ram_util_percent: 52.2\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.99\n",
      "    policy1: 16.99\n",
      "  policy_reward_min:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1080031201505348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07165160380178615\n",
      "    mean_inference_ms: 1.6318473750383589\n",
      "    mean_raw_obs_processing_ms: 0.33963834122090186\n",
      "  time_since_restore: 124.07459306716919\n",
      "  time_this_iter_s: 1.5077135562896729\n",
      "  time_total_s: 124.07459306716919\n",
      "  timers:\n",
      "    learn_throughput: 3948.161\n",
      "    learn_time_ms: 8.105\n",
      "    load_throughput: 72148.432\n",
      "    load_time_ms: 0.444\n",
      "    update_time_ms: 5.014\n",
      "  timestamp: 1648811622\n",
      "  timesteps_since_restore: 3008\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 25150\n",
      "  training_iteration: 94\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:45 (running for 00:02:23.26)<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         125.551</td><td style=\"text-align: right;\">25354</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 51932\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-48\n",
      "  done: false\n",
      "  episode_len_mean: 3.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.88\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 3214\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 25870\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.18553352355957\n",
      "          mean_q: 9.20468521118164\n",
      "          mean_td_error: -0.378124475479126\n",
      "          min_q: 2.0992326736450195\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.14053058624267578\n",
      "        - -0.43143653869628906\n",
      "        - -0.1893024444580078\n",
      "        - -0.5519256591796875\n",
      "        - -2.0222153663635254\n",
      "        - -0.33851099014282227\n",
      "        - -1.0572710037231445\n",
      "        - -0.1812124252319336\n",
      "        - -0.1947951316833496\n",
      "        - 0.9139566421508789\n",
      "        - -0.3907604217529297\n",
      "        - -0.7726316452026367\n",
      "        - 0.22774457931518555\n",
      "        - -2.2759621143341064\n",
      "        - -5.172881126403809\n",
      "        - 1.8913592100143433\n",
      "        - 1.0676398277282715\n",
      "        - 1.252962589263916\n",
      "        - -0.3907604217529297\n",
      "        - -0.5441951751708984\n",
      "        - -0.5597925186157227\n",
      "        - 0.7602710723876953\n",
      "        - 0.40465545654296875\n",
      "        - 5.278876304626465\n",
      "        - -0.43143653869628906\n",
      "        - -0.43143653869628906\n",
      "        - 0.42479705810546875\n",
      "        - -0.5568327903747559\n",
      "        - -9.050495147705078\n",
      "        - 2.3506383895874023\n",
      "        - -0.3809370994567871\n",
      "        - -0.8886241912841797\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.459678649902344\n",
      "          mean_q: 8.6758451461792\n",
      "          mean_td_error: -0.06376908719539642\n",
      "          min_q: -1.191933274269104\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -4.416545867919922\n",
      "        - 2.1554336547851562\n",
      "        - 0.7815084457397461\n",
      "        - -0.29082489013671875\n",
      "        - 2.4656496047973633\n",
      "        - 5.684493541717529\n",
      "        - -1.2942085266113281\n",
      "        - -0.7147645950317383\n",
      "        - -2.3853530883789062\n",
      "        - -0.4175891876220703\n",
      "        - 1.953801155090332\n",
      "        - 1.5877819061279297\n",
      "        - 1.800100326538086\n",
      "        - -0.7147645950317383\n",
      "        - -2.2292609214782715\n",
      "        - -1.4781789779663086\n",
      "        - -0.5986447334289551\n",
      "        - -1.6278605461120605\n",
      "        - 0.5011844635009766\n",
      "        - -1.807440996170044\n",
      "        - 0.11540025472640991\n",
      "        - -0.358245849609375\n",
      "        - 0.37185096740722656\n",
      "        - -0.14791631698608398\n",
      "        - -0.358245849609375\n",
      "        - 2.1554336547851562\n",
      "        - -1.3588368892669678\n",
      "        - 3.7551209926605225\n",
      "        - -0.4175891876220703\n",
      "        - -3.6197457313537598\n",
      "        - -0.4175872802734375\n",
      "        - -0.7147655487060547\n",
      "    num_agent_steps_sampled: 51932\n",
      "    num_agent_steps_trained: 157312\n",
      "    num_steps_sampled: 25966\n",
      "    num_steps_trained: 78656\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 231\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.15\n",
      "    ram_util_percent: 52.3\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.94\n",
      "    policy1: 16.94\n",
      "  policy_reward_min:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10811087436473628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0717361794403284\n",
      "    mean_inference_ms: 1.6330913485118301\n",
      "    mean_raw_obs_processing_ms: 0.34488114305670226\n",
      "  time_since_restore: 129.95566296577454\n",
      "  time_this_iter_s: 1.474266767501831\n",
      "  time_total_s: 129.95566296577454\n",
      "  timers:\n",
      "    learn_throughput: 4315.294\n",
      "    learn_time_ms: 7.415\n",
      "    load_throughput: 72094.176\n",
      "    load_time_ms: 0.444\n",
      "    update_time_ms: 4.754\n",
      "  timestamp: 1648811628\n",
      "  timesteps_since_restore: 3136\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 25966\n",
      "  training_iteration: 98\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:50 (running for 00:02:28.35)<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         131.448</td><td style=\"text-align: right;\">26170</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQNTrainer pid=19093)\u001b[0m 2022-04-01 04:13:52,068\tWARNING deprecation.py:46 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 53564\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 3486\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 26686\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.840282440185547\n",
      "          mean_q: 10.432519912719727\n",
      "          mean_td_error: 0.10497783869504929\n",
      "          min_q: 3.476055145263672\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -1.094869613647461\n",
      "        - -0.07267570495605469\n",
      "        - -0.20101118087768555\n",
      "        - -0.5071282386779785\n",
      "        - 0.7454538345336914\n",
      "        - 0.33957815170288086\n",
      "        - -1.3115243911743164\n",
      "        - -1.3390159606933594\n",
      "        - -2.012603759765625\n",
      "        - -0.48386192321777344\n",
      "        - 4.476055145263672\n",
      "        - -0.3043403625488281\n",
      "        - 0.5544857978820801\n",
      "        - 0.5664348602294922\n",
      "        - -0.20093202590942383\n",
      "        - 1.7081565856933594\n",
      "        - 0.25905704498291016\n",
      "        - -0.23699188232421875\n",
      "        - 2.777095079421997\n",
      "        - -0.3224163055419922\n",
      "        - 0.11425590515136719\n",
      "        - -1.0458641052246094\n",
      "        - -0.23699188232421875\n",
      "        - 1.1884946823120117\n",
      "        - 2.48746395111084\n",
      "        - -1.1178524494171143\n",
      "        - 0.6505670547485352\n",
      "        - -1.0812444686889648\n",
      "        - 0.13609933853149414\n",
      "        - 0.3588685989379883\n",
      "        - -0.10477733612060547\n",
      "        - -1.3286738395690918\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.478899002075195\n",
      "          mean_q: 9.895263671875\n",
      "          mean_td_error: 0.21830391883850098\n",
      "          min_q: 0.3020334243774414\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.0031723976135253906\n",
      "        - -0.3807544708251953\n",
      "        - -2.2674479484558105\n",
      "        - 0.03590869903564453\n",
      "        - -4.609165191650391\n",
      "        - -0.19971942901611328\n",
      "        - -0.5252718925476074\n",
      "        - 0.03590869903564453\n",
      "        - -0.5197982788085938\n",
      "        - 0.8490171432495117\n",
      "        - 0.03590869903564453\n",
      "        - -0.7876186370849609\n",
      "        - 6.168485641479492\n",
      "        - -0.12864017486572266\n",
      "        - 0.03590869903564453\n",
      "        - -0.7876186370849609\n",
      "        - -0.7876186370849609\n",
      "        - 8.097996711730957\n",
      "        - -0.3807544708251953\n",
      "        - 2.89290189743042\n",
      "        - -0.7876186370849609\n",
      "        - 0.03590869903564453\n",
      "        - -0.5197982788085938\n",
      "        - 0.7878987193107605\n",
      "        - -0.3807544708251953\n",
      "        - -5.2933549880981445\n",
      "        - 0.03590869903564453\n",
      "        - -0.9843850135803223\n",
      "        - 5.628276824951172\n",
      "        - -1.3001317977905273\n",
      "        - 2.979801654815674\n",
      "        - 0.003173351287841797\n",
      "    num_agent_steps_sampled: 53564\n",
      "    num_agent_steps_trained: 166016\n",
      "    num_steps_sampled: 26782\n",
      "    num_steps_trained: 83008\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 239\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.200000000000003\n",
      "    ram_util_percent: 52.45\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_min:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10833652486327564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07189112131301911\n",
      "    mean_inference_ms: 1.6356518279826966\n",
      "    mean_raw_obs_processing_ms: 0.3500659613630738\n",
      "  time_since_restore: 135.9205424785614\n",
      "  time_this_iter_s: 1.4367523193359375\n",
      "  time_total_s: 135.9205424785614\n",
      "  timers:\n",
      "    learn_throughput: 4327.58\n",
      "    learn_time_ms: 7.394\n",
      "    load_throughput: 67824.412\n",
      "    load_time_ms: 0.472\n",
      "    update_time_ms: 4.496\n",
      "  timestamp: 1648811635\n",
      "  timesteps_since_restore: 3264\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 26782\n",
      "  training_iteration: 102\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:13:56 (running for 00:02:34.16)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         135.921</td><td style=\"text-align: right;\">26782</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 54788\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-14-01\n",
      "  done: false\n",
      "  episode_len_mean: 3.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.88\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 3688\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 27298\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 18.397287368774414\n",
      "          mean_q: 10.352800369262695\n",
      "          mean_td_error: 0.1370566487312317\n",
      "          min_q: 1.4363470077514648\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.5709223747253418\n",
      "        - 0.43103599548339844\n",
      "        - -0.06715583801269531\n",
      "        - 0.45772552490234375\n",
      "        - -0.6518914699554443\n",
      "        - 0.4334087371826172\n",
      "        - 0.011240959167480469\n",
      "        - -0.29571533203125\n",
      "        - 0.45772552490234375\n",
      "        - 1.6446046829223633\n",
      "        - 0.38999176025390625\n",
      "        - 0.05527973175048828\n",
      "        - 0.4537992477416992\n",
      "        - -0.3125772476196289\n",
      "        - 1.224705696105957\n",
      "        - -0.3125772476196289\n",
      "        - -1.0210990905761719\n",
      "        - -0.6605806350708008\n",
      "        - 0.004054069519042969\n",
      "        - -0.3364238739013672\n",
      "        - 0.06642532348632812\n",
      "        - -0.06715583801269531\n",
      "        - -0.06715583801269531\n",
      "        - 0.4320392608642578\n",
      "        - 0.004054069519042969\n",
      "        - 0.7966756820678711\n",
      "        - 0.45772552490234375\n",
      "        - -0.5029773712158203\n",
      "        - 0.32715606689453125\n",
      "        - 0.042858123779296875\n",
      "        - 0.4577293395996094\n",
      "        - -0.03803539276123047\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 18.485319137573242\n",
      "          mean_q: 10.605175971984863\n",
      "          mean_td_error: 0.5122687220573425\n",
      "          min_q: 3.7980446815490723\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 2.5408029556274414\n",
      "        - -0.1880359649658203\n",
      "        - 3.138598918914795\n",
      "        - 1.451223373413086\n",
      "        - 2.779444694519043\n",
      "        - 2.198126792907715\n",
      "        - -1.0757436752319336\n",
      "        - 2.468513011932373\n",
      "        - -1.3330268859863281\n",
      "        - -1.219679832458496\n",
      "        - 0.31786537170410156\n",
      "        - -0.5299539566040039\n",
      "        - -0.6670026779174805\n",
      "        - -1.5454139709472656\n",
      "        - -1.153080940246582\n",
      "        - -3.916576385498047\n",
      "        - 2.358950138092041\n",
      "        - -0.13956260681152344\n",
      "        - 2.977375030517578\n",
      "        - -1.1046504974365234\n",
      "        - 0.31786537170410156\n",
      "        - 0.4495120048522949\n",
      "        - -1.219679832458496\n",
      "        - 2.1867246627807617\n",
      "        - 2.1867246627807617\n",
      "        - 0.21520042419433594\n",
      "        - 0.4271240234375\n",
      "        - 0.6164565086364746\n",
      "        - -1.3586063385009766\n",
      "        - 2.4682698249816895\n",
      "        - 0.4776005744934082\n",
      "        - 2.2672338485717773\n",
      "    num_agent_steps_sampled: 54788\n",
      "    num_agent_steps_trained: 172544\n",
      "    num_steps_sampled: 27394\n",
      "    num_steps_trained: 86272\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 245\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.775000000000006\n",
      "    ram_util_percent: 55.025\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.94\n",
      "    policy1: 16.94\n",
      "  policy_reward_min:\n",
      "    policy0: 14.0\n",
      "    policy1: 14.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10906988912332727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0723180354461312\n",
      "    mean_inference_ms: 1.6473441921825316\n",
      "    mean_raw_obs_processing_ms: 0.35608452267645985\n",
      "  time_since_restore: 141.66076350212097\n",
      "  time_this_iter_s: 2.2647926807403564\n",
      "  time_total_s: 141.66076350212097\n",
      "  timers:\n",
      "    learn_throughput: 2137.058\n",
      "    learn_time_ms: 14.974\n",
      "    load_throughput: 32773.601\n",
      "    load_time_ms: 0.976\n",
      "    update_time_ms: 12.424\n",
      "  timestamp: 1648811641\n",
      "  timesteps_since_restore: 3392\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 27394\n",
      "  training_iteration: 106\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:14:01 (running for 00:02:39.21)<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         141.661</td><td style=\"text-align: right;\">27394</td><td style=\"text-align: right;\">   33.88</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">              3.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:14:07 (running for 00:02:44.93)<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         146.142</td><td style=\"text-align: right;\">27802</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 56012\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 3892\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 27910\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 18.113245010375977\n",
      "          mean_q: 10.728208541870117\n",
      "          mean_td_error: 0.4734596610069275\n",
      "          min_q: 2.3994948863983154\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.4753098487854004\n",
      "        - 0.7502470016479492\n",
      "        - -0.6772093772888184\n",
      "        - -0.034951210021972656\n",
      "        - 2.0293121337890625\n",
      "        - 0.2321786880493164\n",
      "        - -0.15713024139404297\n",
      "        - -0.034951210021972656\n",
      "        - -0.034951210021972656\n",
      "        - 0.03040313720703125\n",
      "        - -0.5768413543701172\n",
      "        - 0.9327106475830078\n",
      "        - 0.054927825927734375\n",
      "        - 0.03342294692993164\n",
      "        - -5.008996963500977\n",
      "        - -0.034951210021972656\n",
      "        - -0.0013914108276367188\n",
      "        - 2.350078582763672\n",
      "        - 0.3150815963745117\n",
      "        - -0.09434986114501953\n",
      "        - 0.4291839599609375\n",
      "        - 2.417572021484375\n",
      "        - 0.4172029495239258\n",
      "        - 8.417230606079102\n",
      "        - -0.5768413543701172\n",
      "        - 0.24263954162597656\n",
      "        - 0.24263954162597656\n",
      "        - 0.24263954162597656\n",
      "        - 2.1892781257629395\n",
      "        - 0.24263954162597656\n",
      "        - 0.02579498291015625\n",
      "        - 0.31278228759765625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.963077545166016\n",
      "          mean_q: 11.526248931884766\n",
      "          mean_td_error: 0.6368125677108765\n",
      "          min_q: -0.9019485712051392\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.4879703521728516\n",
      "        - -1.6159441471099854\n",
      "        - 2.266388416290283\n",
      "        - -6.7431511878967285\n",
      "        - 1.1404180526733398\n",
      "        - 0.15920448303222656\n",
      "        - -2.934390068054199\n",
      "        - 3.9804468154907227\n",
      "        - 0.07934188842773438\n",
      "        - -0.306782603263855\n",
      "        - 0.5801496505737305\n",
      "        - -0.2766456604003906\n",
      "        - 4.377141952514648\n",
      "        - -0.9773797988891602\n",
      "        - 1.8534822463989258\n",
      "        - 1.9573183059692383\n",
      "        - 3.841749668121338\n",
      "        - -0.4578971862792969\n",
      "        - 0.7191877365112305\n",
      "        - 1.6054542064666748\n",
      "        - 1.2258920669555664\n",
      "        - -0.40357112884521484\n",
      "        - 2.2488603591918945\n",
      "        - -0.2766456604003906\n",
      "        - 0.36028480529785156\n",
      "        - 2.3864402770996094\n",
      "        - 0.33936309814453125\n",
      "        - 3.7339649200439453\n",
      "        - -2.6118381023406982\n",
      "        - 2.11968994140625\n",
      "        - 0.3602886199951172\n",
      "        - 0.1592082977294922\n",
      "    num_agent_steps_sampled: 56012\n",
      "    num_agent_steps_trained: 179072\n",
      "    num_steps_sampled: 28006\n",
      "    num_steps_trained: 89536\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 251\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.05\n",
      "    ram_util_percent: 61.849999999999994\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_min:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11183635853587298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07319662092175179\n",
      "    mean_inference_ms: 1.6725590033001878\n",
      "    mean_raw_obs_processing_ms: 0.36463818631121375\n",
      "  time_since_restore: 148.12532496452332\n",
      "  time_this_iter_s: 1.9835398197174072\n",
      "  time_total_s: 148.12532496452332\n",
      "  timers:\n",
      "    learn_throughput: 3009.778\n",
      "    learn_time_ms: 10.632\n",
      "    load_throughput: 46839.2\n",
      "    load_time_ms: 0.683\n",
      "    update_time_ms: 7.021\n",
      "  timestamp: 1648811648\n",
      "  timesteps_since_restore: 3552\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 28006\n",
      "  training_iteration: 111\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:14:12 (running for 00:02:50.40)<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         152.395</td><td style=\"text-align: right;\">28516</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 57440\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 4130\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 28624\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.77568244934082\n",
      "          mean_q: 12.024489402770996\n",
      "          mean_td_error: -0.13698144257068634\n",
      "          min_q: 2.923290252685547\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.36968135833740234\n",
      "        - -0.5135736465454102\n",
      "        - 1.046504020690918\n",
      "        - -0.10833549499511719\n",
      "        - -0.6027355194091797\n",
      "        - 1.6478519439697266\n",
      "        - 0.03274726867675781\n",
      "        - 0.3634309768676758\n",
      "        - -0.10833549499511719\n",
      "        - -0.8186893463134766\n",
      "        - -0.10833549499511719\n",
      "        - -0.10833549499511719\n",
      "        - 0.6165981292724609\n",
      "        - -0.5391883850097656\n",
      "        - 0.8749475479125977\n",
      "        - 0.027570247650146484\n",
      "        - 3.035682201385498\n",
      "        - -0.05753040313720703\n",
      "        - 0.03274726867675781\n",
      "        - -1.8059883117675781\n",
      "        - -1.9894142150878906\n",
      "        - -4.179079055786133\n",
      "        - -0.10833549499511719\n",
      "        - -0.5391883850097656\n",
      "        - -0.5742301940917969\n",
      "        - 0.24062347412109375\n",
      "        - -0.5102272033691406\n",
      "        - 1.69834566116333\n",
      "        - 0.03274726867675781\n",
      "        - 0.2421884536743164\n",
      "        - -1.4343595504760742\n",
      "        - -0.5391902923583984\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.066133499145508\n",
      "          mean_q: 10.142770767211914\n",
      "          mean_td_error: 0.4224279224872589\n",
      "          min_q: -0.5867009162902832\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 3.1829700469970703\n",
      "        - -0.1304607391357422\n",
      "        - -0.1304607391357422\n",
      "        - 2.313797950744629\n",
      "        - 0.5285773277282715\n",
      "        - 0.8753499984741211\n",
      "        - -0.2340679168701172\n",
      "        - 2.4404945373535156\n",
      "        - 2.4589614868164062\n",
      "        - -0.1304607391357422\n",
      "        - -0.7432308197021484\n",
      "        - -0.6002645492553711\n",
      "        - 0.3397560119628906\n",
      "        - -0.1304607391357422\n",
      "        - -1.273094654083252\n",
      "        - 3.304464340209961\n",
      "        - -1.950007438659668\n",
      "        - 3.1829700469970703\n",
      "        - -1.0453863143920898\n",
      "        - 1.9207286834716797\n",
      "        - 0.5379562377929688\n",
      "        - -1.0626716613769531\n",
      "        - -1.1200037002563477\n",
      "        - -0.6879253387451172\n",
      "        - -2.6357827186584473\n",
      "        - 0.5044946670532227\n",
      "        - 0.4960343837738037\n",
      "        - 0.1653904914855957\n",
      "        - 1.4975671768188477\n",
      "        - 0.9041767120361328\n",
      "        - 1.1603343486785889\n",
      "        - -0.42205238342285156\n",
      "    num_agent_steps_sampled: 57440\n",
      "    num_agent_steps_trained: 186688\n",
      "    num_steps_sampled: 28720\n",
      "    num_steps_trained: 93344\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 258\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.2\n",
      "    ram_util_percent: 63.95\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_min:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11258490992728296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07364072532109182\n",
      "    mean_inference_ms: 1.682683154424597\n",
      "    mean_raw_obs_processing_ms: 0.37030793300447923\n",
      "  time_since_restore: 154.0466549396515\n",
      "  time_this_iter_s: 1.651200771331787\n",
      "  time_total_s: 154.0466549396515\n",
      "  timers:\n",
      "    learn_throughput: 3940.927\n",
      "    learn_time_ms: 8.12\n",
      "    load_throughput: 64231.302\n",
      "    load_time_ms: 0.498\n",
      "    update_time_ms: 5.345\n",
      "  timestamp: 1648811654\n",
      "  timesteps_since_restore: 3680\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 28720\n",
      "  training_iteration: 115\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:14:17 (running for 00:02:55.47)<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         157.308</td><td style=\"text-align: right;\">29128</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 58664\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-14-19\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 4334\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29236\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 18.05947494506836\n",
      "          mean_q: 13.764782905578613\n",
      "          mean_td_error: 0.17610222101211548\n",
      "          min_q: 5.303815841674805\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 2.2279796600341797\n",
      "        - 0.21038436889648438\n",
      "        - -1.0966205596923828\n",
      "        - -0.14990615844726562\n",
      "        - -1.0966205596923828\n",
      "        - -0.28546905517578125\n",
      "        - 0.5570449829101562\n",
      "        - 0.5570449829101562\n",
      "        - 0.2869596481323242\n",
      "        - 0.5570449829101562\n",
      "        - 1.8668041229248047\n",
      "        - 0.25156450271606445\n",
      "        - -0.19667911529541016\n",
      "        - -0.26991748809814453\n",
      "        - -0.26991748809814453\n",
      "        - -0.9535198211669922\n",
      "        - -0.40148258209228516\n",
      "        - -0.07723140716552734\n",
      "        - 0.5570449829101562\n",
      "        - -0.26991748809814453\n",
      "        - 0.5570449829101562\n",
      "        - 0.9256391525268555\n",
      "        - -0.14990615844726562\n",
      "        - -0.26991748809814453\n",
      "        - 0.5570449829101562\n",
      "        - -0.9185695648193359\n",
      "        - 0.3577594757080078\n",
      "        - -0.14990615844726562\n",
      "        - -0.44033002853393555\n",
      "        - 0.5570449829101562\n",
      "        - 0.5570449829101562\n",
      "        - 2.047731399536133\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.723033905029297\n",
      "          mean_q: 10.648662567138672\n",
      "          mean_td_error: 0.40413597226142883\n",
      "          min_q: -3.198136329650879\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.24573612213134766\n",
      "        - 0.13913440704345703\n",
      "        - -0.8395562171936035\n",
      "        - -0.5448818206787109\n",
      "        - -0.28327178955078125\n",
      "        - 4.760904312133789\n",
      "        - -0.5084285736083984\n",
      "        - -1.4391298294067383\n",
      "        - -0.5084285736083984\n",
      "        - 0.3551158905029297\n",
      "        - 0.20314931869506836\n",
      "        - -0.5448818206787109\n",
      "        - 0.9196314811706543\n",
      "        - -0.6303882598876953\n",
      "        - -0.39934539794921875\n",
      "        - 1.6822423934936523\n",
      "        - -0.8550035953521729\n",
      "        - -2.7816038131713867\n",
      "        - 2.0908212661743164\n",
      "        - -2.3769869804382324\n",
      "        - -1.1147089004516602\n",
      "        - 0.2974395751953125\n",
      "        - -0.5084285736083984\n",
      "        - 4.927670478820801\n",
      "        - -0.28327178955078125\n",
      "        - -0.29781246185302734\n",
      "        - 3.5355491638183594\n",
      "        - -0.5448818206787109\n",
      "        - 4.760904312133789\n",
      "        - -0.39934539794921875\n",
      "        - 1.8051776885986328\n",
      "        - 2.5607028007507324\n",
      "    num_agent_steps_sampled: 58664\n",
      "    num_agent_steps_trained: 193216\n",
      "    num_steps_sampled: 29332\n",
      "    num_steps_trained: 96608\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 264\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.96666666666667\n",
      "    ram_util_percent: 64.89999999999999\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_min:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1131042812514341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07399609544172496\n",
      "    mean_inference_ms: 1.6894235852504602\n",
      "    mean_raw_obs_processing_ms: 0.3746902529500823\n",
      "  time_since_restore: 159.07523822784424\n",
      "  time_this_iter_s: 1.767186164855957\n",
      "  time_total_s: 159.07523822784424\n",
      "  timers:\n",
      "    learn_throughput: 3515.863\n",
      "    learn_time_ms: 9.102\n",
      "    load_throughput: 58421.576\n",
      "    load_time_ms: 0.548\n",
      "    update_time_ms: 5.526\n",
      "  timestamp: 1648811659\n",
      "  timesteps_since_restore: 3776\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 29332\n",
      "  training_iteration: 118\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:14:22 (running for 00:03:00.59)<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>RUNNING </td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">          162.27</td><td style=\"text-align: right;\">29740</td><td style=\"text-align: right;\">    33.8</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">               3.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_76743_00000:\n",
      "  agent_timesteps_total: 60292\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-14-25\n",
      "  done: true\n",
      "  episode_len_mean: 3.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.98\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 4599\n",
      "  experiment_id: 6af836e00d10403b9eacf3717e17bd18\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 30058\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 18.256772994995117\n",
      "          mean_q: 11.504793167114258\n",
      "          mean_td_error: 0.5185688138008118\n",
      "          min_q: 0.4442082643508911\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 3.143796443939209\n",
      "        - -0.24910831451416016\n",
      "        - -0.6808938980102539\n",
      "        - 0.17530250549316406\n",
      "        - -0.12719345092773438\n",
      "        - 0.4777870178222656\n",
      "        - 0.4033021926879883\n",
      "        - 0.38840293884277344\n",
      "        - 0.4777870178222656\n",
      "        - 0.38840293884277344\n",
      "        - 0.17530250549316406\n",
      "        - -1.026634693145752\n",
      "        - 0.6451239585876465\n",
      "        - 0.4777870178222656\n",
      "        - 3.143796443939209\n",
      "        - 0.38840293884277344\n",
      "        - 0.1222982406616211\n",
      "        - 1.222787857055664\n",
      "        - -0.7691874504089355\n",
      "        - 1.1854963302612305\n",
      "        - -0.403958797454834\n",
      "        - -0.12719345092773438\n",
      "        - -0.4992713928222656\n",
      "        - 0.09194564819335938\n",
      "        - 0.1956644058227539\n",
      "        - 2.9152965545654297\n",
      "        - 0.4033021926879883\n",
      "        - 0.5570492744445801\n",
      "        - 2.664550304412842\n",
      "        - 0.17032480239868164\n",
      "        - 0.9512357711791992\n",
      "        - -0.2875022888183594\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 17.738323211669922\n",
      "          mean_q: 10.177362442016602\n",
      "          mean_td_error: -0.19344809651374817\n",
      "          min_q: -1.7169404029846191\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.07377958297729492\n",
      "        - -0.8875885009765625\n",
      "        - 1.9649238586425781\n",
      "        - -0.8235244750976562\n",
      "        - -0.5381629467010498\n",
      "        - -0.3794822692871094\n",
      "        - -0.35564613342285156\n",
      "        - -0.1419696807861328\n",
      "        - 1.6529541015625\n",
      "        - -0.1419696807861328\n",
      "        - -0.03097677230834961\n",
      "        - -1.478689193725586\n",
      "        - -0.35564613342285156\n",
      "        - 0.2412252426147461\n",
      "        - -0.3794822692871094\n",
      "        - -0.1419696807861328\n",
      "        - -0.5919551849365234\n",
      "        - -1.0376992225646973\n",
      "        - -0.35564613342285156\n",
      "        - -4.348989963531494\n",
      "        - -0.9439144134521484\n",
      "        - 0.6216421127319336\n",
      "        - 0.5948138236999512\n",
      "        - -0.27096450328826904\n",
      "        - -0.2877025604248047\n",
      "        - -0.2575511932373047\n",
      "        - 0.7352861166000366\n",
      "        - -0.1419696807861328\n",
      "        - -0.36146974563598633\n",
      "        - 3.036858081817627\n",
      "        - -0.35564613342285156\n",
      "        - -0.35564613342285156\n",
      "    num_agent_steps_sampled: 60292\n",
      "    num_agent_steps_trained: 201856\n",
      "    num_steps_sampled: 30146\n",
      "    num_steps_trained: 100928\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 272\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.599999999999994\n",
      "    ram_util_percent: 65.5\n",
      "  pid: 19093\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: 16.99\n",
      "    policy1: 16.99\n",
      "  policy_reward_min:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11358885963466381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07425440646045173\n",
      "    mean_inference_ms: 1.6952667343245957\n",
      "    mean_raw_obs_processing_ms: 0.3796016697808349\n",
      "  time_since_restore: 165.43360114097595\n",
      "  time_this_iter_s: 1.5825769901275635\n",
      "  time_total_s: 165.43360114097595\n",
      "  timers:\n",
      "    learn_throughput: 3851.917\n",
      "    learn_time_ms: 8.308\n",
      "    load_throughput: 67314.172\n",
      "    load_time_ms: 0.475\n",
      "    update_time_ms: 5.052\n",
      "  timestamp: 1648811665\n",
      "  timesteps_since_restore: 3904\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 30146\n",
      "  training_iteration: 122\n",
      "  trial_id: '76743_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:14:26 (running for 00:03:03.96)<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:11:22/DQNTrainer_2022-04-01_04-11-22<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_76743_00000</td><td>TERMINATED</td><td>172.17.0.2:19093</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         165.434</td><td style=\"text-align: right;\">30146</td><td style=\"text-align: right;\">   33.98</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">              3.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m 2022-04-01 04:14:26,433\tERROR worker.py:432 -- SystemExit was raised from the worker.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 639, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1156, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1235, in exit_actor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"python/ray/_raylet.pyx\", line 770, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"python/ray/_raylet.pyx\", line 591, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 121, in format_exception\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     capture_locals=capture_locals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 363, in extract\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 285, in line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 136, in updatecache\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     with tokenize.open(fullname) as fp:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/tokenize.py\", line 451, in open\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     text = TextIOWrapper(buffer, encoding, line_buffering=True)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 429, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19092)\u001b[0m SystemExit: 1\n",
      "2022-04-01 04:14:26,549\tINFO tune.py:636 -- Total run time: 184.40 seconds (183.91 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.dqn as dqn\n",
    "\n",
    "e = FoodGame(coop_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "config[\"prioritized_replay\"] = True\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "# eval\n",
    "# config[\"evaluation_interval\"] = 1\n",
    "# config[\"evaluation_duration\"] = 1\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    dqn.DQNTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58954483",
   "metadata": {},
   "source": [
    "# Results Analysis (Coop + DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b256265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 00:36:20,237\tWARNING deprecation.py:46 -- DeprecationWarning: `config['buffer_size']` has been deprecated. Use `config['replay_buffer_config']['capacity']` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14301)\u001b[0m 2022-04-01 00:36:26,116\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-04-01 00:36:28,342\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-04-01 00:36:28,508\tINFO trainable.py:473 -- Restored on 172.17.0.2 from checkpoint: /home/ray/cs4246/01_04_2022_00:33:00/DQNTrainer_2022-04-01_00-33-00/DQNTrainer_foodenv_f5013_00000_0_2022-04-01_00-33-00/checkpoint_000100/checkpoint-100\n",
      "2022-04-01 00:36:28,509\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': 3200, '_time_total': 126.66100001335144, '_episodes_total': 4709}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state at Time 0:\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 1\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 2\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 3\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 4\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '1' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 5\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '1' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode Ended\n",
      "Episode Rewards {'agent0': 15, 'agent1': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14301)\u001b[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14301)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14301)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    }
   ],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "\n",
    "trainer = dqn.DQNTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(best_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f061be",
   "metadata": {},
   "source": [
    "# DQN Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "138acf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:14:52 (running for 00:00:00.14)<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQNTrainer pid=20461)\u001b[0m 2022-04-01 04:14:57,078\tINFO trainer.py:2055 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=20461)\u001b[0m 2022-04-01 04:14:57,079\tINFO simple_q.py:154 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=20461)\u001b[0m 2022-04-01 04:14:57,079\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=20461)\u001b[0m 2022-04-01 04:14:57,079\tWARNING deprecation.py:46 -- DeprecationWarning: `config['buffer_size']` has been deprecated. Use `config['replay_buffer_config']['capacity']` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20460)\u001b[0m 2022-04-01 04:15:03,571\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:06 (running for 00:00:14.02)<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQNTrainer pid=20461)\u001b[0m 2022-04-01 04:15:06,204\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20460)\u001b[0m 2022-04-01 04:15:06,291\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:07 (running for 00:00:15.03)<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 2038\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-09\n",
      "  done: false\n",
      "  episode_len_mean: 17.56896551724138\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -24.275862068965516\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 58\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 1019\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 0.6649793982505798\n",
      "          mean_q: 0.06474529951810837\n",
      "          mean_td_error: 0.7047158479690552\n",
      "          min_q: -0.5340701937675476\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.0407304763793945\n",
      "        - 1.7569246292114258\n",
      "        - 2.4864189624786377\n",
      "        - 2.121119737625122\n",
      "        - 2.077812671661377\n",
      "        - 0.9304206967353821\n",
      "        - 1.3943556547164917\n",
      "        - 2.363179922103882\n",
      "        - -8.617629051208496\n",
      "        - 0.9559100866317749\n",
      "        - -7.877291679382324\n",
      "        - 1.0501161813735962\n",
      "        - 1.309735894203186\n",
      "        - 1.74990713596344\n",
      "        - 1.6455446481704712\n",
      "        - 1.8755619525909424\n",
      "        - 1.8323277235031128\n",
      "        - 2.121119737625122\n",
      "        - 1.7277536392211914\n",
      "        - 2.14385724067688\n",
      "        - -8.40459156036377\n",
      "        - 1.2789502143859863\n",
      "        - 1.6617405414581299\n",
      "        - 2.086656093597412\n",
      "        - 1.847369909286499\n",
      "        - 1.3019031286239624\n",
      "        - 1.0506703853607178\n",
      "        - 1.822333812713623\n",
      "        - 1.8114181756973267\n",
      "        - 1.280498743057251\n",
      "        - 1.4380931854248047\n",
      "        - 1.2879873514175415\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: 0.28372201323509216\n",
      "          mean_q: -0.2282947450876236\n",
      "          mean_td_error: 0.6768286228179932\n",
      "          min_q: -0.9416021704673767\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.0295138359069824\n",
      "        - 0.8170213103294373\n",
      "        - 0.416683554649353\n",
      "        - 0.42153939604759216\n",
      "        - 0.6190270185470581\n",
      "        - 0.20554423332214355\n",
      "        - 0.7445655465126038\n",
      "        - 1.0844581127166748\n",
      "        - 0.9018027782440186\n",
      "        - -0.15134745836257935\n",
      "        - 0.201668381690979\n",
      "        - 0.39682766795158386\n",
      "        - 0.6972134709358215\n",
      "        - 0.6388363838195801\n",
      "        - 0.7104793787002563\n",
      "        - 0.07440754771232605\n",
      "        - 0.6532926559448242\n",
      "        - 0.994727373123169\n",
      "        - 0.7843175530433655\n",
      "        - 0.4632169008255005\n",
      "        - 0.14565527439117432\n",
      "        - 0.6149972677230835\n",
      "        - 1.1373193264007568\n",
      "        - 0.6168556809425354\n",
      "        - 0.986014723777771\n",
      "        - 1.166850209236145\n",
      "        - 0.8486711978912354\n",
      "        - 0.697024941444397\n",
      "        - 0.7396522760391235\n",
      "        - 1.030390977859497\n",
      "        - 1.4532266855239868\n",
      "        - 0.5180611610412598\n",
      "    num_agent_steps_sampled: 2038\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1019\n",
      "    num_steps_trained: 32\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.31666666666667\n",
      "    ram_util_percent: 64.35000000000001\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.327586206896552\n",
      "    policy1: -13.948275862068966\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13389119914933745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08011495365816004\n",
      "    mean_inference_ms: 1.992952589895211\n",
      "    mean_raw_obs_processing_ms: 0.3382505155077168\n",
      "  time_since_restore: 3.5881972312927246\n",
      "  time_this_iter_s: 3.5881972312927246\n",
      "  time_total_s: 3.5881972312927246\n",
      "  timers:\n",
      "    learn_throughput: 83.81\n",
      "    learn_time_ms: 381.816\n",
      "    load_throughput: 81790.206\n",
      "    load_time_ms: 0.391\n",
      "    update_time_ms: 6.48\n",
      "  timestamp: 1648811709\n",
      "  timesteps_since_restore: 32\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 1019\n",
      "  training_iteration: 1\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:12 (running for 00:00:20.24)<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         6.04135</td><td style=\"text-align: right;\">1654</td><td style=\"text-align: right;\">-24.9247</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           17.7849</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 5216\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-16\n",
      "  done: false\n",
      "  episode_len_mean: 18.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: -26.8\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 145\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 2588\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -10.824976921081543\n",
      "          mean_q: -11.952216148376465\n",
      "          mean_td_error: 0.15570113062858582\n",
      "          min_q: -12.53102970123291\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.1191015243530273\n",
      "        - 1.0873136520385742\n",
      "        - 0.6142587661743164\n",
      "        - 0.8652582168579102\n",
      "        - 0.9700965881347656\n",
      "        - 0.7548952102661133\n",
      "        - 0.7548952102661133\n",
      "        - 0.8159418106079102\n",
      "        - 0.7146711349487305\n",
      "        - 0.5946483612060547\n",
      "        - -9.064878463745117\n",
      "        - 1.031336784362793\n",
      "        - 0.9700965881347656\n",
      "        - 0.7486820220947266\n",
      "        - 0.5764188766479492\n",
      "        - 1.0455150604248047\n",
      "        - -9.274691581726074\n",
      "        - 0.22556304931640625\n",
      "        - 0.996312141418457\n",
      "        - 0.9158535003662109\n",
      "        - 0.6725358963012695\n",
      "        - 0.9119606018066406\n",
      "        - 0.6392793655395508\n",
      "        - 0.49342823028564453\n",
      "        - 0.8069877624511719\n",
      "        - 1.0486478805541992\n",
      "        - 0.5643110275268555\n",
      "        - 0.894622802734375\n",
      "        - 0.5048923492431641\n",
      "        - 0.7782316207885742\n",
      "        - 0.5399541854858398\n",
      "        - 0.6662960052490234\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -16.189739227294922\n",
      "          mean_q: -20.851694107055664\n",
      "          mean_td_error: 0.64900141954422\n",
      "          min_q: -22.508487701416016\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.9597949981689453\n",
      "        - 0.45792579650878906\n",
      "        - -0.3435554504394531\n",
      "        - 0.6316471099853516\n",
      "        - -0.319580078125\n",
      "        - 0.7462120056152344\n",
      "        - 0.7419471740722656\n",
      "        - 0.8687305450439453\n",
      "        - 0.9696121215820312\n",
      "        - 0.6059589385986328\n",
      "        - 0.6949043273925781\n",
      "        - 0.2746601104736328\n",
      "        - 0.6944904327392578\n",
      "        - 0.1997356414794922\n",
      "        - 0.5642871856689453\n",
      "        - 1.0054206848144531\n",
      "        - 0.7364864349365234\n",
      "        - 0.8053913116455078\n",
      "        - 0.42272186279296875\n",
      "        - 0.5239009857177734\n",
      "        - 0.7917919158935547\n",
      "        - 0.7496681213378906\n",
      "        - 1.4078407287597656\n",
      "        - 0.9849987030029297\n",
      "        - 0.7086086273193359\n",
      "        - 0.2161579132080078\n",
      "        - -0.15982818603515625\n",
      "        - 0.8542423248291016\n",
      "        - 1.021728515625\n",
      "        - 0.8536205291748047\n",
      "        - 0.6796875\n",
      "        - 1.4188365936279297\n",
      "    num_agent_steps_sampled: 5216\n",
      "    num_agent_steps_trained: 5632\n",
      "    num_steps_sampled: 2608\n",
      "    num_steps_trained: 2816\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 15\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.1\n",
      "    ram_util_percent: 58.2\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 9.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -11.1\n",
      "    policy1: -15.7\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12615124786391635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07651685214444205\n",
      "    mean_inference_ms: 1.8522550148399455\n",
      "    mean_raw_obs_processing_ms: 0.32184299062330884\n",
      "  time_since_restore: 9.664087533950806\n",
      "  time_this_iter_s: 1.34916090965271\n",
      "  time_total_s: 9.664087533950806\n",
      "  timers:\n",
      "    learn_throughput: 4355.004\n",
      "    learn_time_ms: 7.348\n",
      "    load_throughput: 73721.701\n",
      "    load_time_ms: 0.434\n",
      "    update_time_ms: 4.922\n",
      "  timestamp: 1648811716\n",
      "  timesteps_since_restore: 192\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 2608\n",
      "  training_iteration: 6\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:17 (running for 00:00:25.30)<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         10.8767</td><td style=\"text-align: right;\">2927</td><td style=\"text-align: right;\">  -25.88</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 7830\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-21\n",
      "  done: false\n",
      "  episode_len_mean: 17.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: -24.24\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 220\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 3815\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -7.777599334716797\n",
      "          mean_q: -15.26637077331543\n",
      "          mean_td_error: -0.06331312656402588\n",
      "          min_q: -16.97015953063965\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.3578014373779297\n",
      "        - 0.44939613342285156\n",
      "        - 0.4143810272216797\n",
      "        - 0.5985050201416016\n",
      "        - 0.1995105743408203\n",
      "        - -4.723553657531738\n",
      "        - 0.44110679626464844\n",
      "        - 0.01125335693359375\n",
      "        - 0.2763099670410156\n",
      "        - 0.17108440399169922\n",
      "        - 1.2573623657226562\n",
      "        - 0.4281597137451172\n",
      "        - 1.7544078826904297\n",
      "        - 0.3039436340332031\n",
      "        - 1.2847509384155273\n",
      "        - -14.34310245513916\n",
      "        - 1.9346332550048828\n",
      "        - 0.44060707092285156\n",
      "        - 1.6240234375\n",
      "        - 1.1190605163574219\n",
      "        - 0.5087337493896484\n",
      "        - 0.7909622192382812\n",
      "        - 0.9764194488525391\n",
      "        - -0.2902350425720215\n",
      "        - 0.9636831283569336\n",
      "        - 1.4307079315185547\n",
      "        - 1.3805742263793945\n",
      "        - -4.044145584106445\n",
      "        - 0.5247249603271484\n",
      "        - 1.0222845077514648\n",
      "        - 0.39019298553466797\n",
      "        - -0.6795654296875\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -26.82979965209961\n",
      "          mean_q: -29.44297218322754\n",
      "          mean_td_error: -0.02535015344619751\n",
      "          min_q: -30.8150691986084\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.44991493225097656\n",
      "        - 0.8143463134765625\n",
      "        - 1.0226726531982422\n",
      "        - 1.4483509063720703\n",
      "        - 1.4963512420654297\n",
      "        - 0.6337394714355469\n",
      "        - 0.2685890197753906\n",
      "        - 0.5863914489746094\n",
      "        - 0.8565177917480469\n",
      "        - 1.1898918151855469\n",
      "        - 0.9783515930175781\n",
      "        - 1.3263168334960938\n",
      "        - 1.4254264831542969\n",
      "        - 0.4992256164550781\n",
      "        - 0.6887092590332031\n",
      "        - 0.9335536956787109\n",
      "        - 0.1862945556640625\n",
      "        - 1.2072811126708984\n",
      "        - 1.3117408752441406\n",
      "        - 1.1700115203857422\n",
      "        - 1.0971012115478516\n",
      "        - 1.0769386291503906\n",
      "        - 0.8998737335205078\n",
      "        - 1.7108726501464844\n",
      "        - 0.15192604064941406\n",
      "        - 1.329092025756836\n",
      "        - 1.303323745727539\n",
      "        - -0.11161231994628906\n",
      "        - -29.384803771972656\n",
      "        - 1.225198745727539\n",
      "        - 1.0779304504394531\n",
      "        - 0.3192768096923828\n",
      "    num_agent_steps_sampled: 7830\n",
      "    num_agent_steps_trained: 10432\n",
      "    num_steps_sampled: 3915\n",
      "    num_steps_trained: 5216\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 26\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.85\n",
      "    ram_util_percent: 58.3\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 9.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.67\n",
      "    policy1: -14.57\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11948489538430394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07403760458412079\n",
      "    mean_inference_ms: 1.7418326520064285\n",
      "    mean_raw_obs_processing_ms: 0.30931797589241206\n",
      "  time_since_restore: 14.674546003341675\n",
      "  time_this_iter_s: 1.2327139377593994\n",
      "  time_total_s: 14.674546003341675\n",
      "  timers:\n",
      "    learn_throughput: 4079.492\n",
      "    learn_time_ms: 7.844\n",
      "    load_throughput: 74441.336\n",
      "    load_time_ms: 0.43\n",
      "    update_time_ms: 4.712\n",
      "  timestamp: 1648811721\n",
      "  timesteps_since_restore: 320\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 3915\n",
      "  training_iteration: 10\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:22 (running for 00:00:30.45)<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         15.8036</td><td style=\"text-align: right;\">4230</td><td style=\"text-align: right;\">  -25.76</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 10996\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-27\n",
      "  done: false\n",
      "  episode_len_mean: 18.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: -26.42\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 307\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 5398\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -24.400650024414062\n",
      "          mean_q: -25.36679458618164\n",
      "          mean_td_error: -0.6194554567337036\n",
      "          min_q: -26.608016967773438\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.2937183380126953\n",
      "        - 1.1435909271240234\n",
      "        - -23.56553077697754\n",
      "        - 0.8293952941894531\n",
      "        - 0.3928050994873047\n",
      "        - 0.5535430908203125\n",
      "        - 0.7357120513916016\n",
      "        - 1.158987045288086\n",
      "        - 0.6201019287109375\n",
      "        - 0.9122543334960938\n",
      "        - 0.7165679931640625\n",
      "        - 0.6201019287109375\n",
      "        - 0.8808059692382812\n",
      "        - 0.58367919921875\n",
      "        - 1.4886627197265625\n",
      "        - 1.6402759552001953\n",
      "        - 0.6748237609863281\n",
      "        - 1.0614395141601562\n",
      "        - 1.0523853302001953\n",
      "        - 1.338388442993164\n",
      "        - -23.888874053955078\n",
      "        - 1.2618179321289062\n",
      "        - 1.1584587097167969\n",
      "        - 0.5089855194091797\n",
      "        - 0.8211174011230469\n",
      "        - 0.7722911834716797\n",
      "        - 0.39933204650878906\n",
      "        - 0.8049373626708984\n",
      "        - 1.7282428741455078\n",
      "        - 1.1145782470703125\n",
      "        - 0.47756004333496094\n",
      "        - 0.8872699737548828\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -34.755409240722656\n",
      "          mean_q: -38.84788513183594\n",
      "          mean_td_error: -2.139754295349121\n",
      "          min_q: -40.631168365478516\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.6298637390136719\n",
      "        - 0.36234283447265625\n",
      "        - 0.39117431640625\n",
      "        - 0.8243980407714844\n",
      "        - -9.431900024414062\n",
      "        - 0.13673019409179688\n",
      "        - 0.4318199157714844\n",
      "        - 0.13844680786132812\n",
      "        - 0.1134033203125\n",
      "        - 0.3106880187988281\n",
      "        - 0.264801025390625\n",
      "        - 0.3404884338378906\n",
      "        - 0.5232505798339844\n",
      "        - 0.2926063537597656\n",
      "        - -34.97145080566406\n",
      "        - 0.4458122253417969\n",
      "        - 0.31285858154296875\n",
      "        - 0.6676101684570312\n",
      "        - 0.2885704040527344\n",
      "        - -0.06823348999023438\n",
      "        - 0.4894599914550781\n",
      "        - 0.30843353271484375\n",
      "        - 0.5669174194335938\n",
      "        - 0.48278045654296875\n",
      "        - 0.3986320495605469\n",
      "        - 0.1330718994140625\n",
      "        - 1.1767692565917969\n",
      "        - 0.32614898681640625\n",
      "        - -35.24887466430664\n",
      "        - 0.1883697509765625\n",
      "        - 0.24571609497070312\n",
      "        - 0.4571533203125\n",
      "    num_agent_steps_sampled: 10996\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 5498\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 40\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.05\n",
      "    ram_util_percent: 58.55\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 10.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -12.11\n",
      "    policy1: -14.31\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11711017185870742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07310721307872203\n",
      "    mean_inference_ms: 1.7066920220650292\n",
      "    mean_raw_obs_processing_ms: 0.305307353965393\n",
      "  time_since_restore: 20.667280673980713\n",
      "  time_this_iter_s: 1.2721467018127441\n",
      "  time_total_s: 20.667280673980713\n",
      "  timers:\n",
      "    learn_throughput: 4165.73\n",
      "    learn_time_ms: 7.682\n",
      "    load_throughput: 74260.113\n",
      "    load_time_ms: 0.431\n",
      "    update_time_ms: 4.787\n",
      "  timestamp: 1648811727\n",
      "  timesteps_since_restore: 480\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 5498\n",
      "  training_iteration: 15\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:27 (running for 00:00:35.54)<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         20.6673</td><td style=\"text-align: right;\">5498</td><td style=\"text-align: right;\">  -26.42</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:32 (running for 00:00:40.71)<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         24.6712</td><td style=\"text-align: right;\">6468</td><td style=\"text-align: right;\">  -23.62</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 13598\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-33\n",
      "  done: false\n",
      "  episode_len_mean: 16.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -21.84\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 386\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 6729\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -35.30793762207031\n",
      "          mean_q: -36.36309814453125\n",
      "          mean_td_error: -2.5804555416107178\n",
      "          min_q: -37.02524185180664\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.111236572265625\n",
      "        - -0.5400886535644531\n",
      "        - 0.38048553466796875\n",
      "        - 0.39666748046875\n",
      "        - 0.24942398071289062\n",
      "        - -0.12961196899414062\n",
      "        - -0.3407859802246094\n",
      "        - -0.1884307861328125\n",
      "        - 0.10164642333984375\n",
      "        - 0.21698379516601562\n",
      "        - -0.5404472351074219\n",
      "        - -0.5437202453613281\n",
      "        - 0.13698196411132812\n",
      "        - -0.3875007629394531\n",
      "        - 0.3604621887207031\n",
      "        - -0.3213462829589844\n",
      "        - -34.82188034057617\n",
      "        - 0.3326911926269531\n",
      "        - -0.07429122924804688\n",
      "        - -0.5185279846191406\n",
      "        - 0.03522491455078125\n",
      "        - 0.16021347045898438\n",
      "        - -10.542991638183594\n",
      "        - -0.13846969604492188\n",
      "        - -0.2631492614746094\n",
      "        - 0.034881591796875\n",
      "        - 0.029994964599609375\n",
      "        - 0.26194000244140625\n",
      "        - -0.5821380615234375\n",
      "        - -0.18795394897460938\n",
      "        - -35.241336822509766\n",
      "        - 0.20173263549804688\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -40.47690200805664\n",
      "          mean_q: -45.79728698730469\n",
      "          mean_td_error: -2.30707049369812\n",
      "          min_q: -48.50951385498047\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.5489044189453125\n",
      "        - -46.851226806640625\n",
      "        - 1.0889167785644531\n",
      "        - 1.5861167907714844\n",
      "        - 1.28643798828125\n",
      "        - 1.3823394775390625\n",
      "        - 1.2139129638671875\n",
      "        - 2.0492401123046875\n",
      "        - 1.4087905883789062\n",
      "        - 1.3056983947753906\n",
      "        - -3.4678916931152344\n",
      "        - 0.7102470397949219\n",
      "        - 0.9619216918945312\n",
      "        - 1.3649749755859375\n",
      "        - 1.3546104431152344\n",
      "        - 1.7344741821289062\n",
      "        - 0.4510536193847656\n",
      "        - 0.9364013671875\n",
      "        - 0.3949127197265625\n",
      "        - 1.3908119201660156\n",
      "        - 0.7076072692871094\n",
      "        - 0.9469337463378906\n",
      "        - -39.47690200805664\n",
      "        - 1.3446540832519531\n",
      "        - 1.0974655151367188\n",
      "        - 0.8665885925292969\n",
      "        - 0.5603065490722656\n",
      "        - 1.4178428649902344\n",
      "        - 1.0594558715820312\n",
      "        - -14.529243469238281\n",
      "        - 0.5574874877929688\n",
      "        - 0.7709007263183594\n",
      "    num_agent_steps_sampled: 13598\n",
      "    num_agent_steps_trained: 20992\n",
      "    num_steps_sampled: 6799\n",
      "    num_steps_trained: 10496\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 52\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.8\n",
      "    ram_util_percent: 58.7\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 12.0\n",
      "    policy1: 10.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -11.82\n",
      "    policy1: -10.02\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11717254865221928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07310211800062925\n",
      "    mean_inference_ms: 1.704901569800908\n",
      "    mean_raw_obs_processing_ms: 0.3054787016808981\n",
      "  time_since_restore: 26.044888734817505\n",
      "  time_this_iter_s: 1.3737313747406006\n",
      "  time_total_s: 26.044888734817505\n",
      "  timers:\n",
      "    learn_throughput: 4088.564\n",
      "    learn_time_ms: 7.827\n",
      "    load_throughput: 71548.445\n",
      "    load_time_ms: 0.447\n",
      "    update_time_ms: 5.034\n",
      "  timestamp: 1648811733\n",
      "  timesteps_since_restore: 608\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 6799\n",
      "  training_iteration: 19\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:38 (running for 00:00:45.85)<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         29.5284</td><td style=\"text-align: right;\">7654</td><td style=\"text-align: right;\">  -20.94</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             16.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 15952\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 16.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -21.76\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 456\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 7936\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -42.6901969909668\n",
      "          mean_q: -44.66306686401367\n",
      "          mean_td_error: -5.379543781280518\n",
      "          min_q: -46.06056213378906\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.22397994995117188\n",
      "        - 0.5646934509277344\n",
      "        - -0.3327789306640625\n",
      "        - -0.620513916015625\n",
      "        - -0.3621978759765625\n",
      "        - 0.5623207092285156\n",
      "        - -0.24502182006835938\n",
      "        - -44.39485549926758\n",
      "        - -0.3277130126953125\n",
      "        - -0.3383216857910156\n",
      "        - 1.0039253234863281\n",
      "        - 0.07370376586914062\n",
      "        - -0.6860618591308594\n",
      "        - -44.230159759521484\n",
      "        - 0.6489944458007812\n",
      "        - -42.309532165527344\n",
      "        - -0.3615760803222656\n",
      "        - 0.7703361511230469\n",
      "        - 0.0069427490234375\n",
      "        - -0.3325920104980469\n",
      "        - 0.22035598754882812\n",
      "        - -0.0678863525390625\n",
      "        - 0.076141357421875\n",
      "        - -41.93252182006836\n",
      "        - -0.19614028930664062\n",
      "        - -0.2920112609863281\n",
      "        - 0.3991584777832031\n",
      "        - 0.32636260986328125\n",
      "        - -0.119720458984375\n",
      "        - 0.20044708251953125\n",
      "        - -0.1630401611328125\n",
      "        - 0.08989715576171875\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -39.86189651489258\n",
      "          mean_q: -48.2598991394043\n",
      "          mean_td_error: -1.1373740434646606\n",
      "          min_q: -51.98493957519531\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -46.34169387817383\n",
      "        - -0.3104591369628906\n",
      "        - 0.15424346923828125\n",
      "        - -0.3431587219238281\n",
      "        - 0.7583847045898438\n",
      "        - 1.012176513671875\n",
      "        - -0.6601943969726562\n",
      "        - -0.9134712219238281\n",
      "        - 0.7710075378417969\n",
      "        - 0.881134033203125\n",
      "        - -0.342559814453125\n",
      "        - -0.7769126892089844\n",
      "        - 0.31048583984375\n",
      "        - 0.2742652893066406\n",
      "        - 0.9930534362792969\n",
      "        - -1.2377357482910156\n",
      "        - -0.3072357177734375\n",
      "        - 2.1510467529296875\n",
      "        - -0.7116889953613281\n",
      "        - 1.0912361145019531\n",
      "        - 0.20953369140625\n",
      "        - 0.5410385131835938\n",
      "        - 0.06424713134765625\n",
      "        - 0.7512702941894531\n",
      "        - -0.6177215576171875\n",
      "        - 3.4557647705078125\n",
      "        - -0.3863563537597656\n",
      "        - 0.126953125\n",
      "        - 4.0825653076171875\n",
      "        - -0.20387649536132812\n",
      "        - -0.516357421875\n",
      "        - -0.3549537658691406\n",
      "    num_agent_steps_sampled: 15952\n",
      "    num_agent_steps_trained: 25344\n",
      "    num_steps_sampled: 7976\n",
      "    num_steps_trained: 12672\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 63\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.1\n",
      "    ram_util_percent: 58.8\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 7.0\n",
      "    policy1: 8.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.13\n",
      "    policy1: -11.63\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11788229159021366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07349423744419191\n",
      "    mean_inference_ms: 1.7131662260305691\n",
      "    mean_raw_obs_processing_ms: 0.3070970636871319\n",
      "  time_since_restore: 30.85513401031494\n",
      "  time_this_iter_s: 1.326737403869629\n",
      "  time_total_s: 30.85513401031494\n",
      "  timers:\n",
      "    learn_throughput: 3935.392\n",
      "    learn_time_ms: 8.131\n",
      "    load_throughput: 70827.297\n",
      "    load_time_ms: 0.452\n",
      "    update_time_ms: 4.955\n",
      "  timestamp: 1648811738\n",
      "  timesteps_since_restore: 736\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 7976\n",
      "  training_iteration: 23\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:43 (running for 00:00:51.01)<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         34.5079</td><td style=\"text-align: right;\">8924</td><td style=\"text-align: right;\">  -23.48</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 18484\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 17.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -25.08\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 528\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 9242\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -52.877140045166016\n",
      "          mean_q: -53.72016143798828\n",
      "          mean_td_error: -2.418001890182495\n",
      "          min_q: -54.571537017822266\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.036006927490234375\n",
      "        - -0.23481369018554688\n",
      "        - 0.9946098327636719\n",
      "        - 0.07114028930664062\n",
      "        - -9.174995422363281\n",
      "        - -0.47945404052734375\n",
      "        - 1.42645263671875\n",
      "        - -0.5676383972167969\n",
      "        - -9.4267578125\n",
      "        - -0.4753456115722656\n",
      "        - 0.5123558044433594\n",
      "        - 0.2702751159667969\n",
      "        - -53.1571159362793\n",
      "        - 0.22906112670898438\n",
      "        - 0.9578666687011719\n",
      "        - -0.3477783203125\n",
      "        - -0.3221015930175781\n",
      "        - 0.09702301025390625\n",
      "        - 0.4785270690917969\n",
      "        - 0.146331787109375\n",
      "        - 0.6633872985839844\n",
      "        - 0.12984085083007812\n",
      "        - 0.38593292236328125\n",
      "        - 0.5107612609863281\n",
      "        - -0.11361312866210938\n",
      "        - -0.09075164794921875\n",
      "        - -0.5797309875488281\n",
      "        - 1.1257553100585938\n",
      "        - -0.01345062255859375\n",
      "        - -0.7630081176757812\n",
      "        - -9.4267578125\n",
      "        - -0.23807525634765625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -45.133018493652344\n",
      "          mean_q: -49.1434211730957\n",
      "          mean_td_error: 0.6992886066436768\n",
      "          min_q: -54.03697204589844\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.5142822265625\n",
      "        - 0.7763786315917969\n",
      "        - 0.6636810302734375\n",
      "        - 1.8366661071777344\n",
      "        - 0.6980438232421875\n",
      "        - -0.3438835144042969\n",
      "        - 0.7557563781738281\n",
      "        - 0.6100845336914062\n",
      "        - 0.6272125244140625\n",
      "        - 0.1875762939453125\n",
      "        - 0.9461669921875\n",
      "        - 0.7086563110351562\n",
      "        - 0.5268821716308594\n",
      "        - 1.0399589538574219\n",
      "        - 1.1738052368164062\n",
      "        - -0.08582305908203125\n",
      "        - -0.18853759765625\n",
      "        - 1.7315711975097656\n",
      "        - 0.42739105224609375\n",
      "        - 0.441192626953125\n",
      "        - 0.4981575012207031\n",
      "        - 0.3122367858886719\n",
      "        - -0.36093902587890625\n",
      "        - -0.04703521728515625\n",
      "        - 1.1981735229492188\n",
      "        - 2.942523956298828\n",
      "        - 0.1150970458984375\n",
      "        - 2.3193626403808594\n",
      "        - 0.7380561828613281\n",
      "        - 0.4561271667480469\n",
      "        - 0.6579742431640625\n",
      "        - 0.5004386901855469\n",
      "    num_agent_steps_sampled: 18484\n",
      "    num_agent_steps_trained: 29888\n",
      "    num_steps_sampled: 9242\n",
      "    num_steps_trained: 14944\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 75\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.5\n",
      "    ram_util_percent: 59.0\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -12.19\n",
      "    policy1: -12.89\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1180912886269694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07347088134938827\n",
      "    mean_inference_ms: 1.715324817677009\n",
      "    mean_raw_obs_processing_ms: 0.30729309999808924\n",
      "  time_since_restore: 35.77274298667908\n",
      "  time_this_iter_s: 1.264892816543579\n",
      "  time_total_s: 35.77274298667908\n",
      "  timers:\n",
      "    learn_throughput: 3874.948\n",
      "    learn_time_ms: 8.258\n",
      "    load_throughput: 68006.55\n",
      "    load_time_ms: 0.471\n",
      "    update_time_ms: 4.812\n",
      "  timestamp: 1648811743\n",
      "  timesteps_since_restore: 864\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 9242\n",
      "  training_iteration: 27\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:48 (running for 00:00:56.30)<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         39.5194</td><td style=\"text-align: right;\">10203</td><td style=\"text-align: right;\">  -26.72</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 20842\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-48\n",
      "  done: false\n",
      "  episode_len_mean: 18.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: -27.74\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 592\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 10375\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -62.68682098388672\n",
      "          mean_q: -66.73705291748047\n",
      "          mean_td_error: -8.515669822692871\n",
      "          min_q: -68.62277221679688\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.0878143310546875\n",
      "        - -75.9627456665039\n",
      "        - -0.4271659851074219\n",
      "        - -1.0844573974609375\n",
      "        - 0.2740325927734375\n",
      "        - 0.1460418701171875\n",
      "        - 0.5300140380859375\n",
      "        - -0.6036300659179688\n",
      "        - 0.7822494506835938\n",
      "        - -0.941253662109375\n",
      "        - -67.62277221679688\n",
      "        - 0.152252197265625\n",
      "        - 0.1015167236328125\n",
      "        - -61.68682098388672\n",
      "        - -0.3468475341796875\n",
      "        - -0.4321441650390625\n",
      "        - 0.02167510986328125\n",
      "        - -1.1725997924804688\n",
      "        - -0.07459259033203125\n",
      "        - -0.04827117919921875\n",
      "        - 0.25577545166015625\n",
      "        - 0.416229248046875\n",
      "        - -0.9269561767578125\n",
      "        - -65.919677734375\n",
      "        - 0.46012115478515625\n",
      "        - -1.4019699096679688\n",
      "        - 1.5453414916992188\n",
      "        - 0.6257171630859375\n",
      "        - -0.2126312255859375\n",
      "        - 0.23388671875\n",
      "        - -0.5897445678710938\n",
      "        - 0.32019805908203125\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -51.94941329956055\n",
      "          mean_q: -58.1398811340332\n",
      "          mean_td_error: -0.34800171852111816\n",
      "          min_q: -62.893089294433594\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.23228073120117188\n",
      "        - 0.6771812438964844\n",
      "        - -0.5895347595214844\n",
      "        - -1.2439804077148438\n",
      "        - -0.7717971801757812\n",
      "        - -0.2064361572265625\n",
      "        - -0.13341522216796875\n",
      "        - -0.8293418884277344\n",
      "        - -0.7322998046875\n",
      "        - -0.7066230773925781\n",
      "        - -0.019626617431640625\n",
      "        - -0.4809837341308594\n",
      "        - -0.6815872192382812\n",
      "        - 0.6623153686523438\n",
      "        - 0.5890007019042969\n",
      "        - 0.5158882141113281\n",
      "        - -0.40933990478515625\n",
      "        - -0.3851585388183594\n",
      "        - 0.7084693908691406\n",
      "        - 0.3687858581542969\n",
      "        - 0.6174430847167969\n",
      "        - -5.066989898681641\n",
      "        - 0.5570411682128906\n",
      "        - -0.446380615234375\n",
      "        - 1.2906532287597656\n",
      "        - -1.0979576110839844\n",
      "        - -0.5788345336914062\n",
      "        - -5.160579681396484\n",
      "        - 0.7059288024902344\n",
      "        - -0.2856483459472656\n",
      "        - 0.24211883544921875\n",
      "        - 1.5233535766601562\n",
      "    num_agent_steps_sampled: 20842\n",
      "    num_agent_steps_trained: 33984\n",
      "    num_steps_sampled: 10421\n",
      "    num_steps_trained: 16992\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 85\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.9\n",
      "    ram_util_percent: 59.1\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -14.32\n",
      "    policy1: -13.42\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11813653677371072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07343796292205168\n",
      "    mean_inference_ms: 1.7153431476376624\n",
      "    mean_raw_obs_processing_ms: 0.3068666051378679\n",
      "  time_since_restore: 40.55185031890869\n",
      "  time_this_iter_s: 1.0324592590332031\n",
      "  time_total_s: 40.55185031890869\n",
      "  timers:\n",
      "    learn_throughput: 4217.407\n",
      "    learn_time_ms: 7.588\n",
      "    load_throughput: 67378.378\n",
      "    load_time_ms: 0.475\n",
      "    update_time_ms: 4.683\n",
      "  timestamp: 1648811748\n",
      "  timesteps_since_restore: 992\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 10421\n",
      "  training_iteration: 31\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:53 (running for 00:01:01.30)<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         44.3328</td><td style=\"text-align: right;\">11401</td><td style=\"text-align: right;\">   -26.1</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 23432\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 17.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -26.86\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 666\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 11696\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -67.0409927368164\n",
      "          mean_q: -70.81454467773438\n",
      "          mean_td_error: -1.9792735576629639\n",
      "          min_q: -73.5518569946289\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.3636016845703125\n",
      "        - 0.30025482177734375\n",
      "        - -0.2123260498046875\n",
      "        - -0.05278778076171875\n",
      "        - 0.5384979248046875\n",
      "        - -72.39453887939453\n",
      "        - 0.09502410888671875\n",
      "        - 0.880950927734375\n",
      "        - 0.27454376220703125\n",
      "        - 0.25504302978515625\n",
      "        - -0.136749267578125\n",
      "        - 0.12415313720703125\n",
      "        - -0.01886749267578125\n",
      "        - 0.8955078125\n",
      "        - 0.43035125732421875\n",
      "        - -0.287078857421875\n",
      "        - -0.23914337158203125\n",
      "        - 0.19158172607421875\n",
      "        - 0.3669586181640625\n",
      "        - 0.4100799560546875\n",
      "        - 0.291290283203125\n",
      "        - 1.0546188354492188\n",
      "        - 0.5496063232421875\n",
      "        - 0.02935791015625\n",
      "        - 0.084625244140625\n",
      "        - 0.29327392578125\n",
      "        - 0.07196807861328125\n",
      "        - 1.0598831176757812\n",
      "        - 0.21450042724609375\n",
      "        - -0.25537872314453125\n",
      "        - 1.0607528686523438\n",
      "        - 0.4236907958984375\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -56.50678634643555\n",
      "          mean_q: -63.798912048339844\n",
      "          mean_td_error: -4.476251125335693\n",
      "          min_q: -68.175048828125\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.30933380126953125\n",
      "        - -9.500991821289062\n",
      "        - -0.24755859375\n",
      "        - 0.554962158203125\n",
      "        - -0.10619354248046875\n",
      "        - 0.9619140625\n",
      "        - 0.4372749328613281\n",
      "        - 1.0649871826171875\n",
      "        - -0.00234222412109375\n",
      "        - 0.70013427734375\n",
      "        - -0.38895416259765625\n",
      "        - -0.6142921447753906\n",
      "        - 0.20198822021484375\n",
      "        - -66.36663818359375\n",
      "        - 0.2904510498046875\n",
      "        - 0.3156280517578125\n",
      "        - 0.3939552307128906\n",
      "        - -0.06683349609375\n",
      "        - 1.181915283203125\n",
      "        - -66.29269409179688\n",
      "        - 0.9429550170898438\n",
      "        - -1.6199493408203125\n",
      "        - -0.02188873291015625\n",
      "        - 1.1129035949707031\n",
      "        - 0.30218505859375\n",
      "        - -0.3095855712890625\n",
      "        - -0.103118896484375\n",
      "        - 1.1394424438476562\n",
      "        - -9.469108581542969\n",
      "        - -0.1682891845703125\n",
      "        - 0.8963432312011719\n",
      "        - 1.232025146484375\n",
      "    num_agent_steps_sampled: 23432\n",
      "    num_agent_steps_trained: 38656\n",
      "    num_steps_sampled: 11716\n",
      "    num_steps_trained: 19328\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 97\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.55\n",
      "    ram_util_percent: 59.2\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 7.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -13.83\n",
      "    policy1: -13.03\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1177242393581561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07324417236923227\n",
      "    mean_inference_ms: 1.7187721304736123\n",
      "    mean_raw_obs_processing_ms: 0.306251206748591\n",
      "  time_since_restore: 45.482110023498535\n",
      "  time_this_iter_s: 1.1492671966552734\n",
      "  time_total_s: 45.482110023498535\n",
      "  timers:\n",
      "    learn_throughput: 4166.519\n",
      "    learn_time_ms: 7.68\n",
      "    load_throughput: 68949.824\n",
      "    load_time_ms: 0.464\n",
      "    update_time_ms: 4.664\n",
      "  timestamp: 1648811753\n",
      "  timesteps_since_restore: 1120\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 11716\n",
      "  training_iteration: 35\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:15:58 (running for 00:01:06.42)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         50.1303</td><td style=\"text-align: right;\">12837</td><td style=\"text-align: right;\">  -30.08</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 26274\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 19.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: -32.06\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 739\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 13097\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -63.89541244506836\n",
      "          mean_q: -67.17355346679688\n",
      "          mean_td_error: -11.069683074951172\n",
      "          min_q: -70.63358306884766\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -1.449188232421875\n",
      "        - -1.3108596801757812\n",
      "        - 0.22731781005859375\n",
      "        - -63.178855895996094\n",
      "        - -0.5715408325195312\n",
      "        - 1.1113853454589844\n",
      "        - -0.5841445922851562\n",
      "        - -0.8771591186523438\n",
      "        - -1.3963470458984375\n",
      "        - -0.03653717041015625\n",
      "        - -63.4176025390625\n",
      "        - -0.5751953125\n",
      "        - -0.0557098388671875\n",
      "        - -0.008209228515625\n",
      "        - 0.20114898681640625\n",
      "        - -74.12574768066406\n",
      "        - -1.683990478515625\n",
      "        - 1.3396835327148438\n",
      "        - 0.8783950805664062\n",
      "        - -1.0702056884765625\n",
      "        - -0.762481689453125\n",
      "        - -3.0806503295898438\n",
      "        - -66.76253509521484\n",
      "        - 0.28067779541015625\n",
      "        - -0.40941619873046875\n",
      "        - 0.5591812133789062\n",
      "        - -63.236305236816406\n",
      "        - -0.8481674194335938\n",
      "        - 0.16326904296875\n",
      "        - -13.398521423339844\n",
      "        - 0.432586669921875\n",
      "        - -0.5841522216796875\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -56.993202209472656\n",
      "          mean_q: -64.96992492675781\n",
      "          mean_td_error: -0.8060928583145142\n",
      "          min_q: -67.62451171875\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.4812660217285156\n",
      "        - 0.618560791015625\n",
      "        - -1.9679412841796875\n",
      "        - 0.46395111083984375\n",
      "        - -0.05994415283203125\n",
      "        - -1.7122726440429688\n",
      "        - -0.11138916015625\n",
      "        - -0.0711822509765625\n",
      "        - -0.31192779541015625\n",
      "        - -0.12834930419921875\n",
      "        - 0.2108001708984375\n",
      "        - 0.0968780517578125\n",
      "        - -0.69512939453125\n",
      "        - 0.4400787353515625\n",
      "        - -0.55517578125\n",
      "        - -0.02526092529296875\n",
      "        - 0.00041961669921875\n",
      "        - 0.2811126708984375\n",
      "        - -8.442970275878906\n",
      "        - -0.41289520263671875\n",
      "        - -0.883148193359375\n",
      "        - 1.2390289306640625\n",
      "        - 0.5341873168945312\n",
      "        - 0.3529510498046875\n",
      "        - 0.7402992248535156\n",
      "        - -7.9650726318359375\n",
      "        - -0.7070770263671875\n",
      "        - 0.51513671875\n",
      "        - -0.00341033935546875\n",
      "        - 0.6195030212402344\n",
      "        - 0.26891326904296875\n",
      "        - -8.604911804199219\n",
      "    num_agent_steps_sampled: 26274\n",
      "    num_agent_steps_trained: 43200\n",
      "    num_steps_sampled: 13137\n",
      "    num_steps_trained: 21600\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 109\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.2\n",
      "    ram_util_percent: 59.4\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 7.0\n",
      "    policy1: 7.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -14.38\n",
      "    policy1: -17.68\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1181125742419757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07360047842433215\n",
      "    mean_inference_ms: 1.7246321491779215\n",
      "    mean_raw_obs_processing_ms: 0.30695507069881856\n",
      "  time_since_restore: 51.22882437705994\n",
      "  time_this_iter_s: 1.098564624786377\n",
      "  time_total_s: 51.22882437705994\n",
      "  timers:\n",
      "    learn_throughput: 4131.81\n",
      "    learn_time_ms: 7.745\n",
      "    load_throughput: 77744.282\n",
      "    load_time_ms: 0.412\n",
      "    update_time_ms: 4.888\n",
      "  timestamp: 1648811759\n",
      "  timesteps_since_restore: 1280\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 13137\n",
      "  training_iteration: 40\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:04 (running for 00:01:12.00)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         54.5219</td><td style=\"text-align: right;\">14048</td><td style=\"text-align: right;\">  -32.78</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 29324\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-05\n",
      "  done: false\n",
      "  episode_len_mean: 19.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -33.1\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 817\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 14622\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -65.82879638671875\n",
      "          mean_q: -66.524169921875\n",
      "          mean_td_error: -0.4208188056945801\n",
      "          min_q: -67.37167358398438\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.6931610107421875\n",
      "        - 0.8508987426757812\n",
      "        - -0.8996124267578125\n",
      "        - 0.00043487548828125\n",
      "        - 0.7117919921875\n",
      "        - -0.0603790283203125\n",
      "        - -0.3138885498046875\n",
      "        - 0.04476165771484375\n",
      "        - -0.209320068359375\n",
      "        - 0.7201766967773438\n",
      "        - 0.8672714233398438\n",
      "        - -0.33390045166015625\n",
      "        - -0.27762603759765625\n",
      "        - -10.628555297851562\n",
      "        - -10.325096130371094\n",
      "        - 0.793853759765625\n",
      "        - 0.0980072021484375\n",
      "        - 0.2436676025390625\n",
      "        - 0.742645263671875\n",
      "        - 0.6807861328125\n",
      "        - 0.1892547607421875\n",
      "        - -0.0927276611328125\n",
      "        - 0.31792449951171875\n",
      "        - 0.5413436889648438\n",
      "        - 1.1933670043945312\n",
      "        - 1.0618820190429688\n",
      "        - 0.184234619140625\n",
      "        - 0.020660400390625\n",
      "        - -0.46349334716796875\n",
      "        - -0.16046905517578125\n",
      "        - -0.548919677734375\n",
      "        - 0.89166259765625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -58.198211669921875\n",
      "          mean_q: -64.23448181152344\n",
      "          mean_td_error: -4.982745170593262\n",
      "          min_q: -67.3487548828125\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.77569580078125\n",
      "        - -1.0929794311523438\n",
      "        - -57.198211669921875\n",
      "        - -0.22119903564453125\n",
      "        - -0.7501564025878906\n",
      "        - -1.6016044616699219\n",
      "        - -0.3446044921875\n",
      "        - -0.4303016662597656\n",
      "        - -1.0133056640625\n",
      "        - -1.1926841735839844\n",
      "        - -0.4579315185546875\n",
      "        - -1.6095809936523438\n",
      "        - -0.9784660339355469\n",
      "        - -1.8491325378417969\n",
      "        - -0.8847846984863281\n",
      "        - -0.2982635498046875\n",
      "        - -0.0156707763671875\n",
      "        - -1.3926315307617188\n",
      "        - -65.78487396240234\n",
      "        - -5.9460601806640625\n",
      "        - -1.1668701171875\n",
      "        - -0.437957763671875\n",
      "        - -0.688751220703125\n",
      "        - -0.48564910888671875\n",
      "        - -1.8669204711914062\n",
      "        - -0.33425140380859375\n",
      "        - -1.1668701171875\n",
      "        - -7.1994781494140625\n",
      "        - -0.8182296752929688\n",
      "        - -1.2574119567871094\n",
      "        - -0.9411506652832031\n",
      "        - -0.7975502014160156\n",
      "    num_agent_steps_sampled: 29324\n",
      "    num_agent_steps_trained: 48192\n",
      "    num_steps_sampled: 14662\n",
      "    num_steps_trained: 24096\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 122\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.35\n",
      "    ram_util_percent: 59.5\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -7.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -15.85\n",
      "    policy1: -17.25\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11835524090545561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07378981546752383\n",
      "    mean_inference_ms: 1.7253973969550793\n",
      "    mean_raw_obs_processing_ms: 0.30707639375715684\n",
      "  time_since_restore: 57.10722827911377\n",
      "  time_this_iter_s: 1.2404859066009521\n",
      "  time_total_s: 57.10722827911377\n",
      "  timers:\n",
      "    learn_throughput: 4291.369\n",
      "    learn_time_ms: 7.457\n",
      "    load_throughput: 77309.906\n",
      "    load_time_ms: 0.414\n",
      "    update_time_ms: 4.682\n",
      "  timestamp: 1648811765\n",
      "  timesteps_since_restore: 1440\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 14662\n",
      "  training_iteration: 45\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:09 (running for 00:01:17.19)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         59.4329</td><td style=\"text-align: right;\">15262</td><td style=\"text-align: right;\">   -33.5</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 32324\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 19.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -34.48\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 892\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 16062\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -62.212406158447266\n",
      "          mean_q: -63.86095428466797\n",
      "          mean_td_error: 2.0004966259002686\n",
      "          min_q: -65.70736694335938\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 3.4071807861328125\n",
      "        - 1.5128097534179688\n",
      "        - 1.28448486328125\n",
      "        - 1.3609237670898438\n",
      "        - 1.5805435180664062\n",
      "        - 2.103252410888672\n",
      "        - 1.7797470092773438\n",
      "        - 2.9673614501953125\n",
      "        - 0.76031494140625\n",
      "        - 2.8701705932617188\n",
      "        - 1.8328323364257812\n",
      "        - 1.6775588989257812\n",
      "        - 2.305206298828125\n",
      "        - 2.595428466796875\n",
      "        - 1.691009521484375\n",
      "        - 1.2238082885742188\n",
      "        - 2.4841995239257812\n",
      "        - 1.5397224426269531\n",
      "        - 1.1106414794921875\n",
      "        - 1.6775588989257812\n",
      "        - 1.7405242919921875\n",
      "        - 1.9621734619140625\n",
      "        - 2.494739532470703\n",
      "        - 3.2573204040527344\n",
      "        - 1.9178047180175781\n",
      "        - 1.0482177734375\n",
      "        - 2.8534202575683594\n",
      "        - 3.1819000244140625\n",
      "        - 1.294158935546875\n",
      "        - 1.9496612548828125\n",
      "        - 1.8458023071289062\n",
      "        - 2.705413818359375\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -56.96846008300781\n",
      "          mean_q: -64.03361511230469\n",
      "          mean_td_error: -3.7417097091674805\n",
      "          min_q: -67.65902709960938\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -6.875087738037109\n",
      "        - 0.6395721435546875\n",
      "        - 0.5231552124023438\n",
      "        - 1.9992828369140625\n",
      "        - 0.3691520690917969\n",
      "        - -0.1581268310546875\n",
      "        - -0.19536590576171875\n",
      "        - 0.6261253356933594\n",
      "        - -0.35103607177734375\n",
      "        - 1.1300811767578125\n",
      "        - 0.7127418518066406\n",
      "        - 0.5272674560546875\n",
      "        - 1.2727737426757812\n",
      "        - 0.20729827880859375\n",
      "        - -0.39206695556640625\n",
      "        - 0.3632164001464844\n",
      "        - -0.51531982421875\n",
      "        - -55.96846008300781\n",
      "        - -0.7480850219726562\n",
      "        - -0.1285552978515625\n",
      "        - 0.9354934692382812\n",
      "        - -0.1883697509765625\n",
      "        - 0.19355010986328125\n",
      "        - 0.228973388671875\n",
      "        - -0.47957611083984375\n",
      "        - -1.416229248046875\n",
      "        - -0.0807647705078125\n",
      "        - 0.71563720703125\n",
      "        - 0.079925537109375\n",
      "        - 0.38428497314453125\n",
      "        - -63.23455810546875\n",
      "        - 0.08835601806640625\n",
      "    num_agent_steps_sampled: 32324\n",
      "    num_agent_steps_trained: 52992\n",
      "    num_steps_sampled: 16162\n",
      "    num_steps_trained: 26496\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 134\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.9\n",
      "    ram_util_percent: 59.6\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -7.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -17.34\n",
      "    policy1: -17.14\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11884750226487079\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07394022867154199\n",
      "    mean_inference_ms: 1.7307412946497245\n",
      "    mean_raw_obs_processing_ms: 0.307699311531481\n",
      "  time_since_restore: 63.04603672027588\n",
      "  time_this_iter_s: 1.287447452545166\n",
      "  time_total_s: 63.04603672027588\n",
      "  timers:\n",
      "    learn_throughput: 3765.76\n",
      "    learn_time_ms: 8.498\n",
      "    load_throughput: 71544.631\n",
      "    load_time_ms: 0.447\n",
      "    update_time_ms: 5.647\n",
      "  timestamp: 1648811772\n",
      "  timesteps_since_restore: 1600\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 16162\n",
      "  training_iteration: 50\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:14 (running for 00:01:22.39)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         65.3892</td><td style=\"text-align: right;\">16771</td><td style=\"text-align: right;\">  -35.28</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 35382\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 19.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: -34.38\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 971\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 17691\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -67.58438110351562\n",
      "          mean_q: -70.01862335205078\n",
      "          mean_td_error: -3.298466444015503\n",
      "          min_q: -73.185546875\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.8498306274414062\n",
      "        - -1.0848541259765625\n",
      "        - -0.6150665283203125\n",
      "        - -0.02811431884765625\n",
      "        - 0.07430267333984375\n",
      "        - -1.4709243774414062\n",
      "        - -1.379974365234375\n",
      "        - -1.112548828125\n",
      "        - 0.326416015625\n",
      "        - -0.396240234375\n",
      "        - 0.8323974609375\n",
      "        - -0.30312347412109375\n",
      "        - -0.0525054931640625\n",
      "        - 0.3073883056640625\n",
      "        - -1.1601333618164062\n",
      "        - -1.4235992431640625\n",
      "        - -0.293365478515625\n",
      "        - -0.944183349609375\n",
      "        - -0.017425537109375\n",
      "        - -0.5822525024414062\n",
      "        - -7.902809143066406\n",
      "        - 0.3100433349609375\n",
      "        - -0.6932220458984375\n",
      "        - -0.7790908813476562\n",
      "        - -0.9546661376953125\n",
      "        - -0.6553955078125\n",
      "        - 0.30460357666015625\n",
      "        - -0.362274169921875\n",
      "        - -1.6518402099609375\n",
      "        - -71.51246643066406\n",
      "        - -11.478202819824219\n",
      "        - -0.0019683837890625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -58.41928482055664\n",
      "          mean_q: -63.25433349609375\n",
      "          mean_td_error: -7.103594779968262\n",
      "          min_q: -65.94108581542969\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.579559326171875\n",
      "        - 1.668548583984375\n",
      "        - 0.42086029052734375\n",
      "        - 0.5819244384765625\n",
      "        - 0.9135589599609375\n",
      "        - 0.6518478393554688\n",
      "        - 0.5908279418945312\n",
      "        - 1.3523826599121094\n",
      "        - 0.9426040649414062\n",
      "        - -63.21062469482422\n",
      "        - 0.7396621704101562\n",
      "        - 0.5067062377929688\n",
      "        - 1.0637664794921875\n",
      "        - 1.254180908203125\n",
      "        - -58.00992965698242\n",
      "        - -67.55656433105469\n",
      "        - -62.98461151123047\n",
      "        - 0.37963104248046875\n",
      "        - 0.6193504333496094\n",
      "        - 1.0342559814453125\n",
      "        - 1.20220947265625\n",
      "        - 1.1870346069335938\n",
      "        - 1.2465019226074219\n",
      "        - 1.0710792541503906\n",
      "        - 0.325836181640625\n",
      "        - 0.7512626647949219\n",
      "        - 0.0191650390625\n",
      "        - 1.4545059204101562\n",
      "        - 0.9127197265625\n",
      "        - 0.8680419921875\n",
      "        - 1.1105690002441406\n",
      "        - 0.9980926513671875\n",
      "    num_agent_steps_sampled: 35382\n",
      "    num_agent_steps_trained: 58048\n",
      "    num_steps_sampled: 17691\n",
      "    num_steps_trained: 29024\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 148\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.4\n",
      "    ram_util_percent: 59.7\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 6.0\n",
      "    policy1: 13.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -17.49\n",
      "    policy1: -16.89\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1194008670565751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07407746169544716\n",
      "    mean_inference_ms: 1.7372004042507898\n",
      "    mean_raw_obs_processing_ms: 0.30812646091839996\n",
      "  time_since_restore: 69.01715970039368\n",
      "  time_this_iter_s: 1.2050542831420898\n",
      "  time_total_s: 69.01715970039368\n",
      "  timers:\n",
      "    learn_throughput: 4066.896\n",
      "    learn_time_ms: 7.868\n",
      "    load_throughput: 73543.961\n",
      "    load_time_ms: 0.435\n",
      "    update_time_ms: 5.086\n",
      "  timestamp: 1648811778\n",
      "  timesteps_since_restore: 1760\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 17691\n",
      "  training_iteration: 55\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:19 (running for 00:01:27.42)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          70.175</td><td style=\"text-align: right;\">17991</td><td style=\"text-align: right;\">  -34.68</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 38406\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-24\n",
      "  done: false\n",
      "  episode_len_mean: 19.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: -35.52\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1048\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 19103\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -70.66542053222656\n",
      "          mean_q: -72.38944244384766\n",
      "          mean_td_error: -7.35831356048584\n",
      "          min_q: -73.9136962890625\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.9200286865234375\n",
      "        - 0.4076995849609375\n",
      "        - -9.436508178710938\n",
      "        - 0.5185470581054688\n",
      "        - 0.2406005859375\n",
      "        - 0.28678131103515625\n",
      "        - 0.8547515869140625\n",
      "        - 0.49072265625\n",
      "        - -0.11667633056640625\n",
      "        - 0.117034912109375\n",
      "        - -0.12949371337890625\n",
      "        - 0.29012298583984375\n",
      "        - -0.6382980346679688\n",
      "        - 0.1052093505859375\n",
      "        - 0.37233734130859375\n",
      "        - 0.566741943359375\n",
      "        - -0.38234710693359375\n",
      "        - 0.11150360107421875\n",
      "        - 0.2452239990234375\n",
      "        - -0.0437164306640625\n",
      "        - 1.5544509887695312\n",
      "        - 0.01340484619140625\n",
      "        - 0.5581893920898438\n",
      "        - -81.68521881103516\n",
      "        - 0.8744277954101562\n",
      "        - -81.43783569335938\n",
      "        - 0.7704315185546875\n",
      "        - 0.11981964111328125\n",
      "        - -71.3445053100586\n",
      "        - -1.1868896484375\n",
      "        - 0.1655120849609375\n",
      "        - 1.3519134521484375\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -58.27891159057617\n",
      "          mean_q: -63.221214294433594\n",
      "          mean_td_error: -9.796205520629883\n",
      "          min_q: -64.74681091308594\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.4117469787597656\n",
      "        - 0.8269081115722656\n",
      "        - 0.16115570068359375\n",
      "        - 0.6935577392578125\n",
      "        - 0.7905311584472656\n",
      "        - 0.7334823608398438\n",
      "        - 0.7728843688964844\n",
      "        - 1.25714111328125\n",
      "        - -8.235614776611328\n",
      "        - 0.8538017272949219\n",
      "        - 0.6198806762695312\n",
      "        - -60.7836799621582\n",
      "        - 0.15344619750976562\n",
      "        - -73.42774963378906\n",
      "        - 0.532562255859375\n",
      "        - 1.8375930786132812\n",
      "        - 0.4402885437011719\n",
      "        - 0.5877838134765625\n",
      "        - -3.602039337158203\n",
      "        - 0.14056396484375\n",
      "        - 0.506378173828125\n",
      "        - 0.6506729125976562\n",
      "        - -57.48622512817383\n",
      "        - 0.4402885437011719\n",
      "        - 0.19416046142578125\n",
      "        - -61.69623947143555\n",
      "        - -0.4738883972167969\n",
      "        - -63.017539978027344\n",
      "        - 0.7782249450683594\n",
      "        - -0.18810272216796875\n",
      "        - 0.32126617431640625\n",
      "        - 0.7281875610351562\n",
      "    num_agent_steps_sampled: 38406\n",
      "    num_agent_steps_trained: 62976\n",
      "    num_steps_sampled: 19203\n",
      "    num_steps_trained: 31488\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 160\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.549999999999997\n",
      "    ram_util_percent: 59.8\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 6.0\n",
      "    policy1: 6.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -17.96\n",
      "    policy1: -17.56\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11951698718667732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07397694523138329\n",
      "    mean_inference_ms: 1.7386387515393664\n",
      "    mean_raw_obs_processing_ms: 0.3079694959066758\n",
      "  time_since_restore: 74.82341599464417\n",
      "  time_this_iter_s: 1.1844496726989746\n",
      "  time_total_s: 74.82341599464417\n",
      "  timers:\n",
      "    learn_throughput: 4200.947\n",
      "    learn_time_ms: 7.617\n",
      "    load_throughput: 76100.09\n",
      "    load_time_ms: 0.42\n",
      "    update_time_ms: 5.106\n",
      "  timestamp: 1648811784\n",
      "  timesteps_since_restore: 1920\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 19203\n",
      "  training_iteration: 60\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:25 (running for 00:01:33.31)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         74.8234</td><td style=\"text-align: right;\">19203</td><td style=\"text-align: right;\">  -35.52</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 41406\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-30\n",
      "  done: false\n",
      "  episode_len_mean: 19.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: -36.94\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1123\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 20663\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -72.29829406738281\n",
      "          mean_q: -75.0681381225586\n",
      "          mean_td_error: -0.08209395408630371\n",
      "          min_q: -77.34774780273438\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.32318878173828125\n",
      "        - 0.45609283447265625\n",
      "        - 0.6941909790039062\n",
      "        - -0.00542449951171875\n",
      "        - -0.49576568603515625\n",
      "        - -0.25516510009765625\n",
      "        - -0.875274658203125\n",
      "        - -0.303955078125\n",
      "        - 0.298248291015625\n",
      "        - 0.2310333251953125\n",
      "        - -0.8800277709960938\n",
      "        - -0.27756500244140625\n",
      "        - -0.38120269775390625\n",
      "        - 0.46488189697265625\n",
      "        - 0.3095245361328125\n",
      "        - 0.418731689453125\n",
      "        - -1.20843505859375\n",
      "        - 0.4644622802734375\n",
      "        - -0.39952850341796875\n",
      "        - -0.2528228759765625\n",
      "        - -0.14308929443359375\n",
      "        - 0.15421295166015625\n",
      "        - -0.13838958740234375\n",
      "        - 0.25423431396484375\n",
      "        - 0.21178436279296875\n",
      "        - -0.87603759765625\n",
      "        - -0.30835723876953125\n",
      "        - 0.8742752075195312\n",
      "        - -0.5759658813476562\n",
      "        - 0.48387908935546875\n",
      "        - -0.03771209716796875\n",
      "        - -0.20465087890625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -58.97306442260742\n",
      "          mean_q: -63.50714874267578\n",
      "          mean_td_error: -2.657440662384033\n",
      "          min_q: -65.62179565429688\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.0999298095703125\n",
      "        - -0.6590385437011719\n",
      "        - -1.4877166748046875\n",
      "        - -0.4969635009765625\n",
      "        - -0.15438079833984375\n",
      "        - -0.5902481079101562\n",
      "        - -0.8364334106445312\n",
      "        - -1.1397476196289062\n",
      "        - -0.5054550170898438\n",
      "        - -0.03520965576171875\n",
      "        - -0.6119384765625\n",
      "        - -1.3533782958984375\n",
      "        - -1.038330078125\n",
      "        - -0.8628196716308594\n",
      "        - -0.2594413757324219\n",
      "        - -0.8188629150390625\n",
      "        - -0.6323356628417969\n",
      "        - -0.33269500732421875\n",
      "        - -0.774261474609375\n",
      "        - -1.1129837036132812\n",
      "        - -0.8335952758789062\n",
      "        - -0.1150665283203125\n",
      "        - 0.16266632080078125\n",
      "        - -1.1825942993164062\n",
      "        - -0.9849777221679688\n",
      "        - -0.4065513610839844\n",
      "        - -0.6383705139160156\n",
      "        - -1.0479621887207031\n",
      "        - -0.8048057556152344\n",
      "        - -1.0295944213867188\n",
      "        - -6.151172637939453\n",
      "        - -58.20390319824219\n",
      "    num_agent_steps_sampled: 41406\n",
      "    num_agent_steps_trained: 67776\n",
      "    num_steps_sampled: 20703\n",
      "    num_steps_trained: 33888\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 173\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.8\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: 5.0\n",
      "    policy1: 5.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -17.72\n",
      "    policy1: -19.22\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1195174528625126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07394394681665944\n",
      "    mean_inference_ms: 1.7380485587738947\n",
      "    mean_raw_obs_processing_ms: 0.3075301377947098\n",
      "  time_since_restore: 80.55480146408081\n",
      "  time_this_iter_s: 1.1159446239471436\n",
      "  time_total_s: 80.55480146408081\n",
      "  timers:\n",
      "    learn_throughput: 4009.192\n",
      "    learn_time_ms: 7.982\n",
      "    load_throughput: 72448.304\n",
      "    load_time_ms: 0.442\n",
      "    update_time_ms: 4.835\n",
      "  timestamp: 1648811790\n",
      "  timesteps_since_restore: 2080\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 20703\n",
      "  training_iteration: 65\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:30 (running for 00:01:38.33)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         80.5548</td><td style=\"text-align: right;\">20703</td><td style=\"text-align: right;\">  -36.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:36 (running for 00:01:44.20)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         85.2213</td><td style=\"text-align: right;\">21903</td><td style=\"text-align: right;\">   -38.4</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 44406\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -30.0\n",
      "  episode_reward_mean: -38.7\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1198\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 22103\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -76.02014923095703\n",
      "          mean_q: -77.9546890258789\n",
      "          mean_td_error: -9.170734405517578\n",
      "          min_q: -79.3365707397461\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.445037841796875\n",
      "        - 0.980438232421875\n",
      "        - 2.5007171630859375\n",
      "        - 1.7609176635742188\n",
      "        - -87.00434112548828\n",
      "        - 1.213043212890625\n",
      "        - -75.02014923095703\n",
      "        - 2.1174163818359375\n",
      "        - 1.6714096069335938\n",
      "        - 0.9699478149414062\n",
      "        - 1.6492080688476562\n",
      "        - -0.29186248779296875\n",
      "        - 0.221923828125\n",
      "        - 2.2888870239257812\n",
      "        - -75.45913696289062\n",
      "        - 1.4486923217773438\n",
      "        - 1.2909393310546875\n",
      "        - 2.1482315063476562\n",
      "        - 0.7781448364257812\n",
      "        - 0.5576248168945312\n",
      "        - 0.6992111206054688\n",
      "        - 1.5210189819335938\n",
      "        - 0.9427261352539062\n",
      "        - 1.2758636474609375\n",
      "        - -0.044921875\n",
      "        - -8.458641052246094\n",
      "        - 1.7078704833984375\n",
      "        - 0.18025970458984375\n",
      "        - 0.19837188720703125\n",
      "        - 1.4739761352539062\n",
      "        - -76.3755874633789\n",
      "        - 0.03932952880859375\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -57.078025817871094\n",
      "          mean_q: -63.19525909423828\n",
      "          mean_td_error: -0.8302191495895386\n",
      "          min_q: -65.94823455810547\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -10.844295501708984\n",
      "        - -1.0555953979492188\n",
      "        - 0.22953033447265625\n",
      "        - -1.3419456481933594\n",
      "        - 0.5726585388183594\n",
      "        - -0.4132575988769531\n",
      "        - -0.2985572814941406\n",
      "        - -0.36037445068359375\n",
      "        - -0.08306884765625\n",
      "        - -1.481781005859375\n",
      "        - -1.0858001708984375\n",
      "        - -0.7161941528320312\n",
      "        - 0.22900390625\n",
      "        - -0.26625823974609375\n",
      "        - -0.26625823974609375\n",
      "        - -0.9770050048828125\n",
      "        - -0.743316650390625\n",
      "        - -2.5271835327148438\n",
      "        - 1.0007286071777344\n",
      "        - 0.05736541748046875\n",
      "        - -0.00487518310546875\n",
      "        - 0.2577972412109375\n",
      "        - -0.21991729736328125\n",
      "        - -0.3976898193359375\n",
      "        - 0.7500534057617188\n",
      "        - -1.1139297485351562\n",
      "        - 0.2518272399902344\n",
      "        - -0.01033782958984375\n",
      "        - -4.495807647705078\n",
      "        - -0.5459671020507812\n",
      "        - 0.07857131958007812\n",
      "        - -0.7451324462890625\n",
      "    num_agent_steps_sampled: 44406\n",
      "    num_agent_steps_trained: 72576\n",
      "    num_steps_sampled: 22203\n",
      "    num_steps_trained: 36288\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 185\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.75\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -19.2\n",
      "    policy1: -19.5\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11965881171719665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07405043172632447\n",
      "    mean_inference_ms: 1.7395941850424668\n",
      "    mean_raw_obs_processing_ms: 0.30736486306865407\n",
      "  time_since_restore: 86.35602116584778\n",
      "  time_this_iter_s: 1.134673833847046\n",
      "  time_total_s: 86.35602116584778\n",
      "  timers:\n",
      "    learn_throughput: 4169.496\n",
      "    learn_time_ms: 7.675\n",
      "    load_throughput: 73819.012\n",
      "    load_time_ms: 0.433\n",
      "    update_time_ms: 4.737\n",
      "  timestamp: 1648811796\n",
      "  timesteps_since_restore: 2240\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 22203\n",
      "  training_iteration: 70\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:42 (running for 00:01:50.12)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         90.8891</td><td style=\"text-align: right;\">23403</td><td style=\"text-align: right;\">     -39</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 47406\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-42\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -30.0\n",
      "  episode_reward_mean: -39.5\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1273\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 23663\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -75.72018432617188\n",
      "          mean_q: -79.20428466796875\n",
      "          mean_td_error: 0.21366405487060547\n",
      "          min_q: -84.97614288330078\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.1597900390625\n",
      "        - 0.0472412109375\n",
      "        - -0.7767257690429688\n",
      "        - 0.09889984130859375\n",
      "        - -1.6234893798828125\n",
      "        - -0.6915740966796875\n",
      "        - -0.06475067138671875\n",
      "        - 0.16448211669921875\n",
      "        - 0.03115081787109375\n",
      "        - -0.14714813232421875\n",
      "        - 0.17613983154296875\n",
      "        - 2.202239990234375\n",
      "        - 0.0493316650390625\n",
      "        - 0.12380218505859375\n",
      "        - -0.2834625244140625\n",
      "        - 0.12380218505859375\n",
      "        - 0.17613983154296875\n",
      "        - 0.21581268310546875\n",
      "        - 0.16191864013671875\n",
      "        - 0.1388702392578125\n",
      "        - 0.9350204467773438\n",
      "        - 0.5536575317382812\n",
      "        - 0.54498291015625\n",
      "        - 0.5077438354492188\n",
      "        - -0.725921630859375\n",
      "        - 0.2111053466796875\n",
      "        - -0.134307861328125\n",
      "        - 0.08414459228515625\n",
      "        - 0.284149169921875\n",
      "        - 0.33986663818359375\n",
      "        - -0.08829498291015625\n",
      "        - 4.362213134765625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -67.10608673095703\n",
      "          mean_q: -71.81488037109375\n",
      "          mean_td_error: -3.2214126586914062\n",
      "          min_q: -75.14041900634766\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.946746826171875\n",
      "        - -1.1546707153320312\n",
      "        - -0.78302001953125\n",
      "        - -0.4125213623046875\n",
      "        - -1.2878875732421875\n",
      "        - -2.16796875\n",
      "        - -0.7753677368164062\n",
      "        - -1.0298233032226562\n",
      "        - -1.2186126708984375\n",
      "        - -1.474212646484375\n",
      "        - -1.5390243530273438\n",
      "        - -1.4554519653320312\n",
      "        - -1.2263031005859375\n",
      "        - -1.497039794921875\n",
      "        - -1.1413345336914062\n",
      "        - 1.184356689453125\n",
      "        - -1.6219863891601562\n",
      "        - -3.080810546875\n",
      "        - -0.7520904541015625\n",
      "        - -0.8787918090820312\n",
      "        - -69.16348266601562\n",
      "        - 0.009033203125\n",
      "        - -0.31500244140625\n",
      "        - -0.6537628173828125\n",
      "        - -0.9641952514648438\n",
      "        - -1.8734207153320312\n",
      "        - 0.2619171142578125\n",
      "        - -0.33658599853515625\n",
      "        - -1.6115570068359375\n",
      "        - -1.9788284301757812\n",
      "        - -2.301422119140625\n",
      "        - -0.898590087890625\n",
      "    num_agent_steps_sampled: 47406\n",
      "    num_agent_steps_trained: 77376\n",
      "    num_steps_sampled: 23703\n",
      "    num_steps_trained: 38688\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 198\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.4\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -19.7\n",
      "    policy1: -19.8\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11971216224869785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07399470751685022\n",
      "    mean_inference_ms: 1.739735810427535\n",
      "    mean_raw_obs_processing_ms: 0.30718359360549075\n",
      "  time_since_restore: 92.0077497959137\n",
      "  time_this_iter_s: 1.118636131286621\n",
      "  time_total_s: 92.0077497959137\n",
      "  timers:\n",
      "    learn_throughput: 4164.812\n",
      "    learn_time_ms: 7.683\n",
      "    load_throughput: 72082.561\n",
      "    load_time_ms: 0.444\n",
      "    update_time_ms: 4.789\n",
      "  timestamp: 1648811802\n",
      "  timesteps_since_restore: 2400\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 23703\n",
      "  training_iteration: 75\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:47 (running for 00:01:55.30)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         95.8638</td><td style=\"text-align: right;\">24603</td><td style=\"text-align: right;\">   -39.6</td><td style=\"text-align: right;\">                 -30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 49806\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-47\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -30.0\n",
      "  episode_reward_mean: -39.7\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1333\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 24863\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -76.40267944335938\n",
      "          mean_q: -78.58735656738281\n",
      "          mean_td_error: -3.6817989349365234\n",
      "          min_q: -84.9720230102539\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -1.5561981201171875\n",
      "        - -1.1321563720703125\n",
      "        - -1.891021728515625\n",
      "        - -1.8920059204101562\n",
      "        - -1.808319091796875\n",
      "        - -1.5810775756835938\n",
      "        - -1.791168212890625\n",
      "        - -1.7445297241210938\n",
      "        - -1.5983352661132812\n",
      "        - -0.4390869140625\n",
      "        - -1.0609512329101562\n",
      "        - -1.4362945556640625\n",
      "        - -2.3362045288085938\n",
      "        - -2.40478515625\n",
      "        - -1.9046859741210938\n",
      "        - -1.6516571044921875\n",
      "        - -76.51002502441406\n",
      "        - -0.2125701904296875\n",
      "        - -0.5270538330078125\n",
      "        - -2.4876785278320312\n",
      "        - -2.2304153442382812\n",
      "        - 4.405998229980469\n",
      "        - -1.5561981201171875\n",
      "        - -1.5254135131835938\n",
      "        - -1.7571182250976562\n",
      "        - -2.0096817016601562\n",
      "        - -1.1376113891601562\n",
      "        - -0.08100128173828125\n",
      "        - -1.79107666015625\n",
      "        - -1.7506179809570312\n",
      "        - -0.8375473022460938\n",
      "        - -1.5810775756835938\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -66.16787719726562\n",
      "          mean_q: -68.15036010742188\n",
      "          mean_td_error: -5.028358459472656\n",
      "          min_q: -70.76245880126953\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -10.766166687011719\n",
      "        - -1.190460205078125\n",
      "        - -0.186767578125\n",
      "        - -0.28490447998046875\n",
      "        - -69.70249938964844\n",
      "        - -0.6075363159179688\n",
      "        - -0.0526123046875\n",
      "        - -0.7215042114257812\n",
      "        - 0.39196014404296875\n",
      "        - -0.5004348754882812\n",
      "        - -0.9778900146484375\n",
      "        - -0.30678558349609375\n",
      "        - -1.2379837036132812\n",
      "        - 0.204559326171875\n",
      "        - -0.5004348754882812\n",
      "        - 0.20477294921875\n",
      "        - -0.34917449951171875\n",
      "        - -0.2905426025390625\n",
      "        - -1.1393356323242188\n",
      "        - -0.8706283569335938\n",
      "        - 0.12937164306640625\n",
      "        - 0.2264556884765625\n",
      "        - -0.531402587890625\n",
      "        - -0.21518707275390625\n",
      "        - -66.91389465332031\n",
      "        - -1.7462081909179688\n",
      "        - -0.499969482421875\n",
      "        - -0.5851669311523438\n",
      "        - -0.16397857666015625\n",
      "        - -0.7215042114257812\n",
      "        - -0.28011322021484375\n",
      "        - -0.7215118408203125\n",
      "    num_agent_steps_sampled: 49806\n",
      "    num_agent_steps_trained: 81216\n",
      "    num_steps_sampled: 24903\n",
      "    num_steps_trained: 40608\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 208\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.9\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -19.8\n",
      "    policy1: -19.9\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11999326075774552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07406386208403874\n",
      "    mean_inference_ms: 1.7425551212102963\n",
      "    mean_raw_obs_processing_ms: 0.30748813967820277\n",
      "  time_since_restore: 97.02352118492126\n",
      "  time_this_iter_s: 1.159714698791504\n",
      "  time_total_s: 97.02352118492126\n",
      "  timers:\n",
      "    learn_throughput: 4078.624\n",
      "    learn_time_ms: 7.846\n",
      "    load_throughput: 73435.317\n",
      "    load_time_ms: 0.436\n",
      "    update_time_ms: 4.78\n",
      "  timestamp: 1648811807\n",
      "  timesteps_since_restore: 2528\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 24903\n",
      "  training_iteration: 79\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:52 (running for 00:02:00.43)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">          101.74</td><td style=\"text-align: right;\">26103</td><td style=\"text-align: right;\">     -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 52806\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-53\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -40.0\n",
      "  episode_reward_mean: -40.0\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1408\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 26303\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -74.17301940917969\n",
      "          mean_q: -79.2740478515625\n",
      "          mean_td_error: -7.266983985900879\n",
      "          min_q: -87.39598846435547\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.7339324951171875\n",
      "        - -1.3946914672851562\n",
      "        - 1.7386245727539062\n",
      "        - -0.7384262084960938\n",
      "        - -0.937103271484375\n",
      "        - -1.3675079345703125\n",
      "        - 0.2681884765625\n",
      "        - 0.34789276123046875\n",
      "        - -0.9491043090820312\n",
      "        - -0.13492584228515625\n",
      "        - -0.5825729370117188\n",
      "        - 0.34789276123046875\n",
      "        - 0.941162109375\n",
      "        - 0.03939056396484375\n",
      "        - -0.6006927490234375\n",
      "        - 0.5458145141601562\n",
      "        - -1.3001861572265625\n",
      "        - -78.27229309082031\n",
      "        - 0.23119354248046875\n",
      "        - -0.06961822509765625\n",
      "        - -0.982086181640625\n",
      "        - 0.14212799072265625\n",
      "        - 0.0372772216796875\n",
      "        - -0.38330841064453125\n",
      "        - -85.75234985351562\n",
      "        - -0.5349807739257812\n",
      "        - 1.5131072998046875\n",
      "        - 2.252288818359375\n",
      "        - -0.4854583740234375\n",
      "        - -0.0897674560546875\n",
      "        - -74.90660858154297\n",
      "        - 9.267166137695312\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -67.33726501464844\n",
      "          mean_q: -70.3971176147461\n",
      "          mean_td_error: -2.058100700378418\n",
      "          min_q: -72.74483489990234\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.55816650390625\n",
      "        - 0.2514190673828125\n",
      "        - 0.16678619384765625\n",
      "        - 1.6189498901367188\n",
      "        - 0.566192626953125\n",
      "        - 0.7840118408203125\n",
      "        - 1.9722137451171875\n",
      "        - 0.920135498046875\n",
      "        - 0.17972564697265625\n",
      "        - 0.4871368408203125\n",
      "        - -0.152557373046875\n",
      "        - 0.6101455688476562\n",
      "        - 0.6101455688476562\n",
      "        - 0.1753387451171875\n",
      "        - 0.5092315673828125\n",
      "        - -1.8928756713867188\n",
      "        - 0.533172607421875\n",
      "        - 1.004730224609375\n",
      "        - 0.07785797119140625\n",
      "        - 1.1061477661132812\n",
      "        - 0.7837142944335938\n",
      "        - -11.309524536132812\n",
      "        - 0.31304931640625\n",
      "        - 0.9067764282226562\n",
      "        - -0.9377975463867188\n",
      "        - 0.8000564575195312\n",
      "        - 0.20011138916015625\n",
      "        - 0.5162124633789062\n",
      "        - -0.61810302734375\n",
      "        - 0.9481353759765625\n",
      "        - 0.19451904296875\n",
      "        - -67.74244689941406\n",
      "    num_agent_steps_sampled: 52806\n",
      "    num_agent_steps_trained: 86016\n",
      "    num_steps_sampled: 26403\n",
      "    num_steps_trained: 43008\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 220\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.95\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12046719898130111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07421326176349848\n",
      "    mean_inference_ms: 1.7485940485230906\n",
      "    mean_raw_obs_processing_ms: 0.30819155974734835\n",
      "  time_since_restore: 102.88271141052246\n",
      "  time_this_iter_s: 1.1423330307006836\n",
      "  time_total_s: 102.88271141052246\n",
      "  timers:\n",
      "    learn_throughput: 4074.439\n",
      "    learn_time_ms: 7.854\n",
      "    load_throughput: 72397.501\n",
      "    load_time_ms: 0.442\n",
      "    update_time_ms: 4.824\n",
      "  timestamp: 1648811813\n",
      "  timesteps_since_restore: 2688\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 26403\n",
      "  training_iteration: 84\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:16:57 (running for 00:02:05.47)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         106.564</td><td style=\"text-align: right;\">27303</td><td style=\"text-align: right;\">     -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 55206\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-16-58\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -40.0\n",
      "  episode_reward_mean: -40.0\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1468\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 27503\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -71.67064666748047\n",
      "          mean_q: -76.25967407226562\n",
      "          mean_td_error: 0.5345897674560547\n",
      "          min_q: -83.65037536621094\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.04266357421875\n",
      "        - 0.8610000610351562\n",
      "        - 0.5261688232421875\n",
      "        - 0.7689971923828125\n",
      "        - 0.9368438720703125\n",
      "        - 0.3939208984375\n",
      "        - -0.2229766845703125\n",
      "        - 0.9316635131835938\n",
      "        - 1.3775253295898438\n",
      "        - 0.9082489013671875\n",
      "        - -0.0023651123046875\n",
      "        - -0.5432662963867188\n",
      "        - 1.51507568359375\n",
      "        - 2.060089111328125\n",
      "        - -0.5598068237304688\n",
      "        - 1.64996337890625\n",
      "        - 0.8217086791992188\n",
      "        - 0.7833404541015625\n",
      "        - 0.4855804443359375\n",
      "        - 1.3437957763671875\n",
      "        - 1.2388992309570312\n",
      "        - 0.3939208984375\n",
      "        - 0.9675674438476562\n",
      "        - 0.324920654296875\n",
      "        - -0.934539794921875\n",
      "        - 0.4617156982421875\n",
      "        - -0.8042373657226562\n",
      "        - -0.431427001953125\n",
      "        - 0.8936309814453125\n",
      "        - 0.3939208984375\n",
      "        - -0.4195556640625\n",
      "        - 1.0292129516601562\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -65.21110534667969\n",
      "          mean_q: -72.17123413085938\n",
      "          mean_td_error: -6.932674407958984\n",
      "          min_q: -76.53202056884766\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.27142333984375\n",
      "        - -0.14012908935546875\n",
      "        - -0.4961395263671875\n",
      "        - -0.45845794677734375\n",
      "        - 1.3420181274414062\n",
      "        - -0.25464630126953125\n",
      "        - -0.5909576416015625\n",
      "        - 0.000640869140625\n",
      "        - -0.5673446655273438\n",
      "        - 0.39533233642578125\n",
      "        - -4.093452453613281\n",
      "        - -2.2648468017578125\n",
      "        - -0.03205108642578125\n",
      "        - -0.4961395263671875\n",
      "        - 1.8562774658203125\n",
      "        - 1.8562774658203125\n",
      "        - 0.865020751953125\n",
      "        - -68.22306060791016\n",
      "        - 1.1451187133789062\n",
      "        - 0.40834808349609375\n",
      "        - -0.3091583251953125\n",
      "        - -0.39846038818359375\n",
      "        - -0.5673446655273438\n",
      "        - -72.89873504638672\n",
      "        - -0.16616058349609375\n",
      "        - 0.0139312744140625\n",
      "        - -73.36286926269531\n",
      "        - -0.5105743408203125\n",
      "        - -0.4961395263671875\n",
      "        - -2.8391342163085938\n",
      "        - 0.04925537109375\n",
      "        - -0.3405914306640625\n",
      "    num_agent_steps_sampled: 55206\n",
      "    num_agent_steps_trained: 89856\n",
      "    num_steps_sampled: 27603\n",
      "    num_steps_trained: 44928\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 230\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.4\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1206841957589737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07426355547092531\n",
      "    mean_inference_ms: 1.7515277662286184\n",
      "    mean_raw_obs_processing_ms: 0.30857112053635255\n",
      "  time_since_restore: 107.77732014656067\n",
      "  time_this_iter_s: 1.2134308815002441\n",
      "  time_total_s: 107.77732014656067\n",
      "  timers:\n",
      "    learn_throughput: 3921.64\n",
      "    learn_time_ms: 8.16\n",
      "    load_throughput: 66365.57\n",
      "    load_time_ms: 0.482\n",
      "    update_time_ms: 4.922\n",
      "  timestamp: 1648811818\n",
      "  timesteps_since_restore: 2816\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 27603\n",
      "  training_iteration: 88\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:17:03 (running for 00:02:11.41)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         111.301</td><td style=\"text-align: right;\">28503</td><td style=\"text-align: right;\">     -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 58206\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -40.0\n",
      "  episode_reward_mean: -40.0\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1543\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29063\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -71.65473937988281\n",
      "          mean_q: -76.37184143066406\n",
      "          mean_td_error: -4.601147174835205\n",
      "          min_q: -83.63836669921875\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 0.257965087890625\n",
      "        - 0.6244735717773438\n",
      "        - 0.257965087890625\n",
      "        - 1.2918930053710938\n",
      "        - -0.13196563720703125\n",
      "        - 2.7148284912109375\n",
      "        - 0.14471435546875\n",
      "        - -0.26543426513671875\n",
      "        - 0.11617279052734375\n",
      "        - -0.7990951538085938\n",
      "        - -0.06839752197265625\n",
      "        - 0.22015380859375\n",
      "        - 1.8194046020507812\n",
      "        - 0.04524993896484375\n",
      "        - -2.1501846313476562\n",
      "        - 0.37911224365234375\n",
      "        - -72.45427703857422\n",
      "        - -0.5856857299804688\n",
      "        - -0.0099639892578125\n",
      "        - 0.7958450317382812\n",
      "        - -78.16328430175781\n",
      "        - 0.11626434326171875\n",
      "        - 0.5768661499023438\n",
      "        - -2.755615234375\n",
      "        - 0.14502716064453125\n",
      "        - 0.04763031005859375\n",
      "        - 0.3404693603515625\n",
      "        - 0.46686553955078125\n",
      "        - 0.44432830810546875\n",
      "        - -0.09149932861328125\n",
      "        - -0.443359375\n",
      "        - -0.1231842041015625\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -63.275718688964844\n",
      "          mean_q: -66.89043426513672\n",
      "          mean_td_error: 2.321451425552368\n",
      "          min_q: -71.9821548461914\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 2.6895675659179688\n",
      "        - 4.09527587890625\n",
      "        - 3.0968170166015625\n",
      "        - 2.413299560546875\n",
      "        - 5.917472839355469\n",
      "        - 1.27734375\n",
      "        - 2.9842376708984375\n",
      "        - 2.821685791015625\n",
      "        - 2.6630020141601562\n",
      "        - 4.149223327636719\n",
      "        - 2.9999313354492188\n",
      "        - 2.6595993041992188\n",
      "        - -1.3835067749023438\n",
      "        - 4.871147155761719\n",
      "        - 0.701446533203125\n",
      "        - 4.056175231933594\n",
      "        - -1.3835067749023438\n",
      "        - 2.66619873046875\n",
      "        - -1.3835067749023438\n",
      "        - 1.3785552978515625\n",
      "        - 2.890380859375\n",
      "        - 1.04522705078125\n",
      "        - 2.9266357421875\n",
      "        - 1.1650390625\n",
      "        - 3.082733154296875\n",
      "        - 1.0453109741210938\n",
      "        - 3.082733154296875\n",
      "        - 2.358123779296875\n",
      "        - 2.6630020141601562\n",
      "        - 2.4690704345703125\n",
      "        - 1.8155517578125\n",
      "        - 2.452178955078125\n",
      "    num_agent_steps_sampled: 58206\n",
      "    num_agent_steps_trained: 94656\n",
      "    num_steps_sampled: 29103\n",
      "    num_steps_trained: 47328\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 243\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.200000000000003\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12110817409991097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07447503799139207\n",
      "    mean_inference_ms: 1.756612950094962\n",
      "    mean_raw_obs_processing_ms: 0.309212275708661\n",
      "  time_since_restore: 113.71107387542725\n",
      "  time_this_iter_s: 1.2151620388031006\n",
      "  time_total_s: 113.71107387542725\n",
      "  timers:\n",
      "    learn_throughput: 3851.353\n",
      "    learn_time_ms: 8.309\n",
      "    load_throughput: 66725.194\n",
      "    load_time_ms: 0.48\n",
      "    update_time_ms: 5.283\n",
      "  timestamp: 1648811825\n",
      "  timesteps_since_restore: 2976\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 29103\n",
      "  training_iteration: 93\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:17:08 (running for 00:02:16.52)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>RUNNING </td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">           116.2</td><td style=\"text-align: right;\">29703</td><td style=\"text-align: right;\">     -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQNTrainer_foodenv_f3a64_00000:\n",
      "  agent_timesteps_total: 60006\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_04-17-09\n",
      "  done: true\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -40.0\n",
      "  episode_reward_mean: -40.0\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1588\n",
      "  experiment_id: 43f8d33789234291abbd5b44824e2889\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29903\n",
      "    learner:\n",
      "      policy0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -71.06665802001953\n",
      "          mean_q: -74.20606231689453\n",
      "          mean_td_error: -2.535921573638916\n",
      "          min_q: -83.14420318603516\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - -0.07349395751953125\n",
      "        - 0.25847625732421875\n",
      "        - -0.26969146728515625\n",
      "        - -0.45507049560546875\n",
      "        - -1.0364456176757812\n",
      "        - -1.1384201049804688\n",
      "        - -0.08036041259765625\n",
      "        - 0.32134246826171875\n",
      "        - 0.02754974365234375\n",
      "        - -0.472686767578125\n",
      "        - 0.0220184326171875\n",
      "        - 0.1871185302734375\n",
      "        - 0.25847625732421875\n",
      "        - -1.2104034423828125\n",
      "        - -0.6454620361328125\n",
      "        - -74.16815948486328\n",
      "        - -0.42636871337890625\n",
      "        - -1.11224365234375\n",
      "        - -0.01969146728515625\n",
      "        - 0.32134246826171875\n",
      "        - -0.9406280517578125\n",
      "        - 0.42745208740234375\n",
      "        - 0.4047393798828125\n",
      "        - -0.5884857177734375\n",
      "        - -1.078125\n",
      "        - 0.1614990234375\n",
      "        - -0.27335357666015625\n",
      "        - 0.32134246826171875\n",
      "        - 0.510467529296875\n",
      "        - 0.25847625732421875\n",
      "        - 0.8588638305664062\n",
      "        - -1.4995651245117188\n",
      "      policy1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_lr: 0.009999999776482582\n",
      "          max_q: -67.72464752197266\n",
      "          mean_q: -69.87606811523438\n",
      "          mean_td_error: -1.7002253532409668\n",
      "          min_q: -71.7522964477539\n",
      "          model: {}\n",
      "        td_error:\n",
      "        - 1.2308731079101562\n",
      "        - -0.11777496337890625\n",
      "        - 1.1542587280273438\n",
      "        - -0.18183135986328125\n",
      "        - -0.0196075439453125\n",
      "        - 0.6999893188476562\n",
      "        - 0.47438812255859375\n",
      "        - 0.7502288818359375\n",
      "        - 0.9274978637695312\n",
      "        - 1.0632705688476562\n",
      "        - 0.8049850463867188\n",
      "        - 1.0788116455078125\n",
      "        - 1.327972412109375\n",
      "        - 0.8351058959960938\n",
      "        - -9.876243591308594\n",
      "        - 0.7663726806640625\n",
      "        - 0.47699737548828125\n",
      "        - 1.2150192260742188\n",
      "        - 2.0015182495117188\n",
      "        - 1.282928466796875\n",
      "        - 0.6972427368164062\n",
      "        - 0.8181610107421875\n",
      "        - -68.2006607055664\n",
      "        - 0.5908660888671875\n",
      "        - 0.2038116455078125\n",
      "        - 0.645904541015625\n",
      "        - 1.2150192260742188\n",
      "        - 0.8838043212890625\n",
      "        - 0.5908660888671875\n",
      "        - 1.0457916259765625\n",
      "        - 0.8727569580078125\n",
      "        - 0.33446502685546875\n",
      "    num_agent_steps_sampled: 60006\n",
      "    num_agent_steps_trained: 97536\n",
      "    num_steps_sampled: 30003\n",
      "    num_steps_trained: 48768\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 250\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.0\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 20461\n",
      "  policy_reward_max:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12135346185867447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07460993674870131\n",
      "    mean_inference_ms: 1.7595237880252086\n",
      "    mean_raw_obs_processing_ms: 0.3095222110276509\n",
      "  time_since_restore: 117.53974556922913\n",
      "  time_this_iter_s: 1.339681625366211\n",
      "  time_total_s: 117.53974556922913\n",
      "  timers:\n",
      "    learn_throughput: 3626.859\n",
      "    learn_time_ms: 8.823\n",
      "    load_throughput: 63910.16\n",
      "    load_time_ms: 0.501\n",
      "    update_time_ms: 6.506\n",
      "  timestamp: 1648811829\n",
      "  timesteps_since_restore: 3072\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 30003\n",
      "  training_iteration: 96\n",
      "  trial_id: f3a64_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQNTrainer pid=20461)\u001b[0m 2022-04-01 04:17:09,113\tWARNING deprecation.py:46 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 04:17:09 (running for 00:02:17.19)<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_04:14:52/DQNTrainer_2022-04-01_04-14-52<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQNTrainer_foodenv_f3a64_00000</td><td>TERMINATED</td><td>172.17.0.2:20461</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">          117.54</td><td style=\"text-align: right;\">30003</td><td style=\"text-align: right;\">     -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 04:17:09,965\tINFO tune.py:636 -- Total run time: 137.78 seconds (137.14 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.dqn as dqn\n",
    "e = FoodGame(comp_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = comp_env_config\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "config[\"prioritized_replay\"] = False\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "# eval\n",
    "# config[\"evaluation_interval\"] = 1\n",
    "# config[\"evaluation_duration\"] = 1\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    dqn.DQNTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0efbc2",
   "metadata": {},
   "source": [
    "# Results Analysis (Comp + DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cadbfc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 00:04:43,375\tWARNING deprecation.py:46 -- DeprecationWarning: `config['buffer_size']` has been deprecated. Use `config['replay_buffer_config']['capacity']` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13858)\u001b[0m 2022-04-01 00:04:49,704\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-04-01 00:04:52,014\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-04-01 00:04:52,177\tINFO trainable.py:473 -- Restored on 172.17.0.2 from checkpoint: /home/ray/cs4246/31_03_2022_23:50:10/DQNTrainer_2022-03-31_23-50-10/DQNTrainer_foodenv_f928e_00000_0_2022-03-31_23-50-10/checkpoint_000100/checkpoint-100\n",
      "2022-04-01 00:04:52,178\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': 3200, '_time_total': 121.95173215866089, '_episodes_total': 2526}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state at Time 0:\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 1\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 2\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 3\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 4\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 5\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 6\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Episode Ended\n",
      "Episode Rewards {'agent0': 14, 'agent1': -6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=13858)\u001b[0m WARNING:tensorflow:From /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13858)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13858)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    }
   ],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = dqn.DQNTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ee5f",
   "metadata": {},
   "source": [
    "# MADDPG Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "059a29f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:09 (running for 00:00:00.12)<br>Memory usage on this node: 6.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m 2022-04-01 05:28:13,922\tINFO trainer.py:2055 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m 2022-04-01 05:28:13,923\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m 2022-04-01 05:28:13,923\tWARNING deprecation.py:46 -- DeprecationWarning: `config['buffer_size']` has been deprecated. Use `config['replay_buffer_config']['capacity']` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m   \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:342: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m   return layer.apply(inputs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:344: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m   out = tf1.layers.dense(feature, units=1, activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:370: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:372: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m   out, units=act_space.shape[0], activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m 2022-04-01 05:28:19,307\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m   \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:342: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m   return layer.apply(inputs)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:344: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m   out = tf1.layers.dense(feature, units=1, activation=None)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:370: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:372: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m   out, units=act_space.shape[0], activation=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:21 (running for 00:00:11.77)<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m 2022-04-01 05:28:21,521\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27070)\u001b[0m 2022-04-01 05:28:21,573\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m 2022-04-01 05:28:21,682\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:22 (running for 00:00:12.78)<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 2014\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-28-24\n",
      "  done: false\n",
      "  episode_len_mean: 19.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.0\n",
      "  episode_reward_mean: -20.264150943396228\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 53\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 1007\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 2014\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1007\n",
      "    num_steps_trained: 32\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.36\n",
      "    ram_util_percent: 47.92\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.132075471698114\n",
      "    policy1: -10.132075471698114\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09315046999189587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15732787904285253\n",
      "    mean_inference_ms: 1.4309999015596175\n",
      "    mean_raw_obs_processing_ms: 0.14904283341907318\n",
      "  time_since_restore: 2.821289539337158\n",
      "  time_this_iter_s: 2.821289539337158\n",
      "  time_total_s: 2.821289539337158\n",
      "  timers:\n",
      "    learn_throughput: 98.944\n",
      "    learn_time_ms: 323.416\n",
      "    update_time_ms: 4.977\n",
      "  timestamp: 1648816104\n",
      "  timesteps_since_restore: 32\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 1007\n",
      "  training_iteration: 1\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:27 (running for 00:00:18.13)<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         6.05644</td><td style=\"text-align: right;\">2049</td><td style=\"text-align: right;\">  -23.16</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 4950\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 18.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -21.98\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 131\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 2383\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 4950\n",
      "    num_agent_steps_trained: 5056\n",
      "    num_steps_sampled: 2475\n",
      "    num_steps_trained: 2528\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 13\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.1\n",
      "    ram_util_percent: 48.2\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.99\n",
      "    policy1: -10.99\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09348913697393396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15787380888080918\n",
      "    mean_inference_ms: 1.4048991030101279\n",
      "    mean_raw_obs_processing_ms: 0.14854650448663917\n",
      "  time_since_restore: 7.4964330196380615\n",
      "  time_this_iter_s: 0.33271050453186035\n",
      "  time_total_s: 7.4964330196380615\n",
      "  timers:\n",
      "    learn_throughput: 5410.5\n",
      "    learn_time_ms: 5.914\n",
      "    update_time_ms: 3.812\n",
      "  timestamp: 1648816109\n",
      "  timesteps_since_restore: 480\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 2475\n",
      "  training_iteration: 15\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:32 (running for 00:00:23.15)<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         10.6955</td><td style=\"text-align: right;\">3469</td><td style=\"text-align: right;\">  -17.84</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 8034\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-28-34\n",
      "  done: false\n",
      "  episode_len_mean: 18.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -18.36\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 213\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 3997\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 8034\n",
      "    num_agent_steps_trained: 10304\n",
      "    num_steps_sampled: 4017\n",
      "    num_steps_trained: 5152\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 27\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.18\n",
      "    policy1: -9.18\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09361100130233253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15809127123327696\n",
      "    mean_inference_ms: 1.3778482261462455\n",
      "    mean_raw_obs_processing_ms: 0.14885757254332949\n",
      "  time_since_restore: 12.415270328521729\n",
      "  time_this_iter_s: 0.37755703926086426\n",
      "  time_total_s: 12.415270328521729\n",
      "  timers:\n",
      "    learn_throughput: 5152.905\n",
      "    learn_time_ms: 6.21\n",
      "    update_time_ms: 3.819\n",
      "  timestamp: 1648816114\n",
      "  timesteps_since_restore: 928\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 4017\n",
      "  training_iteration: 29\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:38 (running for 00:00:28.55)<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         15.6977</td><td style=\"text-align: right;\">5062</td><td style=\"text-align: right;\">  -18.86</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 11170\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 18.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.0\n",
      "  episode_reward_mean: -18.2\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 297\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 5585\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 11170\n",
      "    num_agent_steps_trained: 15680\n",
      "    num_steps_sampled: 5585\n",
      "    num_steps_trained: 7840\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 41\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2\n",
      "    ram_util_percent: 48.3\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.1\n",
      "    policy1: -9.1\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0937800647707972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15831896803583814\n",
      "    mean_inference_ms: 1.3721755508411715\n",
      "    mean_raw_obs_processing_ms: 0.14931006692789225\n",
      "  time_since_restore: 17.358668327331543\n",
      "  time_this_iter_s: 0.3545081615447998\n",
      "  time_total_s: 17.358668327331543\n",
      "  timers:\n",
      "    learn_throughput: 4851.711\n",
      "    learn_time_ms: 6.596\n",
      "    update_time_ms: 4.094\n",
      "  timestamp: 1648816120\n",
      "  timesteps_since_restore: 1408\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 5585\n",
      "  training_iteration: 44\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:43 (running for 00:00:33.89)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         20.6252</td><td style=\"text-align: right;\">6661</td><td style=\"text-align: right;\">  -19.64</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 14366\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-28-45\n",
      "  done: false\n",
      "  episode_len_mean: 18.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -18.76\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 384\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 7083\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 14366\n",
      "    num_agent_steps_trained: 21184\n",
      "    num_steps_sampled: 7183\n",
      "    num_steps_trained: 10592\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 54\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.38\n",
      "    policy1: -9.38\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09347040370091428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15774759487284082\n",
      "    mean_inference_ms: 1.3639764298104091\n",
      "    mean_raw_obs_processing_ms: 0.14909563700799347\n",
      "  time_since_restore: 22.214566230773926\n",
      "  time_this_iter_s: 0.2915778160095215\n",
      "  time_total_s: 22.214566230773926\n",
      "  timers:\n",
      "    learn_throughput: 5268.151\n",
      "    learn_time_ms: 6.074\n",
      "    update_time_ms: 3.607\n",
      "  timestamp: 1648816125\n",
      "  timesteps_since_restore: 1888\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 7183\n",
      "  training_iteration: 59\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:48 (running for 00:00:39.22)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         25.5536</td><td style=\"text-align: right;\">8275</td><td style=\"text-align: right;\">  -16.16</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 17410\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-28-50\n",
      "  done: false\n",
      "  episode_len_mean: 18.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -18.64\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 468\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 8685\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 17410\n",
      "    num_agent_steps_trained: 26496\n",
      "    num_steps_sampled: 8705\n",
      "    num_steps_trained: 13248\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 68\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.3\n",
      "    ram_util_percent: 48.5\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.32\n",
      "    policy1: -9.32\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09245453964328487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15587874053512965\n",
      "    mean_inference_ms: 1.3500537606266323\n",
      "    mean_raw_obs_processing_ms: 0.1484042427031335\n",
      "  time_since_restore: 26.901233196258545\n",
      "  time_this_iter_s: 0.34821248054504395\n",
      "  time_total_s: 26.901233196258545\n",
      "  timers:\n",
      "    learn_throughput: 5319.072\n",
      "    learn_time_ms: 6.016\n",
      "    update_time_ms: 3.708\n",
      "  timestamp: 1648816130\n",
      "  timesteps_since_restore: 2336\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 8705\n",
      "  training_iteration: 73\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:54 (running for 00:00:44.58)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         30.4998</td><td style=\"text-align: right;\">9878</td><td style=\"text-align: right;\">   -17.2</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">              18.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 20426\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 18.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -16.08\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 550\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 10153\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 20426\n",
      "    num_agent_steps_trained: 31616\n",
      "    num_steps_sampled: 10213\n",
      "    num_steps_trained: 15808\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 81\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1\n",
      "    ram_util_percent: 48.6\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.04\n",
      "    policy1: -8.04\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09208168244539654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15525970445969556\n",
      "    mean_inference_ms: 1.3439247160512104\n",
      "    mean_raw_obs_processing_ms: 0.14797043864581141\n",
      "  time_since_restore: 31.733858108520508\n",
      "  time_this_iter_s: 0.5358765125274658\n",
      "  time_total_s: 31.733858108520508\n",
      "  timers:\n",
      "    learn_throughput: 4781.979\n",
      "    learn_time_ms: 6.692\n",
      "    update_time_ms: 4.086\n",
      "  timestamp: 1648816135\n",
      "  timesteps_since_restore: 2784\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 10213\n",
      "  training_iteration: 87\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:28:59 (running for 00:00:49.63)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         35.1824</td><td style=\"text-align: right;\">11277</td><td style=\"text-align: right;\">  -14.42</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=27071)\u001b[0m 2022-04-01 05:29:00,461\tWARNING deprecation.py:46 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 23378\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-00\n",
      "  done: false\n",
      "  episode_len_mean: 17.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -14.82\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 632\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 11603\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 23378\n",
      "    num_agent_steps_trained: 36864\n",
      "    num_steps_sampled: 11689\n",
      "    num_steps_trained: 18432\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 94\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.9\n",
      "    ram_util_percent: 48.7\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.41\n",
      "    policy1: -7.41\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09240952835670009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1559427619809367\n",
      "    mean_inference_ms: 1.346757566391583\n",
      "    mean_raw_obs_processing_ms: 0.148472625921596\n",
      "  time_since_restore: 36.51618814468384\n",
      "  time_this_iter_s: 0.3414170742034912\n",
      "  time_total_s: 36.51618814468384\n",
      "  timers:\n",
      "    learn_throughput: 5013.399\n",
      "    learn_time_ms: 6.383\n",
      "    update_time_ms: 3.764\n",
      "  timestamp: 1648816140\n",
      "  timesteps_since_restore: 3232\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 11689\n",
      "  training_iteration: 101\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:04 (running for 00:00:54.66)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         39.7841</td><td style=\"text-align: right;\">12690</td><td style=\"text-align: right;\">  -17.08</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 26398\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 18.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -16.8\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 714\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 13179\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 26398\n",
      "    num_agent_steps_trained: 42112\n",
      "    num_steps_sampled: 13199\n",
      "    num_steps_trained: 21056\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 108\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.4\n",
      "    policy1: -8.4\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09307905550319547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1571069511036117\n",
      "    mean_inference_ms: 1.3540694372902777\n",
      "    mean_raw_obs_processing_ms: 0.14931733718242157\n",
      "  time_since_restore: 41.415093183517456\n",
      "  time_this_iter_s: 0.3015162944793701\n",
      "  time_total_s: 41.415093183517456\n",
      "  timers:\n",
      "    learn_throughput: 5161.208\n",
      "    learn_time_ms: 6.2\n",
      "    update_time_ms: 3.789\n",
      "  timestamp: 1648816146\n",
      "  timesteps_since_restore: 3680\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 13199\n",
      "  training_iteration: 115\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:09 (running for 00:01:00.03)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         44.7518</td><td style=\"text-align: right;\">14273</td><td style=\"text-align: right;\">  -15.82</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 29408\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 18.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -15.28\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 798\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 14632\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 29408\n",
      "    num_agent_steps_trained: 47424\n",
      "    num_steps_sampled: 14704\n",
      "    num_steps_trained: 23712\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 121\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.64\n",
      "    policy1: -7.64\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09326709633173003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15739764377816903\n",
      "    mean_inference_ms: 1.3556494431188202\n",
      "    mean_raw_obs_processing_ms: 0.14958786240061808\n",
      "  time_since_restore: 46.09544658660889\n",
      "  time_this_iter_s: 0.3778247833251953\n",
      "  time_total_s: 46.09544658660889\n",
      "  timers:\n",
      "    learn_throughput: 5042.197\n",
      "    learn_time_ms: 6.346\n",
      "    update_time_ms: 3.846\n",
      "  timestamp: 1648816151\n",
      "  timesteps_since_restore: 4128\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 14704\n",
      "  training_iteration: 129\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:14 (running for 00:01:05.08)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         49.4219</td><td style=\"text-align: right;\">15768</td><td style=\"text-align: right;\">  -12.64</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 32374\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-16\n",
      "  done: false\n",
      "  episode_len_mean: 17.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -14.6\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 881\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 16103\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 32374\n",
      "    num_agent_steps_trained: 52672\n",
      "    num_steps_sampled: 16187\n",
      "    num_steps_trained: 26336\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 134\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.3\n",
      "    policy1: -7.3\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0930556926847324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15706683091260923\n",
      "    mean_inference_ms: 1.3521476172216773\n",
      "    mean_raw_obs_processing_ms: 0.1494966124626612\n",
      "  time_since_restore: 50.73649311065674\n",
      "  time_this_iter_s: 0.32797694206237793\n",
      "  time_total_s: 50.73649311065674\n",
      "  timers:\n",
      "    learn_throughput: 5020.864\n",
      "    learn_time_ms: 6.373\n",
      "    update_time_ms: 3.751\n",
      "  timestamp: 1648816156\n",
      "  timesteps_since_restore: 4576\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 16187\n",
      "  training_iteration: 143\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:20 (running for 00:01:10.49)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         54.4137</td><td style=\"text-align: right;\">17360</td><td style=\"text-align: right;\">  -19.34</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 35340\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-21\n",
      "  done: false\n",
      "  episode_len_mean: 18.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -20.38\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 959\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 17600\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 35340\n",
      "    num_agent_steps_trained: 57664\n",
      "    num_steps_sampled: 17670\n",
      "    num_steps_trained: 28832\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 147\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.5\n",
      "    ram_util_percent: 49.0\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.19\n",
      "    policy1: -10.19\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09301586130348614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15694251878109147\n",
      "    mean_inference_ms: 1.3508776524344313\n",
      "    mean_raw_obs_processing_ms: 0.1493817411784055\n",
      "  time_since_restore: 55.40067481994629\n",
      "  time_this_iter_s: 0.37369561195373535\n",
      "  time_total_s: 55.40067481994629\n",
      "  timers:\n",
      "    learn_throughput: 4800.039\n",
      "    learn_time_ms: 6.667\n",
      "    update_time_ms: 3.852\n",
      "  timestamp: 1648816161\n",
      "  timesteps_since_restore: 5024\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 17670\n",
      "  training_iteration: 157\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:25 (running for 00:01:15.57)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         59.1061</td><td style=\"text-align: right;\">18850</td><td style=\"text-align: right;\">  -17.14</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 38344\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 18.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -14.84\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1043\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 19172\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 38344\n",
      "    num_agent_steps_trained: 63040\n",
      "    num_steps_sampled: 19172\n",
      "    num_steps_trained: 31520\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 161\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.3\n",
      "    ram_util_percent: 49.0\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.42\n",
      "    policy1: -7.42\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09315027986227027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15716031174944142\n",
      "    mean_inference_ms: 1.3518980412379726\n",
      "    mean_raw_obs_processing_ms: 0.149524152448584\n",
      "  time_since_restore: 60.14247250556946\n",
      "  time_this_iter_s: 0.3573892116546631\n",
      "  time_total_s: 60.14247250556946\n",
      "  timers:\n",
      "    learn_throughput: 5294.752\n",
      "    learn_time_ms: 6.044\n",
      "    update_time_ms: 3.72\n",
      "  timestamp: 1648816166\n",
      "  timesteps_since_restore: 5472\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 19172\n",
      "  training_iteration: 171\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:30 (running for 00:01:20.93)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         64.0865</td><td style=\"text-align: right;\">20371</td><td style=\"text-align: right;\">  -11.16</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 41380\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 16.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -10.3\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1132\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 20690\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 41380\n",
      "    num_agent_steps_trained: 68736\n",
      "    num_steps_sampled: 20690\n",
      "    num_steps_trained: 34368\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 175\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.2\n",
      "    ram_util_percent: 49.1\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -5.15\n",
      "    policy1: -5.15\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09297452019270974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15686874066031334\n",
      "    mean_inference_ms: 1.3491491258195005\n",
      "    mean_raw_obs_processing_ms: 0.14950404259044195\n",
      "  time_since_restore: 65.14406085014343\n",
      "  time_this_iter_s: 0.36340785026550293\n",
      "  time_total_s: 65.14406085014343\n",
      "  timers:\n",
      "    learn_throughput: 4915.338\n",
      "    learn_time_ms: 6.51\n",
      "    update_time_ms: 4.111\n",
      "  timestamp: 1648816171\n",
      "  timesteps_since_restore: 5920\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 20690\n",
      "  training_iteration: 185\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:35 (running for 00:01:26.25)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         68.9823</td><td style=\"text-align: right;\">21930</td><td style=\"text-align: right;\">  -14.56</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 44514\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 18.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -17.06\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1214\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 22177\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 44514\n",
      "    num_agent_steps_trained: 73984\n",
      "    num_steps_sampled: 22257\n",
      "    num_steps_trained: 36992\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 188\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.53\n",
      "    policy1: -8.53\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09294383166094698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1567285319677558\n",
      "    mean_inference_ms: 1.3478946620050964\n",
      "    mean_raw_obs_processing_ms: 0.14952884840248287\n",
      "  time_since_restore: 69.96097588539124\n",
      "  time_this_iter_s: 0.34254002571105957\n",
      "  time_total_s: 69.96097588539124\n",
      "  timers:\n",
      "    learn_throughput: 5129.922\n",
      "    learn_time_ms: 6.238\n",
      "    update_time_ms: 3.749\n",
      "  timestamp: 1648816177\n",
      "  timesteps_since_restore: 6400\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 22257\n",
      "  training_iteration: 200\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:41 (running for 00:01:31.30)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         73.6286</td><td style=\"text-align: right;\">23399</td><td style=\"text-align: right;\">  -20.16</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 47416\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 19.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -20.56\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1290\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 23688\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 47416\n",
      "    num_agent_steps_trained: 78784\n",
      "    num_steps_sampled: 23708\n",
      "    num_steps_trained: 39392\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 201\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.28\n",
      "    policy1: -10.28\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09301520691476753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15672233292935583\n",
      "    mean_inference_ms: 1.348118857269481\n",
      "    mean_raw_obs_processing_ms: 0.1495190748421204\n",
      "  time_since_restore: 74.63646817207336\n",
      "  time_this_iter_s: 0.30769944190979004\n",
      "  time_total_s: 74.63646817207336\n",
      "  timers:\n",
      "    learn_throughput: 4928.387\n",
      "    learn_time_ms: 6.493\n",
      "    update_time_ms: 4.143\n",
      "  timestamp: 1648816182\n",
      "  timesteps_since_restore: 6848\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 23708\n",
      "  training_iteration: 214\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:46 (running for 00:01:36.46)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         78.4127</td><td style=\"text-align: right;\">24919</td><td style=\"text-align: right;\">  -17.34</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 50466\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-47\n",
      "  done: false\n",
      "  episode_len_mean: 18.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -16.42\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1375\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 25147\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 50466\n",
      "    num_agent_steps_trained: 84160\n",
      "    num_steps_sampled: 25233\n",
      "    num_steps_trained: 42080\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 214\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.1\n",
      "    ram_util_percent: 49.1\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.21\n",
      "    policy1: -8.21\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09320367107019858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15702237100108368\n",
      "    mean_inference_ms: 1.3502266645674859\n",
      "    mean_raw_obs_processing_ms: 0.14975220108428064\n",
      "  time_since_restore: 79.44781494140625\n",
      "  time_this_iter_s: 0.41592955589294434\n",
      "  time_total_s: 79.44781494140625\n",
      "  timers:\n",
      "    learn_throughput: 4641.0\n",
      "    learn_time_ms: 6.895\n",
      "    update_time_ms: 3.992\n",
      "  timestamp: 1648816187\n",
      "  timesteps_since_restore: 7296\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 25233\n",
      "  training_iteration: 228\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:51 (running for 00:01:41.80)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         83.3531</td><td style=\"text-align: right;\">26423</td><td style=\"text-align: right;\">  -16.26</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 53468\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-52\n",
      "  done: false\n",
      "  episode_len_mean: 17.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -14.3\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1458\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 26724\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 53468\n",
      "    num_agent_steps_trained: 89472\n",
      "    num_steps_sampled: 26734\n",
      "    num_steps_trained: 44736\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 228\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.15\n",
      "    policy1: -7.15\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09334313505431684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15723329384805163\n",
      "    mean_inference_ms: 1.351590827729457\n",
      "    mean_raw_obs_processing_ms: 0.14993434314847282\n",
      "  time_since_restore: 84.41073846817017\n",
      "  time_this_iter_s: 0.3362717628479004\n",
      "  time_total_s: 84.41073846817017\n",
      "  timers:\n",
      "    learn_throughput: 4847.803\n",
      "    learn_time_ms: 6.601\n",
      "    update_time_ms: 3.908\n",
      "  timestamp: 1648816192\n",
      "  timesteps_since_restore: 7744\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 26734\n",
      "  training_iteration: 242\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:29:56 (running for 00:01:47.04)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         88.2034</td><td style=\"text-align: right;\">27910</td><td style=\"text-align: right;\">  -15.68</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 56442\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-29-57\n",
      "  done: false\n",
      "  episode_len_mean: 17.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: -15.16\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1540\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 28167\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 56442\n",
      "    num_agent_steps_trained: 94720\n",
      "    num_steps_sampled: 28221\n",
      "    num_steps_trained: 47360\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 241\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.3\n",
      "    ram_util_percent: 49.1\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.58\n",
      "    policy1: -7.58\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09372351464437081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15786161399509613\n",
      "    mean_inference_ms: 1.3558112610322564\n",
      "    mean_raw_obs_processing_ms: 0.1503864568539398\n",
      "  time_since_restore: 89.19617748260498\n",
      "  time_this_iter_s: 0.3208446502685547\n",
      "  time_total_s: 89.19617748260498\n",
      "  timers:\n",
      "    learn_throughput: 5024.454\n",
      "    learn_time_ms: 6.369\n",
      "    update_time_ms: 3.823\n",
      "  timestamp: 1648816197\n",
      "  timesteps_since_restore: 8192\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 28221\n",
      "  training_iteration: 256\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:30:02 (running for 00:01:52.34)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>RUNNING </td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         93.1012</td><td style=\"text-align: right;\">29395</td><td style=\"text-align: right;\">  -16.38</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 59428\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 18.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -16.64\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1621\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29651\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 59428\n",
      "    num_agent_steps_trained: 99840\n",
      "    num_steps_sampled: 29714\n",
      "    num_steps_trained: 49920\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 254\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.32\n",
      "    policy1: -8.32\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09388566885566402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1581779619264986\n",
      "    mean_inference_ms: 1.3578247656976163\n",
      "    mean_raw_obs_processing_ms: 0.1505316483405228\n",
      "  time_since_restore: 94.09165358543396\n",
      "  time_this_iter_s: 0.3227815628051758\n",
      "  time_total_s: 94.09165358543396\n",
      "  timers:\n",
      "    learn_throughput: 5134.455\n",
      "    learn_time_ms: 6.232\n",
      "    update_time_ms: 3.812\n",
      "  timestamp: 1648816203\n",
      "  timesteps_since_restore: 8640\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 29714\n",
      "  training_iteration: 270\n",
      "  trial_id: 30cc8_00000\n",
      "  \n",
      "Result for contrib_MADDPG_foodenv_30cc8_00000:\n",
      "  agent_timesteps_total: 60050\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-30-04\n",
      "  done: true\n",
      "  episode_len_mean: 18.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -16.56\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1638\n",
      "  experiment_id: 987af3d6744e4009b4bc9d92518314c0\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29965\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 60050\n",
      "    num_agent_steps_trained: 100928\n",
      "    num_steps_sampled: 30025\n",
      "    num_steps_trained: 50464\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 257\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.4\n",
      "    ram_util_percent: 49.1\n",
      "  pid: 27071\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.28\n",
      "    policy1: -8.28\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09394933382694219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15828800694451856\n",
      "    mean_inference_ms: 1.358584496657514\n",
      "    mean_raw_obs_processing_ms: 0.15059359527528768\n",
      "  time_since_restore: 95.13726425170898\n",
      "  time_this_iter_s: 0.32519030570983887\n",
      "  time_total_s: 95.13726425170898\n",
      "  timers:\n",
      "    learn_throughput: 4833.975\n",
      "    learn_time_ms: 6.62\n",
      "    update_time_ms: 3.781\n",
      "  timestamp: 1648816204\n",
      "  timesteps_since_restore: 8736\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 30025\n",
      "  training_iteration: 273\n",
      "  trial_id: 30cc8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:30:04 (running for 00:01:54.60)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:28:09/contrib/MADDPG<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_30cc8_00000</td><td>TERMINATED</td><td>172.17.0.2:27071</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         95.1373</td><td style=\"text-align: right;\">30025</td><td style=\"text-align: right;\">  -16.56</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 05:30:04,765\tINFO tune.py:636 -- Total run time: 115.02 seconds (114.58 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "coop_env_config['trainer'] = 'maddpg'\n",
    "comp_env_config['trainer'] = 'maddpg'\n",
    "e = FoodGame(coop_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "\n",
    "# model\n",
    "config[\"actor_hiddens\"] = [256, 256]\n",
    "config[\"actor_hidden_activation\"] = \"tanh\"\n",
    "config[\"critic_hiddens\"] = [256, 256]\n",
    "config[\"critic_hidden_activation\"] = \"tanh\"\n",
    "config[\"learning_starts\"] = 1000 # in terms of samples\n",
    "config[\"critic_lr\"] = 1e-2 # in terms of samples\n",
    "config[\"actor_lr\"] = 1e-2 # in terms of samples\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    \"contrib/MADDPG\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eb176",
   "metadata": {},
   "source": [
    "# Results Analysis (Coop + MADDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f2edf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m   \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:342: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m   return layer.apply(inputs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:344: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m   out = tf1.layers.dense(feature, units=1, activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:370: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:372: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m   out, units=act_space.shape[0], activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2860)\u001b[0m 2022-03-06 05:54:35,499\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-06 05:54:38,040\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-06 05:54:38,128\tINFO trainable.py:473 -- Restored on 172.17.0.2 from checkpoint: /home/ray/cs4246/06_03_2022_05:42:24/contrib/MADDPG/contrib_MADDPG_foodenv_41590_00000_0_2022-03-06_05-42-24/checkpoint_000020/checkpoint-20\n",
      "2022-03-06 05:54:38,130\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 20480, '_time_total': 681.5899374485016, '_episodes_total': 1157}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state at Time 0:\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 1\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 2\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 3\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '1' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 4\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 5\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '1' '' '' '' '']]\n",
      "\n",
      "Time: 6\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 7\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 8\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 9\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 10\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 11\n",
      "[['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 12\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 13\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '1' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 14\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 15\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '1' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 16\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 17\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '1' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 18\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 19\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 20\n",
      "[['' '' '0' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Episode Ended\n",
      "Episode Rewards {'agent0': -20, 'agent1': -20}\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.contrib.maddpg as maddpg\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = maddpg.MADDPGTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca3a2fa",
   "metadata": {},
   "source": [
    "# MADDPG Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4ae56e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:05 (running for 00:00:00.13)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m 2022-04-01 05:25:09,635\tINFO trainer.py:2055 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m 2022-04-01 05:25:09,637\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m 2022-04-01 05:25:09,637\tWARNING deprecation.py:46 -- DeprecationWarning: `config['buffer_size']` has been deprecated. Use `config['replay_buffer_config']['capacity']` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:342: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   return layer.apply(inputs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:344: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   out = tf1.layers.dense(feature, units=1, activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:370: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:372: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   out, units=act_space.shape[0], activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m 2022-04-01 05:25:15,450\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m   \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:342: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m   return layer.apply(inputs)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:344: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m   out = tf1.layers.dense(feature, units=1, activation=None)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:370: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:372: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m   out, units=act_space.shape[0], activation=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:17 (running for 00:00:12.33)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m 2022-04-01 05:25:17,624\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m 2022-04-01 05:25:17,679\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m 2022-04-01 05:25:17,788\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:18 (running for 00:00:13.33)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 2020\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-20\n",
      "  done: false\n",
      "  episode_len_mean: 19.80392156862745\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 18.0\n",
      "  episode_reward_mean: -28.627450980392158\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 51\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 1010\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 2020\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1010\n",
      "    num_steps_trained: 32\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.24\n",
      "    ram_util_percent: 53.9\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 9.0\n",
      "    policy1: 9.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -14.313725490196079\n",
      "    policy1: -14.313725490196079\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09916868690920866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1638562931622996\n",
      "    mean_inference_ms: 1.4866365287471364\n",
      "    mean_raw_obs_processing_ms: 0.15475841941041127\n",
      "  time_since_restore: 2.9996285438537598\n",
      "  time_this_iter_s: 2.9996285438537598\n",
      "  time_total_s: 2.9996285438537598\n",
      "  timers:\n",
      "    learn_throughput: 81.37\n",
      "    learn_time_ms: 393.267\n",
      "    update_time_ms: 5.555\n",
      "  timestamp: 1648815920\n",
      "  timesteps_since_restore: 32\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 1010\n",
      "  training_iteration: 1\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:23 (running for 00:00:18.61)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.99324</td><td style=\"text-align: right;\">1910</td><td style=\"text-align: right;\">-29.7917</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           19.8958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 4820\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-25\n",
      "  done: false\n",
      "  episode_len_mean: 20.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20.0\n",
      "  episode_reward_mean: -32.4\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 121\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 2330\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 4820\n",
      "    num_agent_steps_trained: 4544\n",
      "    num_steps_sampled: 2410\n",
      "    num_steps_trained: 2272\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 12\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: -10.0\n",
      "    policy1: -10.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -16.2\n",
      "    policy1: -16.2\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10041631009380098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16691172475235763\n",
      "    mean_inference_ms: 1.4834898854602443\n",
      "    mean_raw_obs_processing_ms: 0.15566741953505692\n",
      "  time_since_restore: 7.731484413146973\n",
      "  time_this_iter_s: 0.3303699493408203\n",
      "  time_total_s: 7.731484413146973\n",
      "  timers:\n",
      "    learn_throughput: 4619.675\n",
      "    learn_time_ms: 6.927\n",
      "    update_time_ms: 4.002\n",
      "  timestamp: 1648815925\n",
      "  timesteps_since_restore: 480\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 2410\n",
      "  training_iteration: 15\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:29 (running for 00:00:23.81)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         10.8762</td><td style=\"text-align: right;\">3410</td><td style=\"text-align: right;\">   -35.8</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 7820\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-30\n",
      "  done: false\n",
      "  episode_len_mean: 19.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 22.0\n",
      "  episode_reward_mean: -36.0\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 197\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 3890\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 7820\n",
      "    num_agent_steps_trained: 9408\n",
      "    num_steps_sampled: 3910\n",
      "    num_steps_trained: 4704\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 25\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 11.0\n",
      "    policy1: 11.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -18.0\n",
      "    policy1: -18.0\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10105234621234399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16907689615980775\n",
      "    mean_inference_ms: 1.4655916643661357\n",
      "    mean_raw_obs_processing_ms: 0.15590795489242396\n",
      "  time_since_restore: 12.449599504470825\n",
      "  time_this_iter_s: 0.30271387100219727\n",
      "  time_total_s: 12.449599504470825\n",
      "  timers:\n",
      "    learn_throughput: 5028.708\n",
      "    learn_time_ms: 6.363\n",
      "    update_time_ms: 3.962\n",
      "  timestamp: 1648815930\n",
      "  timesteps_since_restore: 960\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 3910\n",
      "  training_iteration: 30\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:34 (running for 00:00:28.99)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         15.6459</td><td style=\"text-align: right;\">4925</td><td style=\"text-align: right;\">   -32.9</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 10700\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-35\n",
      "  done: false\n",
      "  episode_len_mean: 19.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -27.2\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 272\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 5310\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 10700\n",
      "    num_agent_steps_trained: 14144\n",
      "    num_steps_sampled: 5350\n",
      "    num_steps_trained: 7072\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 37\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -13.6\n",
      "    policy1: -13.6\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09958402802636027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16658323728317012\n",
      "    mean_inference_ms: 1.4387606597992695\n",
      "    mean_raw_obs_processing_ms: 0.15418603471050102\n",
      "  time_since_restore: 17.080541610717773\n",
      "  time_this_iter_s: 0.34777092933654785\n",
      "  time_total_s: 17.080541610717773\n",
      "  timers:\n",
      "    learn_throughput: 4663.495\n",
      "    learn_time_ms: 6.862\n",
      "    update_time_ms: 3.98\n",
      "  timestamp: 1648815935\n",
      "  timesteps_since_restore: 1408\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 5350\n",
      "  training_iteration: 44\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:39 (running for 00:00:34.01)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         20.3021</td><td style=\"text-align: right;\">6323</td><td style=\"text-align: right;\">  -20.06</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 13706\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 18.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.0\n",
      "  episode_reward_mean: -19.16\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 351\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 6797\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 13706\n",
      "    num_agent_steps_trained: 19200\n",
      "    num_steps_sampled: 6853\n",
      "    num_steps_trained: 9600\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 50\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 12.0\n",
      "    policy1: 12.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.58\n",
      "    policy1: -9.58\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09977648585108899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1667474195862042\n",
      "    mean_inference_ms: 1.43659965629248\n",
      "    mean_raw_obs_processing_ms: 0.1548213830342552\n",
      "  time_since_restore: 22.0727801322937\n",
      "  time_this_iter_s: 0.3902466297149658\n",
      "  time_total_s: 22.0727801322937\n",
      "  timers:\n",
      "    learn_throughput: 4767.744\n",
      "    learn_time_ms: 6.712\n",
      "    update_time_ms: 3.989\n",
      "  timestamp: 1648815941\n",
      "  timesteps_since_restore: 1856\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 6853\n",
      "  training_iteration: 58\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:44 (running for 00:00:39.12)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         25.0404</td><td style=\"text-align: right;\">7715</td><td style=\"text-align: right;\">  -22.52</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             19.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 16644\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-46\n",
      "  done: false\n",
      "  episode_len_mean: 19.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -22.9\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 428\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 8314\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 16644\n",
      "    num_agent_steps_trained: 24128\n",
      "    num_steps_sampled: 8322\n",
      "    num_steps_trained: 12064\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 63\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.7\n",
      "    ram_util_percent: 54.8\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -11.45\n",
      "    policy1: -11.45\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1006015791264096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16799718449729972\n",
      "    mean_inference_ms: 1.4461906259208126\n",
      "    mean_raw_obs_processing_ms: 0.15613537927698773\n",
      "  time_since_restore: 27.010186910629272\n",
      "  time_this_iter_s: 0.3679349422454834\n",
      "  time_total_s: 27.010186910629272\n",
      "  timers:\n",
      "    learn_throughput: 5150.987\n",
      "    learn_time_ms: 6.212\n",
      "    update_time_ms: 3.942\n",
      "  timestamp: 1648815946\n",
      "  timesteps_since_restore: 2304\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 8322\n",
      "  training_iteration: 72\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:49 (running for 00:00:44.29)<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         29.8205</td><td style=\"text-align: right;\">9158</td><td style=\"text-align: right;\">   -18.8</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">              18.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 19552\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-51\n",
      "  done: false\n",
      "  episode_len_mean: 18.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -20.62\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 505\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 9716\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 19552\n",
      "    num_agent_steps_trained: 28928\n",
      "    num_steps_sampled: 9776\n",
      "    num_steps_trained: 14464\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 75\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.9\n",
      "    ram_util_percent: 54.9\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.31\n",
      "    policy1: -10.31\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1002642784413305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16756505276559636\n",
      "    mean_inference_ms: 1.4420121802554657\n",
      "    mean_raw_obs_processing_ms: 0.15606230512859437\n",
      "  time_since_restore: 31.779311418533325\n",
      "  time_this_iter_s: 0.3953883647918701\n",
      "  time_total_s: 31.779311418533325\n",
      "  timers:\n",
      "    learn_throughput: 4820.589\n",
      "    learn_time_ms: 6.638\n",
      "    update_time_ms: 3.964\n",
      "  timestamp: 1648815951\n",
      "  timesteps_since_restore: 2752\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 9776\n",
      "  training_iteration: 86\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:54 (running for 00:00:49.29)<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         34.4281</td><td style=\"text-align: right;\">10610</td><td style=\"text-align: right;\">  -21.84</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 22672\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-25-57\n",
      "  done: false\n",
      "  episode_len_mean: 18.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -20.76\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 592\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 11316\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 22672\n",
      "    num_agent_steps_trained: 34240\n",
      "    num_steps_sampled: 11336\n",
      "    num_steps_trained: 17120\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 89\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.1\n",
      "    ram_util_percent: 55.0\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -10.38\n",
      "    policy1: -10.38\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09929729443011359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16634789700846897\n",
      "    mean_inference_ms: 1.4290804203237912\n",
      "    mean_raw_obs_processing_ms: 0.15511819635650997\n",
      "  time_since_restore: 36.69110441207886\n",
      "  time_this_iter_s: 0.3034348487854004\n",
      "  time_total_s: 36.69110441207886\n",
      "  timers:\n",
      "    learn_throughput: 4865.076\n",
      "    learn_time_ms: 6.577\n",
      "    update_time_ms: 3.868\n",
      "  timestamp: 1648815957\n",
      "  timesteps_since_restore: 3232\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 11336\n",
      "  training_iteration: 101\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:25:59 (running for 00:00:54.50)<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         39.2179</td><td style=\"text-align: right;\">12144</td><td style=\"text-align: right;\">  -19.68</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 25742\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 18.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.0\n",
      "  episode_reward_mean: -19.02\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 674\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 12824\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 25742\n",
      "    num_agent_steps_trained: 39488\n",
      "    num_steps_sampled: 12871\n",
      "    num_steps_trained: 19744\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 102\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.51\n",
      "    policy1: -9.51\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09851039006302362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16514132483666558\n",
      "    mean_inference_ms: 1.4178019468917151\n",
      "    mean_raw_obs_processing_ms: 0.15414165244721117\n",
      "  time_since_restore: 41.50987696647644\n",
      "  time_this_iter_s: 0.37650370597839355\n",
      "  time_total_s: 41.50987696647644\n",
      "  timers:\n",
      "    learn_throughput: 5143.94\n",
      "    learn_time_ms: 6.221\n",
      "    update_time_ms: 3.891\n",
      "  timestamp: 1648815962\n",
      "  timesteps_since_restore: 3712\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 12871\n",
      "  training_iteration: 116\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:04 (running for 00:00:59.64)<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         43.9627</td><td style=\"text-align: right;\">13625</td><td style=\"text-align: right;\">  -15.78</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 28730\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-07\n",
      "  done: false\n",
      "  episode_len_mean: 18.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -15.62\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 755\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 14285\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 28730\n",
      "    num_agent_steps_trained: 44672\n",
      "    num_steps_sampled: 14365\n",
      "    num_steps_trained: 22336\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 115\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.1\n",
      "    ram_util_percent: 55.2\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.81\n",
      "    policy1: -7.81\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09807704883716134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16469968525998127\n",
      "    mean_inference_ms: 1.4119116011109816\n",
      "    mean_raw_obs_processing_ms: 0.1536729754698806\n",
      "  time_since_restore: 46.405909299850464\n",
      "  time_this_iter_s: 0.31376147270202637\n",
      "  time_total_s: 46.405909299850464\n",
      "  timers:\n",
      "    learn_throughput: 4875.999\n",
      "    learn_time_ms: 6.563\n",
      "    update_time_ms: 4.051\n",
      "  timestamp: 1648815967\n",
      "  timesteps_since_restore: 4160\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 14365\n",
      "  training_iteration: 130\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:10 (running for 00:01:04.76)<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">          48.681</td><td style=\"text-align: right;\">15080</td><td style=\"text-align: right;\">  -17.26</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 31658\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-12\n",
      "  done: false\n",
      "  episode_len_mean: 17.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -13.84\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 839\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 15760\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 31658\n",
      "    num_agent_steps_trained: 49920\n",
      "    num_steps_sampled: 15829\n",
      "    num_steps_trained: 24960\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 128\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.5\n",
      "    ram_util_percent: 55.2\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -6.92\n",
      "    policy1: -6.92\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09800854434359557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16466066208513927\n",
      "    mean_inference_ms: 1.4103576470723962\n",
      "    mean_raw_obs_processing_ms: 0.15373417403607875\n",
      "  time_since_restore: 51.115065574645996\n",
      "  time_this_iter_s: 0.3457458019256592\n",
      "  time_total_s: 51.115065574645996\n",
      "  timers:\n",
      "    learn_throughput: 5020.732\n",
      "    learn_time_ms: 6.374\n",
      "    update_time_ms: 3.884\n",
      "  timestamp: 1648815972\n",
      "  timesteps_since_restore: 4608\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 15829\n",
      "  training_iteration: 144\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:15 (running for 00:01:09.92)<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         53.4469</td><td style=\"text-align: right;\">16564</td><td style=\"text-align: right;\">  -14.34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 34636\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 18.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -14.94\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 920\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 17251\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 34636\n",
      "    num_agent_steps_trained: 55104\n",
      "    num_steps_sampled: 17318\n",
      "    num_steps_trained: 27552\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 141\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.47\n",
      "    policy1: -7.47\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09767232620093207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16410971222227527\n",
      "    mean_inference_ms: 1.4055407003789921\n",
      "    mean_raw_obs_processing_ms: 0.15335676287747302\n",
      "  time_since_restore: 55.82532501220703\n",
      "  time_this_iter_s: 0.3400139808654785\n",
      "  time_total_s: 55.82532501220703\n",
      "  timers:\n",
      "    learn_throughput: 4948.411\n",
      "    learn_time_ms: 6.467\n",
      "    update_time_ms: 3.843\n",
      "  timestamp: 1648815977\n",
      "  timesteps_since_restore: 5056\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 17318\n",
      "  training_iteration: 158\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:20 (running for 00:01:15.24)<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         58.3755</td><td style=\"text-align: right;\">18069</td><td style=\"text-align: right;\">  -18.54</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 37616\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 18.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -15.6\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1002\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 18808\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 37616\n",
      "    num_agent_steps_trained: 60352\n",
      "    num_steps_sampled: 18808\n",
      "    num_steps_trained: 30176\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 155\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.5\n",
      "    ram_util_percent: 55.4\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.8\n",
      "    policy1: -7.8\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09727307372271199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16358271280271225\n",
      "    mean_inference_ms: 1.4004512881633144\n",
      "    mean_raw_obs_processing_ms: 0.1528205288950114\n",
      "  time_since_restore: 60.76395559310913\n",
      "  time_this_iter_s: 0.3053581714630127\n",
      "  time_total_s: 60.76395559310913\n",
      "  timers:\n",
      "    learn_throughput: 5127.002\n",
      "    learn_time_ms: 6.241\n",
      "    update_time_ms: 3.842\n",
      "  timestamp: 1648815983\n",
      "  timesteps_since_restore: 5504\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 18808\n",
      "  training_iteration: 172\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:25 (running for 00:01:20.49)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         63.2207</td><td style=\"text-align: right;\">19560</td><td style=\"text-align: right;\">  -11.86</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             17.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 40638\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 17.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -13.46\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1089\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 20247\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 40638\n",
      "    num_agent_steps_trained: 65728\n",
      "    num_steps_sampled: 20319\n",
      "    num_steps_trained: 32864\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 168\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -6.73\n",
      "    policy1: -6.73\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09688670489914228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16306350528005228\n",
      "    mean_inference_ms: 1.3954989166803948\n",
      "    mean_raw_obs_processing_ms: 0.15250666246450123\n",
      "  time_since_restore: 65.47229337692261\n",
      "  time_this_iter_s: 0.33063292503356934\n",
      "  time_total_s: 65.47229337692261\n",
      "  timers:\n",
      "    learn_throughput: 5274.03\n",
      "    learn_time_ms: 6.067\n",
      "    update_time_ms: 3.645\n",
      "  timestamp: 1648815988\n",
      "  timesteps_since_restore: 5952\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 20319\n",
      "  training_iteration: 186\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:30 (running for 00:01:25.64)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">          67.968</td><td style=\"text-align: right;\">21160</td><td style=\"text-align: right;\">  -15.74</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 43840\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-33\n",
      "  done: false\n",
      "  episode_len_mean: 17.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -16.46\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1179\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 21820\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 43840\n",
      "    num_agent_steps_trained: 71360\n",
      "    num_steps_sampled: 21920\n",
      "    num_steps_trained: 35680\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 182\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0\n",
      "    ram_util_percent: 49.1\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.23\n",
      "    policy1: -8.23\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0961543015068488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1618521701421832\n",
      "    mean_inference_ms: 1.3865124130092101\n",
      "    mean_raw_obs_processing_ms: 0.15165027822355795\n",
      "  time_since_restore: 70.26148676872253\n",
      "  time_this_iter_s: 0.344651460647583\n",
      "  time_total_s: 70.26148676872253\n",
      "  timers:\n",
      "    learn_throughput: 5348.875\n",
      "    learn_time_ms: 5.983\n",
      "    update_time_ms: 3.785\n",
      "  timestamp: 1648815993\n",
      "  timesteps_since_restore: 6432\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 21920\n",
      "  training_iteration: 201\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:36 (running for 00:01:30.88)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         72.8069</td><td style=\"text-align: right;\">22776</td><td style=\"text-align: right;\">   -16.6</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">                18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 47032\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-38\n",
      "  done: false\n",
      "  episode_len_mean: 18.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 28.0\n",
      "  episode_reward_mean: -16.82\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1265\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 23436\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 47032\n",
      "    num_agent_steps_trained: 76864\n",
      "    num_steps_sampled: 23516\n",
      "    num_steps_trained: 38432\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 196\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 14.0\n",
      "    policy1: 14.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.41\n",
      "    policy1: -8.41\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09549841704911576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16084544126258127\n",
      "    mean_inference_ms: 1.3783605933426895\n",
      "    mean_raw_obs_processing_ms: 0.15089039560856818\n",
      "  time_since_restore: 75.13667845726013\n",
      "  time_this_iter_s: 0.2960031032562256\n",
      "  time_total_s: 75.13667845726013\n",
      "  timers:\n",
      "    learn_throughput: 5016.585\n",
      "    learn_time_ms: 6.379\n",
      "    update_time_ms: 4.76\n",
      "  timestamp: 1648815998\n",
      "  timesteps_since_restore: 6912\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 23516\n",
      "  training_iteration: 216\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:41 (running for 00:01:35.95)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         77.4875</td><td style=\"text-align: right;\">24270</td><td style=\"text-align: right;\">  -17.14</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 50094\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 18.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -15.94\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1349\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 25010\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 50094\n",
      "    num_agent_steps_trained: 82176\n",
      "    num_steps_sampled: 25047\n",
      "    num_steps_trained: 41088\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 210\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -7.97\n",
      "    policy1: -7.97\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0952055513133748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1604785028041665\n",
      "    mean_inference_ms: 1.3749029093433618\n",
      "    mean_raw_obs_processing_ms: 0.15054805716132733\n",
      "  time_since_restore: 79.94870042800903\n",
      "  time_this_iter_s: 0.3600142002105713\n",
      "  time_total_s: 79.94870042800903\n",
      "  timers:\n",
      "    learn_throughput: 5103.995\n",
      "    learn_time_ms: 6.27\n",
      "    update_time_ms: 3.875\n",
      "  timestamp: 1648816003\n",
      "  timesteps_since_restore: 7360\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 25047\n",
      "  training_iteration: 230\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:46 (running for 00:01:41.26)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         82.3997</td><td style=\"text-align: right;\">25805</td><td style=\"text-align: right;\">  -17.18</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 53120\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-48\n",
      "  done: false\n",
      "  episode_len_mean: 18.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -17.54\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1429\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 26501\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 53120\n",
      "    num_agent_steps_trained: 87296\n",
      "    num_steps_sampled: 26560\n",
      "    num_steps_trained: 43648\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 223\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.4\n",
      "    ram_util_percent: 49.1\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 17.0\n",
      "    policy1: 17.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.77\n",
      "    policy1: -8.77\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09509745394884817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16046197215931401\n",
      "    mean_inference_ms: 1.3742679394582569\n",
      "    mean_raw_obs_processing_ms: 0.15043278583898664\n",
      "  time_since_restore: 84.65994763374329\n",
      "  time_this_iter_s: 0.34562158584594727\n",
      "  time_total_s: 84.65994763374329\n",
      "  timers:\n",
      "    learn_throughput: 5259.233\n",
      "    learn_time_ms: 6.085\n",
      "    update_time_ms: 3.54\n",
      "  timestamp: 1648816008\n",
      "  timesteps_since_restore: 7808\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 26560\n",
      "  training_iteration: 244\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:51 (running for 00:01:46.51)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         87.2383</td><td style=\"text-align: right;\">27416</td><td style=\"text-align: right;\">  -18.26</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 56302\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-54\n",
      "  done: false\n",
      "  episode_len_mean: 18.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -17.2\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1516\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 28111\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 56302\n",
      "    num_agent_steps_trained: 92864\n",
      "    num_steps_sampled: 28151\n",
      "    num_steps_trained: 46432\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 237\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.6\n",
      "    policy1: -8.6\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09471959050284434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1599294780887306\n",
      "    mean_inference_ms: 1.370000235630547\n",
      "    mean_raw_obs_processing_ms: 0.14989409997421674\n",
      "  time_since_restore: 89.47194647789001\n",
      "  time_this_iter_s: 0.37034034729003906\n",
      "  time_total_s: 89.47194647789001\n",
      "  timers:\n",
      "    learn_throughput: 5260.635\n",
      "    learn_time_ms: 6.083\n",
      "    update_time_ms: 3.591\n",
      "  timestamp: 1648816014\n",
      "  timesteps_since_restore: 8288\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 28151\n",
      "  training_iteration: 259\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:26:56 (running for 00:01:51.68)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>RUNNING </td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         92.0076</td><td style=\"text-align: right;\">29003</td><td style=\"text-align: right;\">  -16.52</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 59490\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-26-59\n",
      "  done: false\n",
      "  episode_len_mean: 18.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -16.32\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1601\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29705\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 59490\n",
      "    num_agent_steps_trained: 98304\n",
      "    num_steps_sampled: 29745\n",
      "    num_steps_trained: 49152\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 251\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2\n",
      "    ram_util_percent: 49.1\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 16.0\n",
      "    policy1: 16.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -8.16\n",
      "    policy1: -8.16\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09428868980659458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15930033114604558\n",
      "    mean_inference_ms: 1.3644568392163978\n",
      "    mean_raw_obs_processing_ms: 0.14934033783370254\n",
      "  time_since_restore: 94.25655174255371\n",
      "  time_this_iter_s: 0.2929081916809082\n",
      "  time_total_s: 94.25655174255371\n",
      "  timers:\n",
      "    learn_throughput: 5380.438\n",
      "    learn_time_ms: 5.947\n",
      "    update_time_ms: 3.666\n",
      "  timestamp: 1648816019\n",
      "  timesteps_since_restore: 8768\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 29745\n",
      "  training_iteration: 274\n",
      "  trial_id: c2dbf_00000\n",
      "  \n",
      "Result for contrib_MADDPG_foodenv_c2dbf_00000:\n",
      "  agent_timesteps_total: 60114\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-27-00\n",
      "  done: true\n",
      "  episode_len_mean: 18.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: -18.12\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1617\n",
      "  experiment_id: eb5006d1bbc74e1d929a5571fd0b2b83\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 30057\n",
      "    learner:\n",
      "      policy0:\n",
      "        learner_stats: {}\n",
      "      policy1:\n",
      "        learner_stats: {}\n",
      "    num_agent_steps_sampled: 60114\n",
      "    num_agent_steps_trained: 99328\n",
      "    num_steps_sampled: 30057\n",
      "    num_steps_trained: 49664\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 254\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 26986\n",
      "  policy_reward_max:\n",
      "    policy0: 15.0\n",
      "    policy1: 15.0\n",
      "  policy_reward_mean:\n",
      "    policy0: -9.06\n",
      "    policy1: -9.06\n",
      "  policy_reward_min:\n",
      "    policy0: -20.0\n",
      "    policy1: -20.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09420536691639375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15918813025564904\n",
      "    mean_inference_ms: 1.3634634569841677\n",
      "    mean_raw_obs_processing_ms: 0.1492370480212925\n",
      "  time_since_restore: 95.17267060279846\n",
      "  time_this_iter_s: 0.33479833602905273\n",
      "  time_total_s: 95.17267060279846\n",
      "  timers:\n",
      "    learn_throughput: 5168.363\n",
      "    learn_time_ms: 6.192\n",
      "    update_time_ms: 3.645\n",
      "  timestamp: 1648816020\n",
      "  timesteps_since_restore: 8864\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 30057\n",
      "  training_iteration: 277\n",
      "  trial_id: c2dbf_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MADDPGTrainer pid=26986)\u001b[0m 2022-04-01 05:27:00,423\tWARNING deprecation.py:46 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:27:00 (running for 00:01:55.19)<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:25:05/contrib/MADDPG<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_MADDPG_foodenv_c2dbf_00000</td><td>TERMINATED</td><td>172.17.0.2:26986</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         95.1727</td><td style=\"text-align: right;\">30057</td><td style=\"text-align: right;\">  -18.12</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m 2022-04-01 05:27:00,748\tERROR worker.py:432 -- SystemExit was raised from the worker.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"python/ray/_raylet.pyx\", line 636, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"python/ray/_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"python/ray/_raylet.pyx\", line 589, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 639, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1156, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/actor.py\", line 1235, in exit_actor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"python/ray/_raylet.pyx\", line 770, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"python/ray/_raylet.pyx\", line 591, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 121, in format_exception\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     capture_locals=capture_locals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 363, in extract\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/traceback.py\", line 285, in line\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/linecache.py\", line 137, in updatecache\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     lines = fp.readlines()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 429, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26985)\u001b[0m SystemExit: 1\n",
      "2022-04-01 05:27:00,862\tINFO tune.py:636 -- Total run time: 115.56 seconds (115.17 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "coop_env_config['trainer'] = 'maddpg'\n",
    "comp_env_config['trainer'] = 'maddpg'\n",
    "\n",
    "e = FoodGame(comp_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "\n",
    "# model\n",
    "config[\"actor_hiddens\"] = [256, 256]\n",
    "config[\"actor_hidden_activation\"] = \"tanh\"\n",
    "config[\"critic_hiddens\"] = [256, 256]\n",
    "config[\"critic_hidden_activation\"] = \"tanh\"\n",
    "config[\"learning_starts\"] = 1000 # in terms of samples\n",
    "config[\"critic_lr\"] = 1e-2 # in terms of samples\n",
    "config[\"actor_lr\"] = 1e-2 # in terms of samples\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    \"contrib/MADDPG\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "#     checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72caa7ef",
   "metadata": {},
   "source": [
    "# Results Analysis (Comp + MADDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afce79ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m   \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:342: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m   return layer.apply(inputs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:344: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m   out = tf1.layers.dense(feature, units=1, activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:370: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m   out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/contrib/maddpg/maddpg_policy.py:372: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m   out, units=act_space.shape[0], activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858)\u001b[0m 2022-03-06 05:13:18,842\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-06 05:13:20,740\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-06 05:13:20,798\tINFO trainable.py:473 -- Restored on 172.17.0.2 from checkpoint: /home/ray/cs4246/06_03_2022_05:10:47/contrib/MADDPG/contrib_MADDPG_foodenv_d6889_00000_0_2022-03-06_05-10-47/checkpoint_000020/checkpoint-20\n",
      "2022-03-06 05:13:20,799\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 640, '_time_total': 130.02117204666138, '_episodes_total': 1095}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state at Time 0:\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 1\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '1' '']]\n",
      "\n",
      "Time: 2\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 3\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '0' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 4\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '1' '' '']]\n",
      "\n",
      "Time: 5\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '0' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 6\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '1' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 7\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '1' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 8\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' 'F' '1' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 9\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '0' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 10\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '1' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 11\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 12\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '1' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 13\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 14\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '1' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 15\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '1' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 16\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '0' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '1' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Time: 17\n",
      "[['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '1' '' '' '' '' '']]\n",
      "\n",
      "Time: 18\n",
      "[['' '' '' '' '' '' '0' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '1' '' '' '' '']]\n",
      "\n",
      "Time: 19\n",
      "[['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '' '' '' '']\n",
      " ['' '' '' '' '' '' '1' '' '' '']]\n",
      "\n",
      "Time: 20\n",
      "[['' '' '' '' '' '0' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' 'F' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' 'F' '1' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode Ended\n",
      "Episode Rewards {'agent0': -20, 'agent1': -20}\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.contrib.maddpg as maddpg\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = maddpg.MADDPGTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b06b3",
   "metadata": {},
   "source": [
    "# QMIX Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "46c004af",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_env_config['trainer'] = 'qmix'\n",
    "comp_env_config['trainer'] = 'qmix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bb5aca9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:14:49 (running for 00:00:00.12)<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMixTrainer pid=26724)\u001b[0m 2022-04-01 05:14:53,115\tINFO simple_q.py:154 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(QMixTrainer pid=26724)\u001b[0m 2022-04-01 05:14:53,115\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:14:56 (running for 00:00:07.80)<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMixTrainer pid=26724)\u001b[0m 2022-04-01 05:14:56,678\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26723)\u001b[0m 2022-04-01 05:14:56,651\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26723)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/qmix/qmix_policy.py:455: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811697362/work/torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=26723)\u001b[0m   for k, v in state_dict.items()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 118\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-14-57\n",
      "  done: false\n",
      "  episode_len_mean: 19.666666666666668\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -22.666666666666668\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 118\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 37.676612854003906\n",
      "          loss: 10.890652656555176\n",
      "          q_taken_mean: -1.9169570521304482\n",
      "          target_mean: -0.15125996188113564\n",
      "          td_error_abs: 2.2955468830309416\n",
      "    num_agent_steps_sampled: 118\n",
      "    num_steps_sampled: 118\n",
      "    num_steps_trained: 236\n",
      "    num_steps_trained_this_iter: 38\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.2\n",
      "    ram_util_percent: 52.6\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11898890262892269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08409964938123687\n",
      "    mean_inference_ms: 1.0095003272305016\n",
      "    mean_raw_obs_processing_ms: 0.294797560747932\n",
      "  time_since_restore: 0.5045998096466064\n",
      "  time_this_iter_s: 0.5045998096466064\n",
      "  time_total_s: 0.5045998096466064\n",
      "  timers:\n",
      "    learn_throughput: 868.792\n",
      "    learn_time_ms: 45.274\n",
      "    sample_throughput: 513.614\n",
      "    sample_time_ms: 76.581\n",
      "    update_time_ms: 3.04\n",
      "  timestamp: 1648815297\n",
      "  timesteps_since_restore: 38\n",
      "  timesteps_this_iter: 38\n",
      "  timesteps_total: 118\n",
      "  training_iteration: 1\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:00 (running for 00:00:11.36)<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         2.49261</td><td style=\"text-align: right;\"> 559</td><td style=\"text-align: right;\"> -19.931</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           19.2759</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 1512\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-03\n",
      "  done: false\n",
      "  episode_len_mean: 18.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.0\n",
      "  episode_reward_mean: -17.55\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 80\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 1492\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 6.091006278991699\n",
      "          loss: 1.0168132781982422\n",
      "          q_taken_mean: -2.2065654754638673\n",
      "          target_mean: -2.2836544036865236\n",
      "          td_error_abs: 0.7652877807617188\n",
      "    num_agent_steps_sampled: 1512\n",
      "    num_steps_sampled: 1512\n",
      "    num_steps_trained: 3127\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 13\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.45\n",
      "    ram_util_percent: 52.849999999999994\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1195536372066482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08595131036520318\n",
      "    mean_inference_ms: 1.0227720240872025\n",
      "    mean_raw_obs_processing_ms: 0.2793741536271205\n",
      "  time_since_restore: 6.642300128936768\n",
      "  time_this_iter_s: 1.4168734550476074\n",
      "  time_total_s: 6.642300128936768\n",
      "  timers:\n",
      "    learn_throughput: 848.31\n",
      "    learn_time_ms: 44.913\n",
      "    sample_throughput: 484.135\n",
      "    sample_time_ms: 78.697\n",
      "    update_time_ms: 2.331\n",
      "  timestamp: 1648815303\n",
      "  timesteps_since_restore: 234\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 1512\n",
      "  training_iteration: 6\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:05 (running for 00:00:17.02)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         8.07036</td><td style=\"text-align: right;\">1847</td><td style=\"text-align: right;\"> -17.898</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">           18.8469</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 2618\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-08\n",
      "  done: false\n",
      "  episode_len_mean: 18.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: -18.18\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 139\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 2530\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 19.242727279663086\n",
      "          loss: 4.682595252990723\n",
      "          q_taken_mean: 1.179016399383545\n",
      "          target_mean: 1.6498706817626954\n",
      "          td_error_abs: 1.6385274887084962\n",
      "    num_agent_steps_sampled: 2618\n",
      "    num_steps_sampled: 2618\n",
      "    num_steps_trained: 5453\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 22\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.9\n",
      "    ram_util_percent: 53.0\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12051511579949692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08689066097928397\n",
      "    mean_inference_ms: 1.0302854075519758\n",
      "    mean_raw_obs_processing_ms: 0.278430326765916\n",
      "  time_since_restore: 11.617461919784546\n",
      "  time_this_iter_s: 1.0413219928741455\n",
      "  time_total_s: 11.617461919784546\n",
      "  timers:\n",
      "    learn_throughput: 829.255\n",
      "    learn_time_ms: 47.995\n",
      "    sample_throughput: 451.502\n",
      "    sample_time_ms: 88.15\n",
      "    update_time_ms: 2.517\n",
      "  timestamp: 1648815308\n",
      "  timesteps_since_restore: 394\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 2618\n",
      "  training_iteration: 10\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:11 (running for 00:00:22.73)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         13.6798</td><td style=\"text-align: right;\">3064</td><td style=\"text-align: right;\">  -17.06</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             18.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 3952\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-14\n",
      "  done: false\n",
      "  episode_len_mean: 17.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: -13.26\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 216\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 3952\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 15.08895206451416\n",
      "          loss: 2.8729515075683594\n",
      "          q_taken_mean: 1.2467153549194336\n",
      "          target_mean: 1.651203727722168\n",
      "          td_error_abs: 1.2765398979187013\n",
      "    num_agent_steps_sampled: 3952\n",
      "    num_steps_sampled: 3952\n",
      "    num_steps_trained: 8583\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 35\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.75\n",
      "    ram_util_percent: 53.0\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12266957503020358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08855932879781453\n",
      "    mean_inference_ms: 1.0500881823864732\n",
      "    mean_raw_obs_processing_ms: 0.28231115627426945\n",
      "  time_since_restore: 17.692323446273804\n",
      "  time_this_iter_s: 1.4761104583740234\n",
      "  time_total_s: 17.692323446273804\n",
      "  timers:\n",
      "    learn_throughput: 945.018\n",
      "    learn_time_ms: 42.645\n",
      "    sample_throughput: 534.914\n",
      "    sample_time_ms: 75.339\n",
      "    update_time_ms: 2.973\n",
      "  timestamp: 1648815314\n",
      "  timesteps_since_restore: 599\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 3952\n",
      "  training_iteration: 15\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:17 (running for 00:00:28.38)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         19.2474</td><td style=\"text-align: right;\">4271</td><td style=\"text-align: right;\">   -9.66</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             16.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 5157\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-20\n",
      "  done: false\n",
      "  episode_len_mean: 16.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -5.92\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 293\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 5157\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 26.302133560180664\n",
      "          loss: 5.062967777252197\n",
      "          q_taken_mean: 9.802593994140626\n",
      "          target_mean: 9.133711242675782\n",
      "          td_error_abs: 1.7761575698852539\n",
      "    num_agent_steps_sampled: 5157\n",
      "    num_steps_sampled: 5157\n",
      "    num_steps_trained: 11546\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 46\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.0\n",
      "    ram_util_percent: 53.1\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12227914680456432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0882988247035528\n",
      "    mean_inference_ms: 1.0482664673478763\n",
      "    mean_raw_obs_processing_ms: 0.2821557980519858\n",
      "  time_since_restore: 23.63514733314514\n",
      "  time_this_iter_s: 1.2634453773498535\n",
      "  time_total_s: 23.63514733314514\n",
      "  timers:\n",
      "    learn_throughput: 753.177\n",
      "    learn_time_ms: 50.984\n",
      "    sample_throughput: 457.607\n",
      "    sample_time_ms: 83.915\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1648815320\n",
      "  timesteps_since_restore: 762\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 5157\n",
      "  training_iteration: 19\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:22 (running for 00:00:33.86)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         24.6401</td><td style=\"text-align: right;\">5358</td><td style=\"text-align: right;\">      -5</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">              15.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 6308\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 15.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: -2.5\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 368\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 6208\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 21.795682907104492\n",
      "          loss: 3.997018575668335\n",
      "          q_taken_mean: 8.517296685112846\n",
      "          target_mean: 8.993734571668837\n",
      "          td_error_abs: 1.499956766764323\n",
      "    num_agent_steps_sampled: 6308\n",
      "    num_steps_sampled: 6308\n",
      "    num_steps_trained: 14566\n",
      "    num_steps_trained_this_iter: 36\n",
      "    num_target_updates: 56\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.5\n",
      "    ram_util_percent: 53.2\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12202706494327423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08812401759972895\n",
      "    mean_inference_ms: 1.0470535355927704\n",
      "    mean_raw_obs_processing_ms: 0.28245914091131247\n",
      "  time_since_restore: 29.313836574554443\n",
      "  time_this_iter_s: 1.4673340320587158\n",
      "  time_total_s: 29.313836574554443\n",
      "  timers:\n",
      "    learn_throughput: 860.759\n",
      "    learn_time_ms: 47.516\n",
      "    sample_throughput: 514.431\n",
      "    sample_time_ms: 79.505\n",
      "    update_time_ms: 2.193\n",
      "  timestamp: 1648815326\n",
      "  timesteps_since_restore: 959\n",
      "  timesteps_this_iter: 36\n",
      "  timesteps_total: 6308\n",
      "  training_iteration: 24\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:28 (running for 00:00:39.76)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         30.4504</td><td style=\"text-align: right;\">6514</td><td style=\"text-align: right;\">    -1.6</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">              15.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 7388\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 14.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 0.96\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 444\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 7313\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 26.471118927001953\n",
      "          loss: 4.46091365814209\n",
      "          q_taken_mean: 6.800485992431641\n",
      "          target_mean: 6.2359130859375\n",
      "          td_error_abs: 1.539873218536377\n",
      "    num_agent_steps_sampled: 7388\n",
      "    num_steps_sampled: 7388\n",
      "    num_steps_trained: 17573\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 66\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.85\n",
      "    ram_util_percent: 53.3\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12134674865443676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08785649743849422\n",
      "    mean_inference_ms: 1.0435505932143923\n",
      "    mean_raw_obs_processing_ms: 0.28240220809504973\n",
      "  time_since_restore: 34.938244342803955\n",
      "  time_this_iter_s: 1.2158150672912598\n",
      "  time_total_s: 34.938244342803955\n",
      "  timers:\n",
      "    learn_throughput: 874.991\n",
      "    learn_time_ms: 44.0\n",
      "    sample_throughput: 568.822\n",
      "    sample_time_ms: 67.684\n",
      "    update_time_ms: 2.218\n",
      "  timestamp: 1648815332\n",
      "  timesteps_since_restore: 1163\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 7388\n",
      "  training_iteration: 29\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:34 (running for 00:00:45.62)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         36.2056</td><td style=\"text-align: right;\">7609</td><td style=\"text-align: right;\">    2.78</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             14.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 8464\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 13.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 8.4\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 526\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 8412\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 20.04689598083496\n",
      "          loss: 2.567929744720459\n",
      "          q_taken_mean: 6.935249328613281\n",
      "          target_mean: 6.176515367296007\n",
      "          td_error_abs: 1.228915320502387\n",
      "    num_agent_steps_sampled: 8464\n",
      "    num_steps_sampled: 8464\n",
      "    num_steps_trained: 20806\n",
      "    num_steps_trained_this_iter: 36\n",
      "    num_target_updates: 76\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.85\n",
      "    ram_util_percent: 53.4\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12091954135028418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08767686012945224\n",
      "    mean_inference_ms: 1.04152837145571\n",
      "    mean_raw_obs_processing_ms: 0.2831738015901617\n",
      "  time_since_restore: 40.863831520080566\n",
      "  time_this_iter_s: 1.0508801937103271\n",
      "  time_total_s: 40.863831520080566\n",
      "  timers:\n",
      "    learn_throughput: 873.948\n",
      "    learn_time_ms: 44.854\n",
      "    sample_throughput: 512.115\n",
      "    sample_time_ms: 76.545\n",
      "    update_time_ms: 2.327\n",
      "  timestamp: 1648815338\n",
      "  timesteps_since_restore: 1372\n",
      "  timesteps_this_iter: 36\n",
      "  timesteps_total: 8464\n",
      "  training_iteration: 34\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:39 (running for 00:00:50.70)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         42.1776</td><td style=\"text-align: right;\">8696</td><td style=\"text-align: right;\">    7.94</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             13.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 9552\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-44\n",
      "  done: false\n",
      "  episode_len_mean: 13.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 6.48\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 610\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 9490\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 13.64406681060791\n",
      "          loss: 2.415139675140381\n",
      "          q_taken_mean: 4.395039367675781\n",
      "          target_mean: 4.784038543701172\n",
      "          td_error_abs: 1.1995048522949219\n",
      "    num_agent_steps_sampled: 9552\n",
      "    num_steps_sampled: 9552\n",
      "    num_steps_trained: 23952\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 86\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.5\n",
      "    ram_util_percent: 53.4\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12095702165021936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08768981204961829\n",
      "    mean_inference_ms: 1.0427733932513663\n",
      "    mean_raw_obs_processing_ms: 0.2842819728616426\n",
      "  time_since_restore: 47.00192308425903\n",
      "  time_this_iter_s: 1.3550820350646973\n",
      "  time_total_s: 47.00192308425903\n",
      "  timers:\n",
      "    learn_throughput: 754.205\n",
      "    learn_time_ms: 51.71\n",
      "    sample_throughput: 494.852\n",
      "    sample_time_ms: 78.811\n",
      "    update_time_ms: 2.577\n",
      "  timestamp: 1648815344\n",
      "  timesteps_since_restore: 1571\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 9552\n",
      "  training_iteration: 39\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:45 (running for 00:00:56.62)<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         47.0019</td><td style=\"text-align: right;\">9552</td><td style=\"text-align: right;\">    6.48</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             13.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 10399\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-49\n",
      "  done: false\n",
      "  episode_len_mean: 11.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 10.82\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 682\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 10345\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 14.509479522705078\n",
      "          loss: 2.0883240699768066\n",
      "          q_taken_mean: 8.024468463400137\n",
      "          target_mean: 7.6453990106997285\n",
      "          td_error_abs: 1.13653216154679\n",
      "    num_agent_steps_sampled: 10399\n",
      "    num_steps_sampled: 10399\n",
      "    num_steps_trained: 26621\n",
      "    num_steps_trained_this_iter: 46\n",
      "    num_target_updates: 94\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.05\n",
      "    ram_util_percent: 53.5\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12141608853721991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08795336177431916\n",
      "    mean_inference_ms: 1.0530703458735444\n",
      "    mean_raw_obs_processing_ms: 0.28585444823687267\n",
      "  time_since_restore: 52.3756582736969\n",
      "  time_this_iter_s: 1.4042637348175049\n",
      "  time_total_s: 52.3756582736969\n",
      "  timers:\n",
      "    learn_throughput: 853.639\n",
      "    learn_time_ms: 48.147\n",
      "    sample_throughput: 562.965\n",
      "    sample_time_ms: 73.006\n",
      "    update_time_ms: 2.352\n",
      "  timestamp: 1648815349\n",
      "  timesteps_since_restore: 1729\n",
      "  timesteps_this_iter: 46\n",
      "  timesteps_total: 10399\n",
      "  training_iteration: 43\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:50 (running for 00:01:02.08)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         52.3757</td><td style=\"text-align: right;\">10399</td><td style=\"text-align: right;\">   10.82</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             11.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 11252\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-15-55\n",
      "  done: false\n",
      "  episode_len_mean: 11.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 12.38\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 756\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 11192\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 14.41120433807373\n",
      "          loss: 2.503575563430786\n",
      "          q_taken_mean: 5.074590725368924\n",
      "          target_mean: 5.487166680230034\n",
      "          td_error_abs: 1.2776251051161025\n",
      "    num_agent_steps_sampled: 11252\n",
      "    num_steps_sampled: 11252\n",
      "    num_steps_trained: 29301\n",
      "    num_steps_trained_this_iter: 45\n",
      "    num_target_updates: 102\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.65\n",
      "    ram_util_percent: 53.6\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12176668760680563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08823663365810905\n",
      "    mean_inference_ms: 1.0677324962215193\n",
      "    mean_raw_obs_processing_ms: 0.28727692314487113\n",
      "  time_since_restore: 57.50801467895508\n",
      "  time_this_iter_s: 1.2077176570892334\n",
      "  time_total_s: 57.50801467895508\n",
      "  timers:\n",
      "    learn_throughput: 890.897\n",
      "    learn_time_ms: 45.909\n",
      "    sample_throughput: 533.505\n",
      "    sample_time_ms: 76.663\n",
      "    update_time_ms: 2.454\n",
      "  timestamp: 1648815355\n",
      "  timesteps_since_restore: 1883\n",
      "  timesteps_this_iter: 45\n",
      "  timesteps_total: 11252\n",
      "  training_iteration: 47\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:15:56 (running for 00:01:07.30)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          57.508</td><td style=\"text-align: right;\">11252</td><td style=\"text-align: right;\">   12.38</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             11.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 12119\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 10.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 15.44\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 840\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 12047\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 12.133341789245605\n",
      "          loss: 2.030532121658325\n",
      "          q_taken_mean: 4.134061431884765\n",
      "          target_mean: 4.340554809570312\n",
      "          td_error_abs: 1.1806828498840332\n",
      "    num_agent_steps_sampled: 12119\n",
      "    num_steps_sampled: 12119\n",
      "    num_steps_trained: 32371\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 110\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.8\n",
      "    ram_util_percent: 53.6\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12212039500068762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08852275969986867\n",
      "    mean_inference_ms: 1.0701284643142952\n",
      "    mean_raw_obs_processing_ms: 0.288881355515488\n",
      "  time_since_restore: 63.21075677871704\n",
      "  time_this_iter_s: 1.4126219749450684\n",
      "  time_total_s: 63.21075677871704\n",
      "  timers:\n",
      "    learn_throughput: 816.908\n",
      "    learn_time_ms: 50.189\n",
      "    sample_throughput: 517.403\n",
      "    sample_time_ms: 79.242\n",
      "    update_time_ms: 2.475\n",
      "  timestamp: 1648815360\n",
      "  timesteps_since_restore: 2051\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 12119\n",
      "  training_iteration: 51\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:01 (running for 00:01:13.09)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         63.2108</td><td style=\"text-align: right;\">12119</td><td style=\"text-align: right;\">   15.44</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             10.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 12971\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-06\n",
      "  done: false\n",
      "  episode_len_mean: 10.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 15.02\n",
      "  episode_reward_min: -40.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 919\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 12914\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 22.82637596130371\n",
      "          loss: 3.8069634437561035\n",
      "          q_taken_mean: 7.1233573913574215\n",
      "          target_mean: 6.523442077636719\n",
      "          td_error_abs: 1.2393315315246582\n",
      "    num_agent_steps_sampled: 12971\n",
      "    num_steps_sampled: 12971\n",
      "    num_steps_trained: 35173\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 118\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.4\n",
      "    ram_util_percent: 53.7\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12237487863450319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08874942864297479\n",
      "    mean_inference_ms: 1.0717970422695071\n",
      "    mean_raw_obs_processing_ms: 0.2900848298762452\n",
      "  time_since_restore: 68.48039674758911\n",
      "  time_this_iter_s: 1.325063943862915\n",
      "  time_total_s: 68.48039674758911\n",
      "  timers:\n",
      "    learn_throughput: 741.371\n",
      "    learn_time_ms: 56.382\n",
      "    sample_throughput: 471.169\n",
      "    sample_time_ms: 88.715\n",
      "    update_time_ms: 2.398\n",
      "  timestamp: 1648815366\n",
      "  timesteps_since_restore: 2216\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 12971\n",
      "  training_iteration: 55\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:07 (running for 00:01:18.44)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         68.4804</td><td style=\"text-align: right;\">12971</td><td style=\"text-align: right;\">   15.02</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -40</td><td style=\"text-align: right;\">             10.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 13834\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 10.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 17.32\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1007\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 13779\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 31.711084365844727\n",
      "          loss: 3.842224597930908\n",
      "          q_taken_mean: 4.40436019897461\n",
      "          target_mean: 5.074214935302734\n",
      "          td_error_abs: 1.5944124221801759\n",
      "    num_agent_steps_sampled: 13834\n",
      "    num_steps_sampled: 13834\n",
      "    num_steps_trained: 38213\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 126\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.3\n",
      "    ram_util_percent: 53.8\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12232516116813809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08875363362659755\n",
      "    mean_inference_ms: 1.0712513423342391\n",
      "    mean_raw_obs_processing_ms: 0.29099623932778085\n",
      "  time_since_restore: 73.80778074264526\n",
      "  time_this_iter_s: 1.0937740802764893\n",
      "  time_total_s: 73.80778074264526\n",
      "  timers:\n",
      "    learn_throughput: 975.494\n",
      "    learn_time_ms: 43.158\n",
      "    sample_throughput: 624.209\n",
      "    sample_time_ms: 67.445\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1648815371\n",
      "  timesteps_since_restore: 2365\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 13834\n",
      "  training_iteration: 59\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:12 (running for 00:01:23.85)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         73.8078</td><td style=\"text-align: right;\">13834</td><td style=\"text-align: right;\">   17.32</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">             10.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 14690\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 9.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 19.3\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1100\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 14641\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 22.439298629760742\n",
      "          loss: 2.7357242107391357\n",
      "          q_taken_mean: 6.140370941162109\n",
      "          target_mean: 6.692203521728516\n",
      "          td_error_abs: 1.2784499168395995\n",
      "    num_agent_steps_sampled: 14690\n",
      "    num_steps_sampled: 14690\n",
      "    num_steps_trained: 41317\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 134\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.950000000000003\n",
      "    ram_util_percent: 53.9\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12220541321397356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08869026342913902\n",
      "    mean_inference_ms: 1.0697830765275498\n",
      "    mean_raw_obs_processing_ms: 0.29189380720334834\n",
      "  time_since_restore: 79.5795726776123\n",
      "  time_this_iter_s: 1.451988697052002\n",
      "  time_total_s: 79.5795726776123\n",
      "  timers:\n",
      "    learn_throughput: 837.74\n",
      "    learn_time_ms: 47.151\n",
      "    sample_throughput: 564.178\n",
      "    sample_time_ms: 70.013\n",
      "    update_time_ms: 2.172\n",
      "  timestamp: 1648815377\n",
      "  timesteps_since_restore: 2516\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 14690\n",
      "  training_iteration: 63\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:18 (running for 00:01:29.71)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         79.5796</td><td style=\"text-align: right;\">14690</td><td style=\"text-align: right;\">    19.3</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">              9.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 15522\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-23\n",
      "  done: false\n",
      "  episode_len_mean: 7.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 23.84\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1204\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 15478\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 19.723146438598633\n",
      "          loss: 2.3781943321228027\n",
      "          q_taken_mean: 11.560941808363971\n",
      "          target_mean: 12.204405840705423\n",
      "          td_error_abs: 1.204710231107824\n",
      "    num_agent_steps_sampled: 15522\n",
      "    num_steps_sampled: 15522\n",
      "    num_steps_trained: 44681\n",
      "    num_steps_trained_this_iter: 34\n",
      "    num_target_updates: 142\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.15\n",
      "    ram_util_percent: 53.95\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12233326783880666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08880980154910807\n",
      "    mean_inference_ms: 1.0708523119139728\n",
      "    mean_raw_obs_processing_ms: 0.293580914409234\n",
      "  time_since_restore: 85.74371266365051\n",
      "  time_this_iter_s: 1.3822617530822754\n",
      "  time_total_s: 85.74371266365051\n",
      "  timers:\n",
      "    learn_throughput: 939.508\n",
      "    learn_time_ms: 38.637\n",
      "    sample_throughput: 592.475\n",
      "    sample_time_ms: 61.268\n",
      "    update_time_ms: 2.221\n",
      "  timestamp: 1648815383\n",
      "  timesteps_since_restore: 2671\n",
      "  timesteps_this_iter: 34\n",
      "  timesteps_total: 15522\n",
      "  training_iteration: 67\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:23 (running for 00:01:34.96)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         85.7437</td><td style=\"text-align: right;\">15522</td><td style=\"text-align: right;\">   23.84</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">              7.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:29 (running for 00:01:40.92)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         90.6404</td><td style=\"text-align: right;\">16159</td><td style=\"text-align: right;\">   23.86</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">              7.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 16361\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-30\n",
      "  done: false\n",
      "  episode_len_mean: 7.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 23.56\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1310\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 16324\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 16.057506561279297\n",
      "          loss: 1.4599661827087402\n",
      "          q_taken_mean: 10.563768174913195\n",
      "          target_mean: 10.343276977539062\n",
      "          td_error_abs: 0.9056981404622396\n",
      "    num_agent_steps_sampled: 16361\n",
      "    num_steps_sampled: 16361\n",
      "    num_steps_trained: 48092\n",
      "    num_steps_trained_this_iter: 36\n",
      "    num_target_updates: 150\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.950000000000003\n",
      "    ram_util_percent: 54.0\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12265760354757223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08906999748440886\n",
      "    mean_inference_ms: 1.0735649792158655\n",
      "    mean_raw_obs_processing_ms: 0.29549287461818197\n",
      "  time_since_restore: 92.02149534225464\n",
      "  time_this_iter_s: 1.3811113834381104\n",
      "  time_total_s: 92.02149534225464\n",
      "  timers:\n",
      "    learn_throughput: 871.648\n",
      "    learn_time_ms: 43.94\n",
      "    sample_throughput: 531.225\n",
      "    sample_time_ms: 72.097\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1648815390\n",
      "  timesteps_since_restore: 2825\n",
      "  timesteps_this_iter: 36\n",
      "  timesteps_total: 16361\n",
      "  training_iteration: 71\n",
      "  trial_id: 5371f_00000\n",
      "  \n",
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 17001\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 6.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 26.06\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 1407\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 16948\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 34.938724517822266\n",
      "          loss: 2.9200949668884277\n",
      "          q_taken_mean: 9.773307181693411\n",
      "          target_mean: 10.899491593644425\n",
      "          td_error_abs: 1.3245315551757812\n",
      "    num_agent_steps_sampled: 17001\n",
      "    num_steps_sampled: 17001\n",
      "    num_steps_trained: 51121\n",
      "    num_steps_trained_this_iter: 37\n",
      "    num_target_updates: 156\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.099999999999998\n",
      "    ram_util_percent: 54.03333333333333\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12281764231446585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08918257292355967\n",
      "    mean_inference_ms: 1.0745788692496\n",
      "    mean_raw_obs_processing_ms: 0.29710208375884617\n",
      "  time_since_restore: 97.12881922721863\n",
      "  time_this_iter_s: 1.8782243728637695\n",
      "  time_total_s: 97.12881922721863\n",
      "  timers:\n",
      "    learn_throughput: 814.099\n",
      "    learn_time_ms: 48.643\n",
      "    sample_throughput: 583.487\n",
      "    sample_time_ms: 67.868\n",
      "    update_time_ms: 3.142\n",
      "  timestamp: 1648815395\n",
      "  timesteps_since_restore: 2947\n",
      "  timesteps_this_iter: 37\n",
      "  timesteps_total: 17001\n",
      "  training_iteration: 74\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:35 (running for 00:01:46.51)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         97.1288</td><td style=\"text-align: right;\">17001</td><td style=\"text-align: right;\">   26.06</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">              6.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 17641\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-40\n",
      "  done: false\n",
      "  episode_len_mean: 6.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 27.22\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 1511\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 17584\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 14.343544960021973\n",
      "          loss: 1.4925106763839722\n",
      "          q_taken_mean: 8.486500331333705\n",
      "          target_mean: 8.867338634672619\n",
      "          td_error_abs: 0.956913811819894\n",
      "    num_agent_steps_sampled: 17641\n",
      "    num_steps_sampled: 17641\n",
      "    num_steps_trained: 54147\n",
      "    num_steps_trained_this_iter: 42\n",
      "    num_target_updates: 162\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.8\n",
      "    ram_util_percent: 54.1\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12289437894458487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08920567647078151\n",
      "    mean_inference_ms: 1.0756506243030879\n",
      "    mean_raw_obs_processing_ms: 0.2987476015580237\n",
      "  time_since_restore: 102.31376314163208\n",
      "  time_this_iter_s: 1.6436786651611328\n",
      "  time_total_s: 102.31376314163208\n",
      "  timers:\n",
      "    learn_throughput: 862.561\n",
      "    learn_time_ms: 45.91\n",
      "    sample_throughput: 598.713\n",
      "    sample_time_ms: 66.142\n",
      "    update_time_ms: 2.295\n",
      "  timestamp: 1648815400\n",
      "  timesteps_since_restore: 3061\n",
      "  timesteps_this_iter: 42\n",
      "  timesteps_total: 17641\n",
      "  training_iteration: 77\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:40 (running for 00:01:51.74)<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         102.314</td><td style=\"text-align: right;\">17641</td><td style=\"text-align: right;\">   27.22</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">              6.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:46 (running for 00:01:57.67)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         107.173</td><td style=\"text-align: right;\">18160</td><td style=\"text-align: right;\">   29.34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              5.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 18264\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-46\n",
      "  done: false\n",
      "  episode_len_mean: 5.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 29.16\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1629\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 18218\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 26.16466522216797\n",
      "          loss: 4.510859489440918\n",
      "          q_taken_mean: 7.341932678222657\n",
      "          target_mean: 6.622486877441406\n",
      "          td_error_abs: 1.4319839477539062\n",
      "    num_agent_steps_sampled: 18264\n",
      "    num_steps_sampled: 18264\n",
      "    num_steps_trained: 57540\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 168\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.3\n",
      "    ram_util_percent: 54.2\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12307262276667791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08933085841097001\n",
      "    mean_inference_ms: 1.0777763320087832\n",
      "    mean_raw_obs_processing_ms: 0.300925628825443\n",
      "  time_since_restore: 108.20082545280457\n",
      "  time_this_iter_s: 1.0276503562927246\n",
      "  time_total_s: 108.20082545280457\n",
      "  timers:\n",
      "    learn_throughput: 860.947\n",
      "    learn_time_ms: 46.809\n",
      "    sample_throughput: 576.415\n",
      "    sample_time_ms: 69.915\n",
      "    update_time_ms: 2.654\n",
      "  timestamp: 1648815406\n",
      "  timesteps_since_restore: 3234\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 18264\n",
      "  training_iteration: 81\n",
      "  trial_id: 5371f_00000\n",
      "  \n",
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 18884\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-51\n",
      "  done: false\n",
      "  episode_len_mean: 5.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 28.86\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 1740\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 18847\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 13.057738304138184\n",
      "          loss: 1.3645730018615723\n",
      "          q_taken_mean: 8.373194376627604\n",
      "          target_mean: 8.457450866699219\n",
      "          td_error_abs: 0.8593925899929471\n",
      "    num_agent_steps_sampled: 18884\n",
      "    num_steps_sampled: 18884\n",
      "    num_steps_trained: 60777\n",
      "    num_steps_trained_this_iter: 36\n",
      "    num_target_updates: 174\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.03333333333333\n",
      "    ram_util_percent: 54.20000000000001\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12286787336921817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923102522632974\n",
      "    mean_inference_ms: 1.0766377012314245\n",
      "    mean_raw_obs_processing_ms: 0.3023708946981642\n",
      "  time_since_restore: 113.37590551376343\n",
      "  time_this_iter_s: 1.7432618141174316\n",
      "  time_total_s: 113.37590551376343\n",
      "  timers:\n",
      "    learn_throughput: 824.067\n",
      "    learn_time_ms: 44.05\n",
      "    sample_throughput: 579.48\n",
      "    sample_time_ms: 62.642\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1648815411\n",
      "  timesteps_since_restore: 3364\n",
      "  timesteps_this_iter: 36\n",
      "  timesteps_total: 18884\n",
      "  training_iteration: 84\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:51 (running for 00:02:02.94)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         113.376</td><td style=\"text-align: right;\">18884</td><td style=\"text-align: right;\">   28.86</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">              5.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:16:57 (running for 00:02:08.62)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         117.988</td><td style=\"text-align: right;\">19397</td><td style=\"text-align: right;\">   29.82</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">              5.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 19502\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-16-57\n",
      "  done: false\n",
      "  episode_len_mean: 4.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 30.1\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1862\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 19473\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 9.443055152893066\n",
      "          loss: 1.2100400924682617\n",
      "          q_taken_mean: 9.628464642693014\n",
      "          target_mean: 9.384708180147058\n",
      "          td_error_abs: 0.8583131677964154\n",
      "    num_agent_steps_sampled: 19502\n",
      "    num_steps_sampled: 19502\n",
      "    num_steps_trained: 64198\n",
      "    num_steps_trained_this_iter: 51\n",
      "    num_target_updates: 180\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.950000000000003\n",
      "    ram_util_percent: 54.3\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272071854891642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0891584365180524\n",
      "    mean_inference_ms: 1.0761409669772801\n",
      "    mean_raw_obs_processing_ms: 0.3038245403106826\n",
      "  time_since_restore: 119.1153039932251\n",
      "  time_this_iter_s: 1.1274235248565674\n",
      "  time_total_s: 119.1153039932251\n",
      "  timers:\n",
      "    learn_throughput: 803.802\n",
      "    learn_time_ms: 51.256\n",
      "    sample_throughput: 602.232\n",
      "    sample_time_ms: 68.412\n",
      "    update_time_ms: 2.223\n",
      "  timestamp: 1648815417\n",
      "  timesteps_since_restore: 3519\n",
      "  timesteps_this_iter: 51\n",
      "  timesteps_total: 19502\n",
      "  training_iteration: 88\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:02 (running for 00:02:13.70)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         124.009</td><td style=\"text-align: right;\">20015</td><td style=\"text-align: right;\">    31.1</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              4.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 20115\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 4.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 31.06\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1996\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 20115\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 13.17340087890625\n",
      "          loss: 1.2109262943267822\n",
      "          q_taken_mean: 8.927580043247767\n",
      "          target_mean: 9.20964093889509\n",
      "          td_error_abs: 0.8973127092633929\n",
      "    num_agent_steps_sampled: 20115\n",
      "    num_steps_sampled: 20115\n",
      "    num_steps_trained: 67610\n",
      "    num_steps_trained_this_iter: 35\n",
      "    num_target_updates: 186\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.75\n",
      "    ram_util_percent: 54.4\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12278033906045095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0892071583486911\n",
      "    mean_inference_ms: 1.0773056613427066\n",
      "    mean_raw_obs_processing_ms: 0.30605901905810695\n",
      "  time_since_restore: 125.1738338470459\n",
      "  time_this_iter_s: 1.164846658706665\n",
      "  time_total_s: 125.1738338470459\n",
      "  timers:\n",
      "    learn_throughput: 729.114\n",
      "    learn_time_ms: 53.215\n",
      "    sample_throughput: 515.358\n",
      "    sample_time_ms: 75.287\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1648815423\n",
      "  timesteps_since_restore: 3689\n",
      "  timesteps_this_iter: 35\n",
      "  timesteps_total: 20115\n",
      "  training_iteration: 92\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:07 (running for 00:02:18.85)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         128.099</td><td style=\"text-align: right;\">20420</td><td style=\"text-align: right;\">   30.58</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              4.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 20629\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 4.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 30.5\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2105\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 20534\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 12.280204772949219\n",
      "          loss: 0.8589314222335815\n",
      "          q_taken_mean: 9.103630065917969\n",
      "          target_mean: 9.511116981506348\n",
      "          td_error_abs: 0.7040891647338867\n",
      "    num_agent_steps_sampled: 20629\n",
      "    num_steps_sampled: 20629\n",
      "    num_steps_trained: 70406\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 190\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.400000000000002\n",
      "    ram_util_percent: 54.5\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12293188091561329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08933123610747323\n",
      "    mean_inference_ms: 1.079096408496174\n",
      "    mean_raw_obs_processing_ms: 0.3080484591230856\n",
      "  time_since_restore: 130.1841015815735\n",
      "  time_this_iter_s: 2.0853748321533203\n",
      "  time_total_s: 130.1841015815735\n",
      "  timers:\n",
      "    learn_throughput: 775.208\n",
      "    learn_time_ms: 47.729\n",
      "    sample_throughput: 566.923\n",
      "    sample_time_ms: 65.265\n",
      "    update_time_ms: 2.346\n",
      "  timestamp: 1648815428\n",
      "  timesteps_since_restore: 3814\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 20629\n",
      "  training_iteration: 95\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:12 (running for 00:02:23.92)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">          133.12</td><td style=\"text-align: right;\">20938</td><td style=\"text-align: right;\">    31.7</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">              4.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 21146\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-13\n",
      "  done: false\n",
      "  episode_len_mean: 4.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 31.72\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2226\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 21046\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 9.541802406311035\n",
      "          loss: 0.928259551525116\n",
      "          q_taken_mean: 9.25687717747044\n",
      "          target_mean: 9.338269207928631\n",
      "          td_error_abs: 0.7277887189710462\n",
      "    num_agent_steps_sampled: 21146\n",
      "    num_steps_sampled: 21146\n",
      "    num_steps_trained: 73353\n",
      "    num_steps_trained_this_iter: 37\n",
      "    num_target_updates: 195\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.7\n",
      "    ram_util_percent: 54.5\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12296076904558008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08940010274138115\n",
      "    mean_inference_ms: 1.0798581455053582\n",
      "    mean_raw_obs_processing_ms: 0.3097172801041799\n",
      "  time_since_restore: 135.291748046875\n",
      "  time_this_iter_s: 1.1124141216278076\n",
      "  time_total_s: 135.291748046875\n",
      "  timers:\n",
      "    learn_throughput: 767.146\n",
      "    learn_time_ms: 51.62\n",
      "    sample_throughput: 539.221\n",
      "    sample_time_ms: 73.439\n",
      "    update_time_ms: 2.54\n",
      "  timestamp: 1648815433\n",
      "  timesteps_since_restore: 3960\n",
      "  timesteps_this_iter: 37\n",
      "  timesteps_total: 21146\n",
      "  training_iteration: 99\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMixTrainer pid=26724)\u001b[0m 2022-04-01 05:17:16,112\tWARNING deprecation.py:46 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:18 (running for 00:02:29.28)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">           138.4</td><td style=\"text-align: right;\">21449</td><td style=\"text-align: right;\">      32</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">                 4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 21760\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-20\n",
      "  done: false\n",
      "  episode_len_mean: 4.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 31.06\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 2369\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 21674\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 14.616578102111816\n",
      "          loss: 1.419653058052063\n",
      "          q_taken_mean: 9.31376720610119\n",
      "          target_mean: 9.039560953776041\n",
      "          td_error_abs: 0.9107217334565663\n",
      "    num_agent_steps_sampled: 21760\n",
      "    num_steps_sampled: 21760\n",
      "    num_steps_trained: 76820\n",
      "    num_steps_trained_this_iter: 42\n",
      "    num_target_updates: 201\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.266666666666666\n",
      "    ram_util_percent: 54.6\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12312182776715477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0895162195016297\n",
      "    mean_inference_ms: 1.081614427043519\n",
      "    mean_raw_obs_processing_ms: 0.31212512407957177\n",
      "  time_since_restore: 141.31041836738586\n",
      "  time_this_iter_s: 1.768479585647583\n",
      "  time_total_s: 141.31041836738586\n",
      "  timers:\n",
      "    learn_throughput: 869.946\n",
      "    learn_time_ms: 46.095\n",
      "    sample_throughput: 611.792\n",
      "    sample_time_ms: 65.545\n",
      "    update_time_ms: 2.481\n",
      "  timestamp: 1648815440\n",
      "  timesteps_since_restore: 4140\n",
      "  timesteps_this_iter: 42\n",
      "  timesteps_total: 21760\n",
      "  training_iteration: 103\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:23 (running for 00:02:34.43)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         143.472</td><td style=\"text-align: right;\">21968</td><td style=\"text-align: right;\">   31.14</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">              4.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 22276\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 3.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 32.52\n",
      "  episode_reward_min: 24.0\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2503\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 22202\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 16.694255828857422\n",
      "          loss: 1.7530947923660278\n",
      "          q_taken_mean: 9.015603807237413\n",
      "          target_mean: 8.86401621500651\n",
      "          td_error_abs: 1.0679110421074762\n",
      "    num_agent_steps_sampled: 22276\n",
      "    num_steps_sampled: 22276\n",
      "    num_steps_trained: 80041\n",
      "    num_steps_trained_this_iter: 36\n",
      "    num_target_updates: 206\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.700000000000003\n",
      "    ram_util_percent: 54.6\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12322184183764023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08960204379918046\n",
      "    mean_inference_ms: 1.082919603405635\n",
      "    mean_raw_obs_processing_ms: 0.3142278549623839\n",
      "  time_since_restore: 146.93342304229736\n",
      "  time_this_iter_s: 1.2045094966888428\n",
      "  time_total_s: 146.93342304229736\n",
      "  timers:\n",
      "    learn_throughput: 801.124\n",
      "    learn_time_ms: 48.557\n",
      "    sample_throughput: 570.116\n",
      "    sample_time_ms: 68.232\n",
      "    update_time_ms: 2.436\n",
      "  timestamp: 1648815445\n",
      "  timesteps_since_restore: 4332\n",
      "  timesteps_this_iter: 36\n",
      "  timesteps_total: 22276\n",
      "  training_iteration: 108\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:28 (running for 00:02:40.03)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         149.985</td><td style=\"text-align: right;\">22584</td><td style=\"text-align: right;\">   32.94</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">              3.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 22787\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-31\n",
      "  done: false\n",
      "  episode_len_mean: 3.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 32.88\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2646\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 22730\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 24.606687545776367\n",
      "          loss: 4.224145412445068\n",
      "          q_taken_mean: 11.769339537009214\n",
      "          target_mean: 11.389540452223558\n",
      "          td_error_abs: 1.0418500655736678\n",
      "    num_agent_steps_sampled: 22787\n",
      "    num_steps_sampled: 22787\n",
      "    num_steps_trained: 83151\n",
      "    num_steps_trained_this_iter: 39\n",
      "    num_target_updates: 211\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.700000000000003\n",
      "    ram_util_percent: 54.7\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12338000474492794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08973412034366757\n",
      "    mean_inference_ms: 1.0847732491843363\n",
      "    mean_raw_obs_processing_ms: 0.3167229305660298\n",
      "  time_since_restore: 152.25917100906372\n",
      "  time_this_iter_s: 1.1465771198272705\n",
      "  time_total_s: 152.25917100906372\n",
      "  timers:\n",
      "    learn_throughput: 810.931\n",
      "    learn_time_ms: 49.943\n",
      "    sample_throughput: 573.667\n",
      "    sample_time_ms: 70.599\n",
      "    update_time_ms: 2.633\n",
      "  timestamp: 1648815451\n",
      "  timesteps_since_restore: 4487\n",
      "  timesteps_this_iter: 39\n",
      "  timesteps_total: 22787\n",
      "  training_iteration: 112\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:34 (running for 00:02:45.39)<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         154.275</td><td style=\"text-align: right;\">22992</td><td style=\"text-align: right;\">   32.72</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">              3.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 23301\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-36\n",
      "  done: false\n",
      "  episode_len_mean: 3.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.2\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 2794\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 23248\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 7.695291042327881\n",
      "          loss: 0.7724437713623047\n",
      "          q_taken_mean: 10.666925048828125\n",
      "          target_mean: 10.58785400390625\n",
      "          td_error_abs: 0.6321249961853027\n",
      "    num_agent_steps_sampled: 23301\n",
      "    num_steps_sampled: 23301\n",
      "    num_steps_trained: 86242\n",
      "    num_steps_trained_this_iter: 40\n",
      "    num_target_updates: 216\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.799999999999997\n",
      "    ram_util_percent: 54.7\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12358311207095457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08984329249111239\n",
      "    mean_inference_ms: 1.0870170500062653\n",
      "    mean_raw_obs_processing_ms: 0.3190436935479374\n",
      "  time_since_restore: 157.63710570335388\n",
      "  time_this_iter_s: 0.9963173866271973\n",
      "  time_total_s: 157.63710570335388\n",
      "  timers:\n",
      "    learn_throughput: 922.518\n",
      "    learn_time_ms: 40.758\n",
      "    sample_throughput: 626.0\n",
      "    sample_time_ms: 60.064\n",
      "    update_time_ms: 2.214\n",
      "  timestamp: 1648815456\n",
      "  timesteps_since_restore: 4670\n",
      "  timesteps_this_iter: 40\n",
      "  timesteps_total: 23301\n",
      "  training_iteration: 117\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:39 (running for 00:02:50.64)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         160.437</td><td style=\"text-align: right;\">23613</td><td style=\"text-align: right;\">   33.18</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">              3.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 23923\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-42\n",
      "  done: false\n",
      "  episode_len_mean: 3.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.42\n",
      "  episode_reward_min: 24.0\n",
      "  episodes_this_iter: 62\n",
      "  episodes_total: 2979\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 23868\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 23.289308547973633\n",
      "          loss: 2.72460675239563\n",
      "          q_taken_mean: 7.788140296936035\n",
      "          target_mean: 7.172396659851074\n",
      "          td_error_abs: 1.328223466873169\n",
      "    num_agent_steps_sampled: 23923\n",
      "    num_steps_sampled: 23923\n",
      "    num_steps_trained: 90042\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 222\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.4\n",
      "    ram_util_percent: 48.5\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12322795502178334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08963850858000129\n",
      "    mean_inference_ms: 1.0849698202038225\n",
      "    mean_raw_obs_processing_ms: 0.321011127303017\n",
      "  time_since_restore: 163.41019248962402\n",
      "  time_this_iter_s: 1.9796326160430908\n",
      "  time_total_s: 163.41019248962402\n",
      "  timers:\n",
      "    learn_throughput: 848.424\n",
      "    learn_time_ms: 42.785\n",
      "    sample_throughput: 607.842\n",
      "    sample_time_ms: 59.719\n",
      "    update_time_ms: 2.201\n",
      "  timestamp: 1648815462\n",
      "  timesteps_since_restore: 4811\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 23923\n",
      "  training_iteration: 121\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:44 (running for 00:02:55.73)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         165.457</td><td style=\"text-align: right;\">24133</td><td style=\"text-align: right;\">   33.48</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">              3.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 24541\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-48\n",
      "  done: false\n",
      "  episode_len_mean: 3.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.9\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 3178\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 24487\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 11.861846923828125\n",
      "          loss: 1.2316670417785645\n",
      "          q_taken_mean: 8.936837332589286\n",
      "          target_mean: 9.080030168805804\n",
      "          td_error_abs: 0.9232434953962053\n",
      "    num_agent_steps_sampled: 24541\n",
      "    num_steps_sampled: 24541\n",
      "    num_steps_trained: 93961\n",
      "    num_steps_trained_this_iter: 35\n",
      "    num_target_updates: 228\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.200000000000001\n",
      "    ram_util_percent: 48.6\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12290926799772475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08945227347872912\n",
      "    mean_inference_ms: 1.0831120196607824\n",
      "    mean_raw_obs_processing_ms: 0.3230114039065085\n",
      "  time_since_restore: 169.4136426448822\n",
      "  time_this_iter_s: 1.8981950283050537\n",
      "  time_total_s: 169.4136426448822\n",
      "  timers:\n",
      "    learn_throughput: 967.285\n",
      "    learn_time_ms: 40.629\n",
      "    sample_throughput: 690.063\n",
      "    sample_time_ms: 56.951\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1648815468\n",
      "  timesteps_since_restore: 4986\n",
      "  timesteps_this_iter: 35\n",
      "  timesteps_total: 24541\n",
      "  training_iteration: 126\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:49 (running for 00:03:00.75)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         170.412</td><td style=\"text-align: right;\">24643</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 25155\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 3.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 33.96\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 3381\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 25101\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 16.98355484008789\n",
      "          loss: 2.1304714679718018\n",
      "          q_taken_mean: 6.199623919547872\n",
      "          target_mean: 6.531167537608045\n",
      "          td_error_abs: 1.0865519097510805\n",
      "    num_agent_steps_sampled: 25155\n",
      "    num_steps_sampled: 25155\n",
      "    num_steps_trained: 97906\n",
      "    num_steps_trained_this_iter: 47\n",
      "    num_target_updates: 234\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.5\n",
      "    ram_util_percent: 48.6\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12244349831871791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0891595476663733\n",
      "    mean_inference_ms: 1.07992785164595\n",
      "    mean_raw_obs_processing_ms: 0.3247006569660213\n",
      "  time_since_restore: 175.2864420413971\n",
      "  time_this_iter_s: 1.062861442565918\n",
      "  time_total_s: 175.2864420413971\n",
      "  timers:\n",
      "    learn_throughput: 783.419\n",
      "    learn_time_ms: 48.505\n",
      "    sample_throughput: 573.178\n",
      "    sample_time_ms: 66.297\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1648815474\n",
      "  timesteps_since_restore: 5157\n",
      "  timesteps_this_iter: 47\n",
      "  timesteps_total: 25155\n",
      "  training_iteration: 130\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:17:55 (running for 00:03:06.68)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         175.286</td><td style=\"text-align: right;\">25155</td><td style=\"text-align: right;\">   33.96</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">              3.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 25665\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-17-59\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 3551\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 25611\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 6.21291971206665\n",
      "          loss: 0.5799204707145691\n",
      "          q_taken_mean: 11.448091052827381\n",
      "          target_mean: 11.515809558686756\n",
      "          td_error_abs: 0.5690237681070963\n",
      "    num_agent_steps_sampled: 25665\n",
      "    num_steps_sampled: 25665\n",
      "    num_steps_trained: 101177\n",
      "    num_steps_trained_this_iter: 42\n",
      "    num_target_updates: 239\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.6\n",
      "    ram_util_percent: 48.7\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12238379291200291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08913181739816377\n",
      "    mean_inference_ms: 1.0800565184184387\n",
      "    mean_raw_obs_processing_ms: 0.3266848916171953\n",
      "  time_since_restore: 180.50854420661926\n",
      "  time_this_iter_s: 1.0646843910217285\n",
      "  time_total_s: 180.50854420661926\n",
      "  timers:\n",
      "    learn_throughput: 952.448\n",
      "    learn_time_ms: 46.407\n",
      "    sample_throughput: 684.87\n",
      "    sample_time_ms: 64.538\n",
      "    update_time_ms: 2.109\n",
      "  timestamp: 1648815479\n",
      "  timesteps_since_restore: 5310\n",
      "  timesteps_this_iter: 42\n",
      "  timesteps_total: 25665\n",
      "  training_iteration: 134\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:00 (running for 00:03:11.98)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         180.509</td><td style=\"text-align: right;\">25665</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 26277\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-05\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 3755\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 26223\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 38.28487777709961\n",
      "          loss: 1.0348012447357178\n",
      "          q_taken_mean: 12.858234405517578\n",
      "          target_mean: 13.833185195922852\n",
      "          td_error_abs: 0.9793069958686829\n",
      "    num_agent_steps_sampled: 26277\n",
      "    num_steps_sampled: 26277\n",
      "    num_steps_trained: 105063\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 245\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.933333333333332\n",
      "    ram_util_percent: 48.70000000000001\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12201900892003191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0888987660085685\n",
      "    mean_inference_ms: 1.0778771489279857\n",
      "    mean_raw_obs_processing_ms: 0.3283188919397669\n",
      "  time_since_restore: 186.29257988929749\n",
      "  time_this_iter_s: 1.884880781173706\n",
      "  time_total_s: 186.29257988929749\n",
      "  timers:\n",
      "    learn_throughput: 996.897\n",
      "    learn_time_ms: 38.118\n",
      "    sample_throughput: 657.865\n",
      "    sample_time_ms: 57.763\n",
      "    update_time_ms: 2.121\n",
      "  timestamp: 1648815485\n",
      "  timesteps_since_restore: 5428\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 26277\n",
      "  training_iteration: 137\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:06 (running for 00:03:17.81)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         186.293</td><td style=\"text-align: right;\">26277</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 26991\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-12\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 3993\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 26937\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 26.726844787597656\n",
      "          loss: 2.6935086250305176\n",
      "          q_taken_mean: 9.050679206848145\n",
      "          target_mean: 8.311569213867188\n",
      "          td_error_abs: 1.1577026844024658\n",
      "    num_agent_steps_sampled: 26991\n",
      "    num_steps_sampled: 26991\n",
      "    num_steps_trained: 109464\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 252\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.65\n",
      "    ram_util_percent: 48.7\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12160143939531616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08865194830443805\n",
      "    mean_inference_ms: 1.0751417108536088\n",
      "    mean_raw_obs_processing_ms: 0.33016114415319725\n",
      "  time_since_restore: 192.8634750843048\n",
      "  time_this_iter_s: 1.818816900253296\n",
      "  time_total_s: 192.8634750843048\n",
      "  timers:\n",
      "    learn_throughput: 925.706\n",
      "    learn_time_ms: 37.593\n",
      "    sample_throughput: 661.155\n",
      "    sample_time_ms: 52.635\n",
      "    update_time_ms: 2.052\n",
      "  timestamp: 1648815492\n",
      "  timesteps_since_restore: 5591\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 26991\n",
      "  training_iteration: 141\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:12 (running for 00:03:23.45)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         192.863</td><td style=\"text-align: right;\">26991</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:18 (running for 00:03:29.42)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         197.774</td><td style=\"text-align: right;\">27501</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 27705\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-19\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 4231\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 27651\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 13.317123413085938\n",
      "          loss: 0.8504083156585693\n",
      "          q_taken_mean: 12.401097153172348\n",
      "          target_mean: 12.223786325165719\n",
      "          td_error_abs: 0.6288442900686553\n",
      "    num_agent_steps_sampled: 27705\n",
      "    num_steps_sampled: 27705\n",
      "    num_steps_trained: 113999\n",
      "    num_steps_trained_this_iter: 33\n",
      "    num_target_updates: 259\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.5\n",
      "    ram_util_percent: 48.79999999999999\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12129395126421069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08847494334525319\n",
      "    mean_inference_ms: 1.0733935244941941\n",
      "    mean_raw_obs_processing_ms: 0.33206715210573245\n",
      "  time_since_restore: 199.75120854377747\n",
      "  time_this_iter_s: 1.9771277904510498\n",
      "  time_total_s: 199.75120854377747\n",
      "  timers:\n",
      "    learn_throughput: 922.446\n",
      "    learn_time_ms: 41.086\n",
      "    sample_throughput: 630.756\n",
      "    sample_time_ms: 60.087\n",
      "    update_time_ms: 2.164\n",
      "  timestamp: 1648815499\n",
      "  timesteps_since_restore: 5779\n",
      "  timesteps_this_iter: 33\n",
      "  timesteps_total: 27705\n",
      "  training_iteration: 146\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:23 (running for 00:03:34.61)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         202.886</td><td style=\"text-align: right;\">28011</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 28215\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 4401\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 28161\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 12.745993614196777\n",
      "          loss: 1.1133652925491333\n",
      "          q_taken_mean: 7.323152715509588\n",
      "          target_mean: 7.683316317471591\n",
      "          td_error_abs: 0.7976856231689453\n",
      "    num_agent_steps_sampled: 28215\n",
      "    num_steps_sampled: 28215\n",
      "    num_steps_trained: 117194\n",
      "    num_steps_trained_this_iter: 44\n",
      "    num_target_updates: 264\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.433333333333332\n",
      "    ram_util_percent: 48.79999999999999\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12114056098236478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08839226058601936\n",
      "    mean_inference_ms: 1.0729211719194787\n",
      "    mean_raw_obs_processing_ms: 0.33354171258280785\n",
      "  time_since_restore: 204.76458406448364\n",
      "  time_this_iter_s: 1.8786725997924805\n",
      "  time_total_s: 204.76458406448364\n",
      "  timers:\n",
      "    learn_throughput: 980.576\n",
      "    learn_time_ms: 40.69\n",
      "    sample_throughput: 710.949\n",
      "    sample_time_ms: 56.122\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1648815504\n",
      "  timesteps_since_restore: 5936\n",
      "  timesteps_this_iter: 44\n",
      "  timesteps_total: 28215\n",
      "  training_iteration: 150\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:28 (running for 00:03:39.72)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         208.911</td><td style=\"text-align: right;\">28623</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 28725\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 4571\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 28671\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 16.859647750854492\n",
      "          loss: 0.932975709438324\n",
      "          q_taken_mean: 11.696784019470215\n",
      "          target_mean: 12.15079116821289\n",
      "          td_error_abs: 0.6500687599182129\n",
      "    num_agent_steps_sampled: 28725\n",
      "    num_steps_sampled: 28725\n",
      "    num_steps_trained: 120398\n",
      "    num_steps_trained_this_iter: 32\n",
      "    num_target_updates: 269\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.5\n",
      "    ram_util_percent: 48.8\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12105521515894725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08833995681174021\n",
      "    mean_inference_ms: 1.0728074144650415\n",
      "    mean_raw_obs_processing_ms: 0.3349913883433904\n",
      "  time_since_restore: 209.9624261856079\n",
      "  time_this_iter_s: 1.051191806793213\n",
      "  time_total_s: 209.9624261856079\n",
      "  timers:\n",
      "    learn_throughput: 869.356\n",
      "    learn_time_ms: 41.295\n",
      "    sample_throughput: 619.272\n",
      "    sample_time_ms: 57.971\n",
      "    update_time_ms: 2.26\n",
      "  timestamp: 1648815509\n",
      "  timesteps_since_restore: 6119\n",
      "  timesteps_this_iter: 32\n",
      "  timesteps_total: 28725\n",
      "  training_iteration: 155\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:33 (running for 00:03:44.82)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         212.956</td><td style=\"text-align: right;\">29031</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 29337\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 4775\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29283\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 30.223304748535156\n",
      "          loss: 3.38950514793396\n",
      "          q_taken_mean: 10.827490604285037\n",
      "          target_mean: 11.640173709753787\n",
      "          td_error_abs: 1.3852162216648911\n",
      "    num_agent_steps_sampled: 29337\n",
      "    num_steps_sampled: 29337\n",
      "    num_steps_trained: 124167\n",
      "    num_steps_trained_this_iter: 33\n",
      "    num_target_updates: 275\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.833333333333334\n",
      "    ram_util_percent: 48.833333333333336\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1209519707887017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0882891677884934\n",
      "    mean_inference_ms: 1.0725152675980751\n",
      "    mean_raw_obs_processing_ms: 0.33699626284009676\n",
      "  time_since_restore: 215.89040088653564\n",
      "  time_this_iter_s: 1.8531441688537598\n",
      "  time_total_s: 215.89040088653564\n",
      "  timers:\n",
      "    learn_throughput: 884.343\n",
      "    learn_time_ms: 41.274\n",
      "    sample_throughput: 641.532\n",
      "    sample_time_ms: 56.895\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1648815515\n",
      "  timesteps_since_restore: 6284\n",
      "  timesteps_this_iter: 33\n",
      "  timesteps_total: 29337\n",
      "  training_iteration: 159\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:39 (running for 00:03:50.74)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>RUNNING </td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         218.812</td><td style=\"text-align: right;\">29643</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 29949\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 4979\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 29895\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 14.164641380310059\n",
      "          loss: 0.9975647926330566\n",
      "          q_taken_mean: 12.541200183686756\n",
      "          target_mean: 12.88115728469122\n",
      "          td_error_abs: 0.8174606504894438\n",
      "    num_agent_steps_sampled: 29949\n",
      "    num_steps_sampled: 29949\n",
      "    num_steps_trained: 128027\n",
      "    num_steps_trained_this_iter: 42\n",
      "    num_target_updates: 281\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2\n",
      "    ram_util_percent: 48.9\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12063530468165919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08811343949242995\n",
      "    mean_inference_ms: 1.070397459334658\n",
      "    mean_raw_obs_processing_ms: 0.33825936517946964\n",
      "  time_since_restore: 221.76011633872986\n",
      "  time_this_iter_s: 1.050948143005371\n",
      "  time_total_s: 221.76011633872986\n",
      "  timers:\n",
      "    learn_throughput: 838.399\n",
      "    learn_time_ms: 47.471\n",
      "    sample_throughput: 616.207\n",
      "    sample_time_ms: 64.589\n",
      "    update_time_ms: 2.212\n",
      "  timestamp: 1648815521\n",
      "  timesteps_since_restore: 6433\n",
      "  timesteps_this_iter: 42\n",
      "  timesteps_total: 29949\n",
      "  training_iteration: 163\n",
      "  trial_id: 5371f_00000\n",
      "  \n",
      "Result for QMIX_grouped_foodenv_5371f_00000:\n",
      "  agent_timesteps_total: 30153\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-01_05-18-43\n",
      "  done: true\n",
      "  episode_len_mean: 3.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 34.0\n",
      "  episode_reward_mean: 34.0\n",
      "  episode_reward_min: 34.0\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 5047\n",
      "  experiment_id: d11fbc22d1c74b2682ad267be9ec3218\n",
      "  hostname: 1d29a0c222c3\n",
      "  info:\n",
      "    last_target_update_ts: 30099\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          grad_norm: 22.495298385620117\n",
      "          loss: 0.7996619343757629\n",
      "          q_taken_mean: 12.66354709201389\n",
      "          target_mean: 12.116841634114584\n",
      "          td_error_abs: 0.7123346328735352\n",
      "    num_agent_steps_sampled: 30153\n",
      "    num_steps_sampled: 30153\n",
      "    num_steps_trained: 129297\n",
      "    num_steps_trained_this_iter: 36\n",
      "    num_target_updates: 283\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.17.0.2\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.966666666666667\n",
      "    ram_util_percent: 48.9\n",
      "  pid: 26724\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12054797050213212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.088059782228582\n",
      "    mean_inference_ms: 1.0699212759999452\n",
      "    mean_raw_obs_processing_ms: 0.33868891716419364\n",
      "  time_since_restore: 223.680490732193\n",
      "  time_this_iter_s: 1.9203743934631348\n",
      "  time_total_s: 223.680490732193\n",
      "  timers:\n",
      "    learn_throughput: 960.567\n",
      "    learn_time_ms: 39.664\n",
      "    sample_throughput: 675.2\n",
      "    sample_time_ms: 56.428\n",
      "    update_time_ms: 2.114\n",
      "  timestamp: 1648815523\n",
      "  timesteps_since_restore: 6469\n",
      "  timesteps_this_iter: 36\n",
      "  timesteps_total: 30153\n",
      "  training_iteration: 164\n",
      "  trial_id: 5371f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:18:43 (running for 00:03:54.68)<br>Memory usage on this node: 7.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:14:48/QMIX<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_5371f_00000</td><td>TERMINATED</td><td>172.17.0.2:26724</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">          223.68</td><td style=\"text-align: right;\">30153</td><td style=\"text-align: right;\">      34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                  34</td><td style=\"text-align: right;\">                 3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 05:18:44,048\tINFO tune.py:636 -- Total run time: 235.17 seconds (234.67 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "e = FoodGame(coop_env_config)\n",
    "tuple_obs_space = Tuple([e.observation_space for i in range(len(e.agents))])\n",
    "tuple_act_space = Tuple([e.action_space for i in range(len(e.agents))])\n",
    "\n",
    "register_env(\"grouped_foodenv\", lambda config: FoodGame(config).with_agent_groups(\n",
    "            groups={\"agents\": e.agent_names},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        ))\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"env\"] = \"grouped_foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-3\n",
    "config[\"optim_alpha\"] = 0.99\n",
    "config[\"optim_eps\"] = 0.00001\n",
    "config[\"grad_norm_clipping\"] = 10\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "config[\"model\"]={\n",
    "        \"lstm_cell_size\": 256,\n",
    "        \"max_seq_len\": 20,\n",
    "    }\n",
    "\n",
    "config[\"mixing_embed_dim\"] = 256\n",
    "# eval\n",
    "# config[\"evaluation_interval\"] = 1\n",
    "# config[\"evaluation_duration\"] = 1\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "analysis = ray.tune.run(\n",
    "    \"QMIX\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2223f",
   "metadata": {},
   "source": [
    "# Results Analysis (QMIX + Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27b356e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 05:20:59,817\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-06 05:20:59,824\tINFO trainable.py:473 -- Restored on 172.17.0.2 from checkpoint: /home/ray/cs4246/06_03_2022_05:19:09/QMIX/QMIX_grouped_foodenv_01f18_00000_0_2022-03-06_05-19-09/checkpoint_000020/checkpoint-20\n",
      "2022-03-06 05:20:59,825\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 818, '_time_total': 64.86543583869934, '_episodes_total': 2417}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2256)\u001b[0m 2022-03-06 05:20:59,804\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2256)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/qmix/qmix_policy.py:455: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811697362/work/torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2256)\u001b[0m   for k, v in state_dict.items()\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.qmix as qmix\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = qmix.QMixTrainer(config=config, env=\"grouped_foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "\n",
    "grouped_e = FoodGame(coop_env_config).with_agent_groups(\n",
    "            groups={\"agents\": FoodGame(coop_env_config).agent_names},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "\n",
    "# episode_reward = {'agent'+str(i):0 for i in range(len(grouped_e.agents))}\n",
    "# done = False\n",
    "# obs = grouped_e.reset()\n",
    "# print(\"Initial state at Time 0:\")\n",
    "# grouped_e.render()\n",
    "# time = 1\n",
    "# while not done:\n",
    "#     print(\"Time:\",time)\n",
    "#     time += 1\n",
    "#     action = trainer.compute_actions(obs,policy_id='default_policy')\n",
    "#     for i in range(1,len(grouped_e.agents)):\n",
    "#         a = trainer.compute_actions(obs,policy_id='default_policy')\n",
    "#         action['agent'+str(i)] = a['agent'+str(i)]\n",
    "#     obs, reward, dones, info = grouped_e.step(action)\n",
    "#     done = dones['__all__']\n",
    "#     grouped_e.render()\n",
    "#     for i in range(len(grouped_e.agents)):\n",
    "#         episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "# print(\"Episode Ended\")\n",
    "# print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981664b",
   "metadata": {},
   "source": [
    "# QMIX Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e7e6b821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:37:56 (running for 00:00:00.13)<br>Memory usage on this node: 6.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:37:56/QMIX<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_8e6da_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMixTrainer pid=27216)\u001b[0m 2022-04-01 05:38:00,381\tINFO simple_q.py:154 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(QMixTrainer pid=27216)\u001b[0m 2022-04-01 05:38:00,381\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:38:03 (running for 00:00:07.34)<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:37:56/QMIX<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_8e6da_00000</td><td>RUNNING </td><td>172.17.0.2:27216</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 05:38:03,688\tERROR trial_runner.py:927 -- Trial QMIX_grouped_foodenv_8e6da_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 893, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 707, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1733, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::QMixTrainer.train()\u001b[39m (pid=27216, ip=172.17.0.2, repr=QMixTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 315, in train\n",
      "    result = self.step()\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 977, in step\n",
      "    raise e\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 963, in step\n",
      "    step_attempt_results = self.step_attempt()\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 1042, in step_attempt\n",
      "    step_results = self._exec_plan_or_training_iteration_fn()\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 1962, in _exec_plan_or_training_iteration_fn\n",
      "    results = next(self.train_exec_impl)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1075, in build_union\n",
      "    item = next(it)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "    yield ray.get(futures, timeout=timeout)\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=27215, ip=172.17.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f95fa4ff990>)\n",
      "ValueError: The two structures don't have the same nested structure.\n",
      "\n",
      "First structure: type=list str=[array([4, 4, 6, 7, 4, 4, 6, 6])]\n",
      "\n",
      "Second structure: type=tuple str=(array([1, 9, 5, 9, 2, 7, 2, 7]), array([5, 4, 2, 4, 1, 7, 3, 5]))\n",
      "\n",
      "More specifically: The two structures don't have the same number of elements. First structure: type=list str=[array([4, 4, 6, 7, 4, 4, 6, 6])]. Second structure: type=tuple str=(array([1, 9, 5, 9, 2, 7, 2, 7]), array([5, 4, 2, 4, 1, 7, 3, 5]))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=27215, ip=172.17.0.2, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f95fa4ff990>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "    return next(self.local_it)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 382, in gen_rollouts\n",
      "    yield self.sample()\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 761, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 104, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 266, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 648, in _env_runner\n",
      "    sample_collector=sample_collector,\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 852, in _process_observations\n",
      "    prep_obs = preprocessor.transform(raw_obs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py\", line 236, in transform\n",
      "    self.check_shape(observation)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py\", line 69, in check_shape\n",
      "    observation, self._obs_for_type_matching)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/spaces/space_utils.py\", line 321, in convert_element_to_space_type\n",
      "    map_, element, sampled_element, check_types=False)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/tree/__init__.py\", line 507, in map_structure\n",
      "    assert_same_structure(structures[0], other, check_types=check_types)\n",
      "  File \"/home/ray/anaconda3/lib/python3.7/site-packages/tree/__init__.py\", line 366, in assert_same_structure\n",
      "    % (e, str1, str2))\n",
      "ValueError: The two structures don't have the same nested structure.\n",
      "\n",
      "First structure: type=list str=[array([4, 4, 6, 7, 4, 4, 6, 6])]\n",
      "\n",
      "Second structure: type=tuple str=(array([1, 9, 5, 9, 2, 7, 2, 7]), array([5, 4, 2, 4, 1, 7, 3, 5]))\n",
      "\n",
      "More specifically: The two structures don't have the same number of elements. First structure: type=list str=[array([4, 4, 6, 7, 4, 4, 6, 6])]. Second structure: type=tuple str=(array([1, 9, 5, 9, 2, 7, 2, 7]), array([5, 4, 2, 4, 1, 7, 3, 5]))\n",
      "Entire first structure:\n",
      "[.]\n",
      "Entire second structure:\n",
      "(., .)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for QMIX_grouped_foodenv_8e6da_00000:\n",
      "  date: 2022-04-01_05-38-03\n",
      "  experiment_id: adc7f78f897e493d94465bc561c0a724\n",
      "  hostname: 1d29a0c222c3\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 27216\n",
      "  timestamp: 1648816683\n",
      "  trial_id: 8e6da_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-01 05:38:03 (running for 00:00:07.36)<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/7.96 GiB heap, 0.0/3.98 GiB objects<br>Result logdir: /home/ray/cs4246/01_04_2022_05:37:56/QMIX<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_8e6da_00000</td><td>ERROR   </td><td>172.17.0.2:27216</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>QMIX_grouped_foodenv_8e6da_00000</td><td style=\"text-align: right;\">           1</td><td>/home/ray/cs4246/01_04_2022_05:37:56/QMIX/QMIX_grouped_foodenv_8e6da_00000_0_2022-04-01_05-37-56/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(QMixTrainer pid=27216)\u001b[0m 2022-04-01 05:38:03,670\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(QMixTrainer pid=27216)\u001b[0m 2022-04-01 05:38:03,685\tWARNING trainer.py:974 -- Worker crashed during call to `step_attempt()`. To try to continue training without the failed worker, set `ignore_worker_failures=True`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27215)\u001b[0m 2022-04-01 05:38:03,655\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27215)\u001b[0m /home/ray/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/qmix/qmix_policy.py:455: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811697362/work/torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27215)\u001b[0m   for k, v in state_dict.items()\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [QMIX_grouped_foodenv_8e6da_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-3b38f0c6930e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_criteria\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mcheckpoint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     checkpoint_at_end=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [QMIX_grouped_foodenv_8e6da_00000])"
     ]
    }
   ],
   "source": [
    "e = FoodGame(comp_env_config)\n",
    "tuple_obs_space = Tuple([e.observation_space for i in range(len(e.agents))])\n",
    "tuple_act_space = Tuple([e.action_space for i in range(len(e.agents))])\n",
    "\n",
    "register_env(\"grouped_foodenv\", lambda config: FoodGame(config).with_agent_groups(\n",
    "            groups={'group'+str(i):[e.agent_names[i]] for i in range(len(e.agents))},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        ))\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"env\"] = \"grouped_foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = comp_env_config\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"target_network_update_freq\"] = 500\n",
    "config[\"rollout_fragment_length\"] = 4\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"timesteps_per_iteration\"] = 1000\n",
    "# config[\"exploration_config\"] = {\"epsilon_timesteps\": 10000, }\n",
    "config[\"exploration_config\"] = {\"epsilon_timesteps\": 15000, \"final_epsilon\": 0.0}\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"training_iteration\": 20}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    \"QMIX\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "82c4fbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group0': 'agent0', 'group1': 'agent1'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'group'+str(i):e.agent_names[i] for i in range(len(e.agents))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365ac89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
