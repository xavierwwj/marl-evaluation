{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Dict, Discrete, MultiDiscrete, Tuple, Box\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, ENV_STATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8496c",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb321d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodGame(MultiAgentEnv):\n",
    "    def __init__(self, env_config):\n",
    "        # Game Settings\n",
    "        self.agents = env_config['agents'] # [0,1,...]\n",
    "        self.agent_names = ['agent'+str(i) for i in range(len(self.agents))]\n",
    "        self.food_loc = env_config['food_loc'] # {'food1': (row,col), ... } in 0 index\n",
    "        self.agent_loc = env_config['agent_loc'] # {0: (row,col), 1: (row,col), ...} in 0 index\n",
    "        self.board_size = env_config['board_size'] # (rows,cols)\n",
    "        self.time_limit = 20\n",
    "        self.cur_timestep = 0\n",
    "        self.mode = env_config['mode']\n",
    "        self.trainer = env_config['trainer']\n",
    "\n",
    "        # Environment Settings\n",
    "        self.action_space = Discrete(4) # L,R,U,D\n",
    "        if self.trainer == 'maddpg':\n",
    "            self.observation_space = Box(low=np.array([0, 0]*len(self.agents)+[0,0]*len(self.food_loc.items())), high=np.array([self.board_size[0]-1, self.board_size[1]-1]*len(self.agents)+[self.board_size[0], self.board_size[1]]*len(self.food_loc.items())), dtype=np.float32)\n",
    "        else:\n",
    "            self.observation_space = MultiDiscrete([self.board_size[0], self.board_size[1]]*len(self.agents)+[1+self.board_size[0], 1+self.board_size[1]]*len(self.food_loc.items())) # agent1_loc, agent2_loc, food1_loc, ...\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        s = ()\n",
    "        self.cur_timestep = 0\n",
    "        for key in self.agent_loc:\n",
    "            s = s + self.agent_loc[key]\n",
    "\n",
    "        for key in self.food_loc:\n",
    "            s = s + (self.food_loc[key][0]+1,self.food_loc[key][1]+1)\n",
    "\n",
    "        self.state = np.array(s)\n",
    "#         if self.trainer == \"qmix\":\n",
    "#             return {agent: {\"obs\": self.state.copy(), ENV_STATE: self.state.copy()} for agent in self.agent_names}\n",
    "        return {agent: self.state.copy() for agent in self.agent_names}\n",
    "\n",
    "    def render(self):\n",
    "        board = np.full(self.board_size,'',dtype=object)\n",
    "        s = self.state.reshape(-1,2)\n",
    "        agents_loc = s[:len(self.agents),:]\n",
    "        foods_loc = s[len(self.agents):,:]\n",
    "        for i in range(len(agents_loc)):\n",
    "            board[agents_loc[i][0],agents_loc[i][1]] += str(i)\n",
    "        for j in range(len(foods_loc)):\n",
    "            if tuple(foods_loc[j]) != (0,0):\n",
    "                board[foods_loc[j][0]-1,foods_loc[j][1]-1] += 'F'\n",
    "        print(board)\n",
    "        print()\n",
    "        return board\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        self.cur_timestep += 1\n",
    "        '''\n",
    "        actions = {\"agent1\": ..., \"agent2\": ..., ...}\n",
    "        '''\n",
    "        if self.trainer == 'maddpg':\n",
    "            action_dict = {\n",
    "                    k: np.random.choice([0, 1, 2, 3], p=v) for k, v in action_dict.items()\n",
    "                }\n",
    "        s = self.state.reshape(-1,2)\n",
    "        rewards = {}\n",
    "        for k in self.agent_names:\n",
    "            rewards[k] = -1 # penalty per step\n",
    "        food_loc_lt = s[len(self.agent_names):,:]\n",
    "        for i in range(len(self.agent_names)):\n",
    "            action = action_dict[self.agent_names[i]]\n",
    "            if action == 0:\n",
    "                # L\n",
    "                new_pos = [s[i][0], max(0,s[i][1]-1)]\n",
    "            elif action == 1:\n",
    "                # R\n",
    "                new_pos = [s[i][0], min(self.board_size[1]-1,s[i][1]+1)]\n",
    "            elif action == 2:\n",
    "                # U\n",
    "                new_pos = [max(0,s[i][0]-1), s[i][1]]\n",
    "            elif action == 3:\n",
    "                # D\n",
    "                new_pos = [min(self.board_size[0]-1,s[i][0]+1), s[i][1]]\n",
    "            else:\n",
    "                raise ValueError('ActionError')\n",
    "\n",
    "            s[i] = np.array(new_pos)\n",
    "            for j in range(len(food_loc_lt)):\n",
    "                if new_pos[0] == food_loc_lt[j][0]-1 and new_pos[1] == food_loc_lt[j][1]-1:\n",
    "                    s[j+len(self.agent_names)] = np.array([0,0])\n",
    "                    if self.mode == \"coop\":\n",
    "                        for k in self.agent_names:\n",
    "                            rewards[k] += 10 # cooperative global reward\n",
    "                    else:\n",
    "                        rewards[self.agent_names[i]] += 10 # competitive individual reward\n",
    "        new_obs_single = s.flatten()\n",
    "        done = True\n",
    "        for j in range(len(food_loc_lt)):\n",
    "            if tuple(food_loc_lt[j]) != (0,0):\n",
    "                done = False\n",
    "                break\n",
    "        if done or self.time_limit == self.cur_timestep:\n",
    "            dones = {agent: True for agent in self.agent_names}\n",
    "            dones['__all__'] = True\n",
    "        else:\n",
    "            dones = {agent: False for agent in self.agent_names}\n",
    "            dones['__all__'] = False\n",
    "            \n",
    "        infos = {agent: dict() for agent in self.agent_names}\n",
    "\n",
    "#         if self.trainer == 'qmix':\n",
    "#             new_obs = {agent: {\"obs\": new_obs_single.copy(), ENV_STATE: new_obs_single.copy()} for agent in self.agent_names}\n",
    "#         else:\n",
    "        new_obs = {agent: new_obs_single.copy() for agent in self.agent_names}\n",
    "\n",
    "        return new_obs, rewards, dones, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a19992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_environment():\n",
    "    env_config = dict()\n",
    "    env_config['agents'] = [0,1]\n",
    "    env_config['food_loc'] = {'food1': (3,3), 'food2': (5,5)}\n",
    "    env_config['agent_loc'] = {0: (4,4), 1: (6,7)}\n",
    "    env_config['board_size'] = (7,10)\n",
    "    env_config['mode'] = \"comp\"\n",
    "    env_config['trainer'] = 'ppo'\n",
    "\n",
    "    fg = FoodGame(env_config)\n",
    "    fg.reset()\n",
    "    fg.render()\n",
    "    MOVE_MAPPING = {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
    "    cumu_rewards = [0,0]\n",
    "    for i in range(100):\n",
    "        a0_action = np.random.randint(4)\n",
    "        a1_action = np.random.randint(4)\n",
    "        print('Actions: agent0 = {}, agent1 = {}'.format(MOVE_MAPPING[a0_action],MOVE_MAPPING[a1_action]))\n",
    "        a, rewards, dones, infos = fg.step(action_dict={'agent0': a0_action, 'agent1': a1_action})\n",
    "        fg.render()\n",
    "        cumu_rewards[0] += rewards['agent0']\n",
    "        cumu_rewards[1] += rewards['agent1']\n",
    "        if (np.array(list(dones.values())) == True).all():\n",
    "            print(\"Episode Ended\")\n",
    "            print(\"Rewards:\",cumu_rewards)\n",
    "            break\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "register_env(\"foodenv\", FoodGame)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738d6f1",
   "metadata": {},
   "source": [
    "# Model (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(\n",
    "            self, obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])\n",
    "    \n",
    "from ray.rllib.models import ModelCatalog\n",
    "ModelCatalog.register_custom_model(\n",
    "        \"my_model\", TorchCustomModel\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2e6b7",
   "metadata": {},
   "source": [
    "\n",
    "# Environment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coop Environment\n",
    "coop_env_config = dict()\n",
    "coop_env_config['agents'] = [0,1]\n",
    "coop_env_config['food_loc'] = {'food1': (3,3), 'food2': (5,5)}\n",
    "coop_env_config['agent_loc'] = {0: (4,4), 1: (6,7)}\n",
    "coop_env_config['board_size'] = (7,10)\n",
    "coop_env_config['mode'] = 'coop'\n",
    "coop_env_config['trainer'] = 'ppo'\n",
    "\n",
    "# Competitive Environment\n",
    "comp_env_config = dict()\n",
    "comp_env_config['agents'] = [0,1]\n",
    "comp_env_config['food_loc'] = {'food1': (3,3), 'food2': (5,5)}\n",
    "comp_env_config['agent_loc'] = {0: (4,4), 1: (6,7)}\n",
    "comp_env_config['board_size'] = (7,10)\n",
    "comp_env_config['mode'] = 'comp'\n",
    "comp_env_config['trainer'] = 'ppo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray import tune\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9371d",
   "metadata": {},
   "source": [
    "# PPO Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326dddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = FoodGame(coop_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"timesteps_per_iteration\"] = 1000\n",
    "config[\"env_config\"] = coop_env_config\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"training_iteration\": 20}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6975622",
   "metadata": {},
   "source": [
    "# Results Analysis (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ppo.PPOTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(best_checkpoint[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8030c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe12e4",
   "metadata": {},
   "source": [
    "# PPO Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaef693",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = FoodGame(comp_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"timesteps_per_iteration\"] = 1000\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = comp_env_config\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"training_iteration\": 20}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    ppo.PPOTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e399f68",
   "metadata": {},
   "source": [
    "# Results Analysis (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f724e43",
   "metadata": {},
   "source": [
    "# DQN Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coop Environment\n",
    "coop_env_config['trainer'] = 'dqn'\n",
    "\n",
    "# Competitive Environment\n",
    "comp_env_config['trainer'] = 'dqn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ead0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.dqn as dqn\n",
    "\n",
    "e = FoodGame(coop_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "config[\"prioritized_replay\"] = True\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "# eval\n",
    "# config[\"evaluation_interval\"] = 1\n",
    "# config[\"evaluation_duration\"] = 1\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    dqn.DQNTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58954483",
   "metadata": {},
   "source": [
    "# Results Analysis (Coop + DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b256265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "\n",
    "trainer = dqn.DQNTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(best_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f061be",
   "metadata": {},
   "source": [
    "# DQN Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138acf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.dqn as dqn\n",
    "e = FoodGame(comp_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = comp_env_config\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "config[\"prioritized_replay\"] = False\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "# eval\n",
    "# config[\"evaluation_interval\"] = 1\n",
    "# config[\"evaluation_duration\"] = 1\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    dqn.DQNTrainer,\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0efbc2",
   "metadata": {},
   "source": [
    "# Results Analysis (Comp + DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadbfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = dqn.DQNTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ee5f",
   "metadata": {},
   "source": [
    "# MADDPG Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_env_config['trainer'] = 'maddpg'\n",
    "comp_env_config['trainer'] = 'maddpg'\n",
    "e = FoodGame(coop_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "\n",
    "# model\n",
    "config[\"actor_hiddens\"] = [256, 256]\n",
    "config[\"actor_hidden_activation\"] = \"tanh\"\n",
    "config[\"critic_hiddens\"] = [256, 256]\n",
    "config[\"critic_hidden_activation\"] = \"tanh\"\n",
    "config[\"learning_starts\"] = 1000 # in terms of samples\n",
    "config[\"critic_lr\"] = 1e-2 # in terms of samples\n",
    "config[\"actor_lr\"] = 1e-2 # in terms of samples\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    \"contrib/MADDPG\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eb176",
   "metadata": {},
   "source": [
    "# Results Analysis (Coop + MADDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2edf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.contrib.maddpg as maddpg\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = maddpg.MADDPGTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca3a2fa",
   "metadata": {},
   "source": [
    "# MADDPG Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_env_config['trainer'] = 'maddpg'\n",
    "comp_env_config['trainer'] = 'maddpg'\n",
    "\n",
    "e = FoodGame(comp_env_config)\n",
    "\n",
    "# For custom NN models\n",
    "# def gen_policy(i):\n",
    "#     config = {\n",
    "#         \"model\": {\n",
    "#             \"custom_model\": \"my_model\",\n",
    "#         },\n",
    "#         \"gamma\": 0.99,\n",
    "#     }\n",
    "#     return (None, e.observation_space, e.action_space, config)\n",
    "\n",
    "policies = {\"policy\"+str(i):  PolicySpec(\n",
    "                        observation_space= e.observation_space,\n",
    "                        action_space= e.action_space,\n",
    "                        config={\"agent_id\": i}) for i in range(len(e.agents))}\n",
    "\n",
    "policy_mapping = {'agent'+str(i): \"policy\"+str(i) for i in range(len(e.agents))}\n",
    "\n",
    "def p(agent_id):\n",
    "    print(agent_id)\n",
    "    return policies[policy_keys[agent_id]]\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"multiagent\"] = {\"policies\": policies,  \"policy_mapping_fn\": lambda agent_id: policy_mapping[agent_id]}\n",
    "config[\"env\"] = \"foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "config[\"batch_mode\"] = \"complete_episodes\"\n",
    "\n",
    "# model\n",
    "config[\"actor_hiddens\"] = [256, 256]\n",
    "config[\"actor_hidden_activation\"] = \"tanh\"\n",
    "config[\"critic_hiddens\"] = [256, 256]\n",
    "config[\"critic_hidden_activation\"] = \"tanh\"\n",
    "config[\"learning_starts\"] = 1000 # in terms of samples\n",
    "config[\"critic_lr\"] = 1e-2 # in terms of samples\n",
    "config[\"actor_lr\"] = 1e-2 # in terms of samples\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    \"contrib/MADDPG\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "#     checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72caa7ef",
   "metadata": {},
   "source": [
    "# Results Analysis (Comp + MADDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.contrib.maddpg as maddpg\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = maddpg.MADDPGTrainer(config=config, env=\"foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "episode_reward = {'agent'+str(i):0 for i in range(len(e.agents))}\n",
    "done = False\n",
    "obs = e.reset()\n",
    "print(\"Initial state at Time 0:\")\n",
    "e.render()\n",
    "time = 1\n",
    "while not done:\n",
    "    print(\"Time:\",time)\n",
    "    time += 1\n",
    "    action = trainer.compute_actions(obs,policy_id='policy0')\n",
    "    for i in range(1,len(e.agents)):\n",
    "        a = trainer.compute_actions(obs,policy_id='policy'+str(i))\n",
    "        action['agent'+str(i)] = a['agent'+str(i)]\n",
    "    obs, reward, dones, info = e.step(action)\n",
    "    done = dones['__all__']\n",
    "    e.render()\n",
    "    for i in range(len(e.agents)):\n",
    "        episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "print(\"Episode Ended\")\n",
    "print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b06b3",
   "metadata": {},
   "source": [
    "# QMIX Trainer (Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c004af",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_env_config['trainer'] = 'qmix'\n",
    "comp_env_config['trainer'] = 'qmix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5aca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = FoodGame(coop_env_config)\n",
    "tuple_obs_space = Tuple([e.observation_space for i in range(len(e.agents))])\n",
    "tuple_act_space = Tuple([e.action_space for i in range(len(e.agents))])\n",
    "\n",
    "register_env(\"grouped_foodenv\", lambda config: FoodGame(config).with_agent_groups(\n",
    "            groups={\"agents\": e.agent_names},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        ))\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"env\"] = \"grouped_foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = coop_env_config\n",
    "\n",
    "# Learning params\n",
    "config[\"lr\"] = 1e-3\n",
    "config[\"optim_alpha\"] = 0.99\n",
    "config[\"optim_eps\"] = 0.00001\n",
    "config[\"grad_norm_clipping\"] = 10\n",
    "config[\"target_network_update_freq\"] = 100\n",
    "\n",
    "# Buffer params\n",
    "config[\"buffer_size\"] = 20000 # in batches\n",
    "\n",
    "# Exploration params\n",
    "config[\"exploration_config\"] = {\"type\": \"EpsilonGreedy\", \"epsilon_timesteps\": 25000, \"final_epsilon\": 0.00}\n",
    "\n",
    "# Learning duration\n",
    "config[\"timesteps_per_iteration\"] = 100\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"rollout_fragment_length\"] = 4 # no. of samples to rollout in each sample of the batch to add to buffer each timestep\n",
    "config[\"model\"]={\n",
    "        \"lstm_cell_size\": 256,\n",
    "        \"max_seq_len\": 20,\n",
    "    }\n",
    "\n",
    "config[\"mixing_embed_dim\"] = 256\n",
    "# eval\n",
    "# config[\"evaluation_interval\"] = 1\n",
    "# config[\"evaluation_duration\"] = 1\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"timesteps_total\": 30000}\n",
    "analysis = ray.tune.run(\n",
    "    \"QMIX\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=100,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2223f",
   "metadata": {},
   "source": [
    "# Results Analysis (QMIX + Coop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b356e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.qmix as qmix\n",
    "\n",
    "# list of lists: one list per checkpoint; each checkpoint list contains\n",
    "# 1st the path, 2nd the metric value\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\")\n",
    "\n",
    "best_checkpoint = sorted(checkpoints, key=lambda x: x[1])[-1]\n",
    "last_checkpoint = checkpoints[-1]\n",
    "\n",
    "trainer = qmix.QMixTrainer(config=config, env=\"grouped_foodenv\")\n",
    "trainer.restore(last_checkpoint[0])\n",
    "\n",
    "# get weights: trainer.get_weights()\n",
    "\n",
    "# run until episode ends\n",
    "\n",
    "grouped_e = FoodGame(coop_env_config).with_agent_groups(\n",
    "            groups={\"agents\": FoodGame(coop_env_config).agent_names},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        )\n",
    "\n",
    "# episode_reward = {'agent'+str(i):0 for i in range(len(grouped_e.agents))}\n",
    "# done = False\n",
    "# obs = grouped_e.reset()\n",
    "# print(\"Initial state at Time 0:\")\n",
    "# grouped_e.render()\n",
    "# time = 1\n",
    "# while not done:\n",
    "#     print(\"Time:\",time)\n",
    "#     time += 1\n",
    "#     action = trainer.compute_actions(obs,policy_id='default_policy')\n",
    "#     for i in range(1,len(grouped_e.agents)):\n",
    "#         a = trainer.compute_actions(obs,policy_id='default_policy')\n",
    "#         action['agent'+str(i)] = a['agent'+str(i)]\n",
    "#     obs, reward, dones, info = grouped_e.step(action)\n",
    "#     done = dones['__all__']\n",
    "#     grouped_e.render()\n",
    "#     for i in range(len(grouped_e.agents)):\n",
    "#         episode_reward['agent'+str(i)] += reward['agent'+str(i)]\n",
    "# print(\"Episode Ended\")\n",
    "# print(\"Episode Rewards\",episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981664b",
   "metadata": {},
   "source": [
    "# QMIX Trainer (Comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = FoodGame(comp_env_config)\n",
    "tuple_obs_space = Tuple([e.observation_space for i in range(len(e.agents))])\n",
    "tuple_act_space = Tuple([e.action_space for i in range(len(e.agents))])\n",
    "\n",
    "register_env(\"grouped_foodenv\", lambda config: FoodGame(config).with_agent_groups(\n",
    "            groups={'group'+str(i):[e.agent_names[i]] for i in range(len(e.agents))},\n",
    "            obs_space=tuple_obs_space,\n",
    "            act_space=tuple_act_space,\n",
    "        ))\n",
    "\n",
    "config = dict()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"env\"] = \"grouped_foodenv\"\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"env_config\"] = comp_env_config\n",
    "config[\"lr\"] = 1e-2\n",
    "config[\"learning_starts\"] = 1000\n",
    "config[\"target_network_update_freq\"] = 500\n",
    "config[\"rollout_fragment_length\"] = 4\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"timesteps_per_iteration\"] = 1000\n",
    "# config[\"exploration_config\"] = {\"epsilon_timesteps\": 10000, }\n",
    "config[\"exploration_config\"] = {\"epsilon_timesteps\": 15000, \"final_epsilon\": 0.0}\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H:%M:%S\")\n",
    "os.mkdir('./'+dt_string)\n",
    "log_dir = './'+dt_string+'/'\n",
    "\n",
    "stop_criteria = {\"training_iteration\": 20}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    \"QMIX\",\n",
    "    config=config,\n",
    "    local_dir=log_dir,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'group'+str(i):e.agent_names[i] for i in range(len(e.agents))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365ac89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
